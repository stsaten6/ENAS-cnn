srun: interrupt (one more within 1 sec to abort)
srun: task 0: running
srun: interrupt (one more within 1 sec to abort)
srun: task 0: running
srun: sending Ctrl-C to job 1542015.0
Traceback (most recent call last):
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
  File "main.py", line 4, in <module>
    import torch
  File "/mnt/lustre/lianqing/anaconda3/lib/python3.6/site-packages/torch/__init__.py", line 37, in <module>
slurmstepd: *** STEP 1542015.0 ON BJ-IDC1-10-10-30-130 CANCELLED AT 2018-03-26T10:53:06 ***
    import numpy as np
  File "/mnt/lustre/lianqing/anaconda3/lib/python3.6/site-packages/numpy/__init__.py", line 156, in <module>
[*] Make directories : logs/cifar10_2018-03-26_10-53-43
Files already downloaded and verified
regularizing:
----- begin to init cnn------
defaultdict(<class 'dict'>, {0: {'1x1': Sequential(
  (0): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(3, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 1: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 2: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 3: {'1x1': Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 4: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 5: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 6: {'1x1': Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 7: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 8: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 9: {'1x1': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 10: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 11: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}})
# of parameters: 145,759,370
---- begin to init controller-----
===begin to cuda
cuda
finish cuda
=======make optimizer========
=======make optimizer========
finish init
[*] MODEL dir: logs/cifar10_2018-03-26_10-53-43
[*] PARAM path: logs/cifar10_2018-03-26_10-53-43/params.json
loss,  
 2.3281
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 2.3281
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 52.9083
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 2.5194
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 105.0917
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 2.5632
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.63 | loss 2.63 | ppl    13.88
loss,  
 24.8520
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.4074
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 75.4652
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.9317
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 126.3038
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 1.2505
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.53 | loss 2.53 | ppl    12.50
loss,  
 49.4046
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.4083
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 98.7641
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.7005
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.47 | loss 2.47 | ppl    11.83
loss,  
 24.6576
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
 0.1532
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 70.8356
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.3914
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 119.9106
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.5966
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.40 | loss 2.40 | ppl    11.00
loss,  
 47.9962
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.2172
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 94.4520
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.3919
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.37 | loss 2.37 | ppl    10.64
loss,  
 25.0937
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  9.6145
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 73.0871
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.2601
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 119.9894
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.3986
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.40 | loss 2.40 | ppl    11.02
loss,  
 46.3649
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
 0.1444
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 89.8139
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.2634
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.23 | loss 2.23 | ppl     9.33
loss,  
 21.8935
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  6.0647
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 68.7265
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1804
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 117.2257
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.2923
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.34 | loss 2.34 | ppl    10.43
| epoch   0 | lr 0.00035 | R 0.78939 | entropy 1.5922 | loss 0.98577
| epoch   0 | lr 0.00035 | R 0.83420 | entropy 1.5922 | loss 0.84242
| epoch   0 | lr 0.00035 | R 0.82321 | entropy 1.5921 | loss 0.06216
| epoch   0 | lr 0.00035 | R 0.94436 | entropy 1.5920 | loss 1.58690
| epoch   0 | lr 0.00035 | R 0.95710 | entropy 1.5912 | loss -0.28076
| epoch   0 | lr 0.00035 | R 1.10215 | entropy 1.5530 | loss 2.25816
| epoch   0 | lr 0.00035 | R 1.24881 | entropy 1.5013 | loss 1.63411
| epoch   0 | lr 0.00035 | R 1.23875 | entropy 1.4775 | loss 0.19599
| epoch   0 | lr 0.00035 | R 1.26478 | entropy 1.4642 | loss -0.06514
| epoch   0 | lr 0.00035 | R 1.28102 | entropy 1.4296 | loss -0.16370
| epoch   0 | lr 0.00035 | R 1.35401 | entropy 1.4062 | loss 0.80392
| epoch   0 | lr 0.00035 | R 1.32079 | entropy 1.3949 | loss -0.74481
| epoch   0 | lr 0.00035 | R 1.40003 | entropy 1.3811 | loss 0.72195
| epoch   0 | lr 0.00035 | R 1.37059 | entropy 1.3634 | loss -0.38013
| epoch   0 | lr 0.00035 | R 1.38042 | entropy 1.3440 | loss -0.42114
| epoch   0 | lr 0.00035 | R 1.42471 | entropy 1.3220 | loss 0.17732
| epoch   0 | lr 0.00035 | R 1.41022 | entropy 1.3135 | loss -0.59973
| epoch   0 | lr 0.00035 | R 1.43105 | entropy 1.3011 | loss 0.30373
| epoch   0 | lr 0.00035 | R 1.45892 | entropy 1.2875 | loss 0.39675
| epoch   0 | lr 0.00035 | R 1.43479 | entropy 1.2670 | loss -0.83237
| epoch   0 | lr 0.00035 | R 1.45152 | entropy 1.2498 | loss 0.29940
| epoch   0 | lr 0.00035 | R 1.44476 | entropy 1.2489 | loss -1.06944
| epoch   0 | lr 0.00035 | R 1.47658 | entropy 1.2398 | loss 0.59712
| epoch   0 | lr 0.00035 | R 1.49963 | entropy 1.2317 | loss 0.33995
| epoch   0 | lr 0.00035 | R 1.48816 | entropy 1.2141 | loss -0.39517
| epoch   0 | lr 0.00035 | R 1.53387 | entropy 1.2138 | loss 0.28761
| epoch   0 | lr 0.00035 | R 1.53978 | entropy 1.2082 | loss -0.06854
| epoch   0 | lr 0.00035 | R 1.51938 | entropy 1.1948 | loss -0.67120
| epoch   0 | lr 0.00035 | R 1.47608 | entropy 1.1932 | loss -0.51691
| epoch   0 | lr 0.00035 | R 1.54258 | entropy 1.1857 | loss 0.42880
| epoch   0 | lr 0.00035 | R 1.55967 | entropy 1.1782 | loss 0.11442
| epoch   0 | lr 0.00035 | R 1.55762 | entropy 1.1771 | loss -0.22013
| epoch   0 | lr 0.00035 | R 1.53709 | entropy 1.1666 | loss -0.64381
| epoch   0 | lr 0.00035 | R 1.56546 | entropy 1.1596 | loss -0.18627
| epoch   0 | lr 0.00035 | R 1.54877 | entropy 1.1468 | loss -0.07453
| epoch   0 | lr 0.00035 | R 1.59914 | entropy 1.1377 | loss 0.33797
| epoch   0 | lr 0.00035 | R 1.58214 | entropy 1.1362 | loss -0.36645
| epoch   0 | lr 0.00035 | R 1.61121 | entropy 1.1365 | loss -0.11917
| epoch   0 | lr 0.00035 | R 1.58387 | entropy 1.1290 | loss -0.22279
loss,  
 2.2901
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 2.2901
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 46.7301
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 2.2252
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 89.3173
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 2.1785
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.20 | loss 2.20 | ppl     9.02
loss,  
 20.0577
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.3288
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 62.0903
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.7665
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 102.0680
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 1.0106
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.04 | loss 2.04 | ppl     7.70
loss,  
 38.9344
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.3218
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 79.0570
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.5607
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.97 | loss 1.97 | ppl     7.15
loss,  
 19.6347
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
 0.1220
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 58.6504
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.3240
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 97.1040
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.4831
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.94 | loss 1.94 | ppl     6.97
loss,  
 40.0184
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.1811
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 79.6451
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.3305
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.97 | loss 1.97 | ppl     7.18
loss,  
 19.0864
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  7.3128
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 57.3379
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.2040
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 97.5836
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.3242
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.95 | loss 1.95 | ppl     7.04
loss,  
 38.3665
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
 0.1195
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 76.8713
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.2254
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.92 | loss 1.92 | ppl     6.81
loss,  
 19.8163
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  5.4893
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 57.3386
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1505
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 95.5029
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.2382
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.91 | loss 1.91 | ppl     6.75
| epoch   0 | lr 0.00035 | R 2.00467 | entropy 1.1020 | loss -4.06187
| epoch   0 | lr 0.00035 | R 2.13195 | entropy 1.0671 | loss -0.62267
| epoch   0 | lr 0.00035 | R 2.02133 | entropy 1.0529 | loss -0.02713
| epoch   0 | lr 0.00035 | R 2.26525 | entropy 1.0417 | loss 1.52942
| epoch   0 | lr 0.00035 | R 2.24247 | entropy 1.0307 | loss -1.00549
| epoch   0 | lr 0.00035 | R 2.28045 | entropy 1.0209 | loss 0.20990
| epoch   0 | lr 0.00035 | R 2.22001 | entropy 1.0014 | loss -1.33428
| epoch   0 | lr 0.00035 | R 2.35091 | entropy 0.9903 | loss 0.23950
| epoch   0 | lr 0.00035 | R 2.29074 | entropy 0.9813 | loss -1.09627
| epoch   0 | lr 0.00035 | R 2.30960 | entropy 0.9773 | loss -0.43332
| epoch   0 | lr 0.00035 | R 2.27154 | entropy 0.9746 | loss -0.44952
srun: forcing job termination
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: *** STEP 1542016.0 ON BJ-IDC1-10-10-30-130 CANCELLED AT 2018-03-26T11:20:00 ***
srun: error: BJ-IDC1-10-10-30-130: task 0: Killed
  File "main.py", line 25
    raise NotImplementedError(f"{args.dataset} is not supported")
                                                               ^
srun: error: BJ-IDC1-10-10-30-223: task 0: Exited with exit code 1
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "main.py", line 9, in <module>
    import trainer
  File "/mnt/lustre/lianqing/enas/cnn/ENAS-cnn/trainer.py", line 220
    best_dag,
    ^
IndentationError: unexpected indent
srun: error: BJ-IDC1-10-10-30-223: task 0: Exited with exit code 1
[*] Make directories : logs/cifar10_2018-03-26_11-32-36
Files already downloaded and verified
regularizing:
----- begin to init cnn------
defaultdict(<class 'dict'>, {0: {'1x1': Sequential(
  (0): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(3, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 1: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 2: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 3: {'1x1': Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 4: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 5: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 6: {'1x1': Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 7: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 8: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 9: {'1x1': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 10: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 11: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}})
# of parameters: 145,759,370
---- begin to init controller-----
===begin to cuda
cuda
finish cuda
=======make optimizer========
=======make optimizer========
finish init
[*] MODEL dir: logs/cifar10_2018-03-26_11-32-36
[*] PARAM path: logs/cifar10_2018-03-26_11-32-36/params.json
loss,  
 2.2606
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 2.2606
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 53.6450
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 2.5545
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 105.1047
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 2.5635
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.61 | loss 2.61 | ppl    13.62
loss,  
 24.4307
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.4005
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 74.7885
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.9233
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 123.5465
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 1.2232
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.47 | loss 2.47 | ppl    11.83
loss,  
 48.0694
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.3973
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 96.9414
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.6875
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.41 | loss 2.41 | ppl    11.18
loss,  
 25.0948
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
 0.1559
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 74.0446
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.4091
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 122.0111
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.6070
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.44 | loss 2.44 | ppl    11.48
loss,  
 47.7635
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.2161
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 93.8065
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.3892
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.33 | loss 2.33 | ppl    10.30
loss,  
 24.1998
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  9.2720
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 70.7185
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.2517
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 115.4561
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.3836
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.31 | loss 2.31 | ppl    10.07
loss,  
 45.4971
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
 0.1417
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 92.1154
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.2701
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.29 | loss 2.29 | ppl     9.92
loss,  
 23.9132
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  6.6242
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 71.4033
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1874
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 116.4460
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.2904
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.33 | loss 2.33 | ppl    10.27
| epoch   0 | lr 0.00035 | R 0.77404 | entropy 1.5922 | loss 3.20481
| epoch   0 | lr 0.00035 | R 0.81299 | entropy 1.5922 | loss 0.63198
| epoch   0 | lr 0.00035 | R 0.87743 | entropy 1.5921 | loss 0.78008
| epoch   0 | lr 0.00035 | R 0.70764 | entropy 1.5920 | loss -1.98276
| epoch   0 | lr 0.00035 | R 0.72730 | entropy 1.5919 | loss -0.39956
| epoch   0 | lr 0.00035 | R 0.73805 | entropy 1.5851 | loss 1.20835
| epoch   0 | lr 0.00035 | R 0.88803 | entropy 1.5389 | loss 1.32728
| epoch   0 | lr 0.00035 | R 0.93143 | entropy 1.5083 | loss 0.52231
| epoch   0 | lr 0.00035 | R 0.97263 | entropy 1.4929 | loss 0.35347
| epoch   0 | lr 0.00035 | R 0.98542 | entropy 1.4970 | loss -0.05802
| epoch   0 | lr 0.00035 | R 1.00901 | entropy 1.5041 | loss 0.48015
| epoch   0 | lr 0.00035 | R 0.97864 | entropy 1.4841 | loss -0.52681
| epoch   0 | lr 0.00035 | R 0.99832 | entropy 1.4756 | loss 0.13235
| epoch   0 | lr 0.00035 | R 0.97073 | entropy 1.4591 | loss -0.20077
| epoch   0 | lr 0.00035 | R 1.01495 | entropy 1.4295 | loss 0.32657
| epoch   0 | lr 0.00035 | R 1.00433 | entropy 1.4133 | loss -0.97082
| epoch   0 | lr 0.00035 | R 1.04209 | entropy 1.3963 | loss 0.51904
| epoch   0 | lr 0.00035 | R 1.03422 | entropy 1.3754 | loss -0.05151
| epoch   0 | lr 0.00035 | R 1.05417 | entropy 1.3606 | loss 0.39165
| epoch   0 | lr 0.00035 | R 1.06341 | entropy 1.3488 | loss -0.22524
| epoch   0 | lr 0.00035 | R 1.03331 | entropy 1.3296 | loss -0.39734
| epoch   0 | lr 0.00035 | R 1.05133 | entropy 1.3159 | loss -0.24377
| epoch   0 | lr 0.00035 | R 1.06648 | entropy 1.2864 | loss 0.20380
| epoch   0 | lr 0.00035 | R 1.05897 | entropy 1.2786 | loss -0.19791
| epoch   0 | lr 0.00035 | R 1.06579 | entropy 1.2820 | loss -0.00814
| epoch   0 | lr 0.00035 | R 1.11507 | entropy 1.2750 | loss 0.69285
| epoch   0 | lr 0.00035 | R 1.07473 | entropy 1.2544 | loss -0.94016
| epoch   0 | lr 0.00035 | R 1.07869 | entropy 1.2304 | loss -0.15626
| epoch   0 | lr 0.00035 | R 1.09265 | entropy 1.2245 | loss 0.04066
| epoch   0 | lr 0.00035 | R 1.07374 | entropy 1.2131 | loss -0.28159
| epoch   0 | lr 0.00035 | R 1.11547 | entropy 1.1962 | loss 0.20679
| epoch   0 | lr 0.00035 | R 1.10714 | entropy 1.1873 | loss -0.36131
| epoch   0 | lr 0.00035 | R 1.10534 | entropy 1.1726 | loss -0.29086
| epoch   0 | lr 0.00035 | R 1.12517 | entropy 1.1544 | loss -0.12040
| epoch   0 | lr 0.00035 | R 1.13746 | entropy 1.1344 | loss 0.06520
| epoch   0 | lr 0.00035 | R 1.15755 | entropy 1.1158 | loss 0.11846
| epoch   0 | lr 0.00035 | R 1.17481 | entropy 1.1005 | loss -0.07266
| epoch   0 | lr 0.00035 | R 1.17531 | entropy 1.0919 | loss -0.31112
| epoch   0 | lr 0.00035 | R 1.16814 | entropy 1.0856 | loss -0.31979
loss,  
 2.1879
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 2.1879
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 45.7517
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 2.1787
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 87.1766
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 2.1263
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.17 | loss 2.17 | ppl     8.72
loss,  
 20.5328
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.3366
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 61.9949
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.7654
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 104.0306
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 1.0300
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.08 | loss 2.08 | ppl     8.01
loss,  
 39.9239
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.3299
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 80.8648
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.5735
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.05 | loss 2.05 | ppl     7.78
loss,  
 19.6321
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
 0.1219
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 59.2118
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.3271
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 98.7764
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.4914
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.98 | loss 1.98 | ppl     7.21
loss,  
 38.3816
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.1737
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 77.7820
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.3227
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.96 | loss 1.96 | ppl     7.09
loss,  
 20.2296
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  7.7508
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 60.0050
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.2135
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 99.0797
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.3292
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.98 | loss 1.98 | ppl     7.25
loss,  
 38.8410
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
 0.1210
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 76.5695
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.2245
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.91 | loss 1.91 | ppl     6.76
loss,  
 18.6889
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  5.1770
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 57.3676
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1506
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 94.6128
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.2359
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 1.89 | loss 1.89 | ppl     6.63
| epoch   0 | lr 0.00035 | R 1.96369 | entropy 1.0571 | loss -6.54653
| epoch   0 | lr 0.00035 | R 1.94096 | entropy 1.0377 | loss -0.75278
| epoch   0 | lr 0.00035 | R 1.98611 | entropy 1.0224 | loss -0.18715
| epoch   0 | lr 0.00035 | R 2.67899 | entropy 0.9975 | loss 5.16142
| epoch   0 | lr 0.00035 | R 2.76471 | entropy 0.9690 | loss -0.46331
| epoch   0 | lr 0.00035 | R 2.87256 | entropy 0.9427 | loss -0.03246
| epoch   0 | lr 0.00035 | R 2.73181 | entropy 0.9101 | loss -2.16118
| epoch   0 | lr 0.00035 | R 2.89699 | entropy 0.8917 | loss 0.01402
| epoch   0 | lr 0.00035 | R 2.77263 | entropy 0.8780 | loss -1.76520
| epoch   0 | lr 0.00035 | R 2.84306 | entropy 0.8673 | loss -0.81057
| epoch   0 | lr 0.00035 | R 2.88344 | entropy 0.8535 | loss 0.10929
| epoch   0 | lr 0.00035 | R 2.96276 | entropy 0.8430 | loss -0.64879
| epoch   0 | lr 0.00035 | R 2.97462 | entropy 0.8315 | loss -0.94330
| epoch   0 | lr 0.00035 | R 2.95227 | entropy 0.8186 | loss -0.75625
| epoch   0 | lr 0.00035 | R 3.03253 | entropy 0.8063 | loss -0.44348
| epoch   0 | lr 0.00035 | R 2.94975 | entropy 0.7981 | loss -1.35368
| epoch   0 | lr 0.00035 | R 2.86410 | entropy 0.7843 | loss -1.90267
| epoch   0 | lr 0.00035 | R 2.98181 | entropy 0.7677 | loss -1.07715
| epoch   0 | lr 0.00035 | R 2.95008 | entropy 0.7643 | loss -0.48242
| epoch   0 | lr 0.00035 | R 2.94293 | entropy 0.7686 | loss -0.43463
| epoch   0 | lr 0.00035 | R 3.03491 | entropy 0.7650 | loss -0.06160
| epoch   0 | lr 0.00035 | R 3.03828 | entropy 0.7563 | loss -1.31420
| epoch   0 | lr 0.00035 | R 3.03285 | entropy 0.7426 | loss -0.61589
| epoch   0 | lr 0.00035 | R 2.93973 | entropy 0.7373 | loss -1.76728
| epoch   0 | lr 0.00035 | R 2.98861 | entropy 0.7334 | loss -0.55879
| epoch   0 | lr 0.00035 | R 3.08788 | entropy 0.7297 | loss 0.10509
| epoch   0 | lr 0.00035 | R 3.05537 | entropy 0.7247 | loss -0.94679
| epoch   0 | lr 0.00035 | R 3.13776 | entropy 0.7208 | loss 0.35123
| epoch   0 | lr 0.00035 | R 3.10806 | entropy 0.7178 | loss -0.40297
| epoch   0 | lr 0.00035 | R 3.03726 | entropy 0.7164 | loss -0.95850
| epoch   0 | lr 0.00035 | R 3.03971 | entropy 0.7143 | loss -0.94032
| epoch   0 | lr 0.00035 | R 3.06637 | entropy 0.7146 | loss -0.70757
| epoch   0 | lr 0.00035 | R 3.04884 | entropy 0.7092 | loss -0.61821
| epoch   0 | lr 0.00035 | R 3.09890 | entropy 0.7050 | loss -0.33097
| epoch   0 | lr 0.00035 | R 3.05253 | entropy 0.6976 | loss -1.06207
| epoch   0 | lr 0.00035 | R 3.13597 | entropy 0.6928 | loss -0.19431
| epoch   0 | lr 0.00035 | R 3.12535 | entropy 0.6903 | loss -0.54072
| epoch   0 | lr 0.00035 | R 3.08500 | entropy 0.6865 | loss -1.26337
| epoch   0 | lr 0.00035 | R 3.12576 | entropy 0.6800 | loss -0.43068
derive | max_R: 4.489138
========> finish evaluate on one epoch<======
eval | loss:   114.05 | ppl: 33840329565627227774676994661841117722081522352128.00
[*] SAVED: logs/cifar10_2018-03-26_11-32-36/shared_epoch0_step802.pth
[*] SAVED: logs/cifar10_2018-03-26_11-32-36/controller_epoch0_step4000.pth
derive | max_R: 3.399895
========> finish evaluate on one epoch<======
eval | loss:   109.73 | ppl: 452514338781458539907254331593545747586971860992.00
[*] SAVED: logs/cifar10_2018-03-26_11-32-36/shared_epoch0_step802.pth
[*] SAVED: logs/cifar10_2018-03-26_11-32-36/controller_epoch0_step4000.pth
loss,  
 1.6937
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 1.6937
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 36.9979
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 1.7618
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 73.2797
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 1.7873
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.81 | loss 1.81 | ppl     6.10
loss,  
 17.5767
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.2881
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 55.0867
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.6801
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 89.8309
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 0.8894
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.80 | loss 1.80 | ppl     6.03
loss,  
 34.1153
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.2819
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 70.0045
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.4965
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.75 | loss 1.75 | ppl     5.74
loss,  
 17.6391
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
 0.1096
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 52.3416
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.2892
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 85.9680
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.4277
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.72 | loss 1.72 | ppl     5.58
loss,  
 33.6027
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.1520
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 67.7989
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.2813
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.68 | loss 1.68 | ppl     5.39
loss,  
 17.1657
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  6.5769
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 51.1435
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.1820
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 84.4109
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.2804
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.69 | loss 1.69 | ppl     5.41
loss,  
 32.8583
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
 0.1024
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 65.3109
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.1915
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.65 | loss 1.65 | ppl     5.18
loss,  
 15.9731
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  4.4247
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 48.9380
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1284
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 82.8734
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.2067
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   1 | lr 0.10 | raw loss 1.66 | loss 1.66 | ppl     5.25
| epoch   1 | lr 0.00035 | R 3.57582 | entropy 0.6631 | loss -8.21155
| epoch   1 | lr 0.00035 | R 3.59173 | entropy 0.6460 | loss -2.51871
| epoch   1 | lr 0.00035 | R 3.50448 | entropy 0.6451 | loss 0.11378
| epoch   1 | lr 0.00035 | R 4.47254 | entropy 0.6501 | loss 5.40919
| epoch   1 | lr 0.00035 | R 4.57772 | entropy 0.6553 | loss -2.04226
| epoch   1 | lr 0.00035 | R 4.74327 | entropy 0.6532 | loss 0.47712
| epoch   1 | lr 0.00035 | R 4.63417 | entropy 0.6530 | loss -0.68129
| epoch   1 | lr 0.00035 | R 4.71503 | entropy 0.6499 | loss -0.99209
| epoch   1 | lr 0.00035 | R 4.66190 | entropy 0.6404 | loss -1.80032
| epoch   1 | lr 0.00035 | R 4.65370 | entropy 0.6356 | loss -1.24892
| epoch   1 | lr 0.00035 | R 4.69488 | entropy 0.6296 | loss -0.76306
| epoch   1 | lr 0.00035 | R 4.77008 | entropy 0.6245 | loss 0.15324
| epoch   1 | lr 0.00035 | R 4.78930 | entropy 0.6238 | loss -0.57517
| epoch   1 | lr 0.00035 | R 4.72610 | entropy 0.6267 | loss -1.40589
| epoch   1 | lr 0.00035 | R 4.69106 | entropy 0.6292 | loss -2.14505
| epoch   1 | lr 0.00035 | R 4.73668 | entropy 0.6201 | loss -0.86407
| epoch   1 | lr 0.00035 | R 4.67817 | entropy 0.6178 | loss -1.30693
| epoch   1 | lr 0.00035 | R 4.53768 | entropy 0.6209 | loss -3.02697
| epoch   1 | lr 0.00035 | R 4.81844 | entropy 0.6113 | loss 0.51611
| epoch   1 | lr 0.00035 | R 4.66492 | entropy 0.6048 | loss -3.26367
| epoch   1 | lr 0.00035 | R 4.87724 | entropy 0.5966 | loss 0.41454
| epoch   1 | lr 0.00035 | R 4.88655 | entropy 0.5879 | loss -1.95357
| epoch   1 | lr 0.00035 | R 4.69464 | entropy 0.5830 | loss -2.00894
| epoch   1 | lr 0.00035 | R 4.67866 | entropy 0.5828 | loss -1.99039
| epoch   1 | lr 0.00035 | R 4.88154 | entropy 0.5746 | loss -1.27124
| epoch   1 | lr 0.00035 | R 4.93164 | entropy 0.5726 | loss -0.23441
| epoch   1 | lr 0.00035 | R 4.90548 | entropy 0.5729 | loss -1.73334
| epoch   1 | lr 0.00035 | R 4.91515 | entropy 0.5698 | loss -1.20653
| epoch   1 | lr 0.00035 | R 5.02554 | entropy 0.5687 | loss -0.90432
| epoch   1 | lr 0.00035 | R 4.99737 | entropy 0.5648 | loss -1.07526
| epoch   1 | lr 0.00035 | R 4.99768 | entropy 0.5602 | loss -1.59168
| epoch   1 | lr 0.00035 | R 4.80783 | entropy 0.5549 | loss -1.88917
| epoch   1 | lr 0.00035 | R 4.84325 | entropy 0.5526 | loss -1.96683
| epoch   1 | lr 0.00035 | R 4.94168 | entropy 0.5524 | loss -1.22814
| epoch   1 | lr 0.00035 | R 5.00431 | entropy 0.5495 | loss -0.27852
| epoch   1 | lr 0.00035 | R 4.92582 | entropy 0.5524 | loss -1.90842
| epoch   1 | lr 0.00035 | R 4.83020 | entropy 0.5504 | loss -2.77831
| epoch   1 | lr 0.00035 | R 4.93245 | entropy 0.5414 | loss -1.11855
| epoch   1 | lr 0.00035 | R 4.98403 | entropy 0.5359 | loss -0.69003
loss,  
 1.7588
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 1.7588
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 34.6351
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 1.6493
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 67.6431
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 1.6498
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.67 | loss 1.67 | ppl     5.30
loss,  
 16.1662
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.2650
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 46.7937
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.5777
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 77.2054
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 0.7644
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.54 | loss 1.54 | ppl     4.68
loss,  
 31.0042
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.2562
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 62.4835
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.4431
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.57 | loss 1.57 | ppl     4.80
loss,  
 15.7910
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
1.00000e-02 *
  9.8081
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 47.7893
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.2640
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 78.6270
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.3912
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.57 | loss 1.57 | ppl     4.82
loss,  
 29.2806
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.1325
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 59.9430
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.2487
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.51 | loss 1.51 | ppl     4.52
loss,  
 15.0885
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  5.7810
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 45.3466
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.1614
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 75.8683
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.2521
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.52 | loss 1.52 | ppl     4.56
loss,  
 29.5935
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
1.00000e-02 *
  9.2191
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 59.5353
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.1746
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.47 | loss 1.47 | ppl     4.34
loss,  
 14.9444
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  4.1397
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 44.1021
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1158
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 74.3042
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.1853
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   2 | lr 0.10 | raw loss 1.49 | loss 1.49 | ppl     4.42
| epoch   2 | lr 0.00035 | R 4.29829 | entropy 0.5327 | loss -5.15139
| epoch   2 | lr 0.00035 | R 4.40021 | entropy 0.5279 | loss -1.21732
| epoch   2 | lr 0.00035 | R 4.32165 | entropy 0.5264 | loss 0.17348
| epoch   2 | lr 0.00035 | R 4.86845 | entropy 0.5277 | loss 1.13774
| epoch   2 | lr 0.00035 | R 4.99778 | entropy 0.5239 | loss -0.92083
| epoch   2 | lr 0.00035 | R 5.04620 | entropy 0.5246 | loss -0.81436
| epoch   2 | lr 0.00035 | R 4.98194 | entropy 0.5255 | loss -2.02801
| epoch   2 | lr 0.00035 | R 5.07737 | entropy 0.5224 | loss -0.68908
| epoch   2 | lr 0.00035 | R 4.83400 | entropy 0.5175 | loss -3.96904
| epoch   2 | lr 0.00035 | R 5.22709 | entropy 0.5106 | loss -0.56699
| epoch   2 | lr 0.00035 | R 5.06854 | entropy 0.5053 | loss -1.63378
| epoch   2 | lr 0.00035 | R 4.99942 | entropy 0.5016 | loss -2.37234
| epoch   2 | lr 0.00035 | R 4.89391 | entropy 0.4999 | loss -1.46787
| epoch   2 | lr 0.00035 | R 5.09119 | entropy 0.4985 | loss -1.55619
| epoch   2 | lr 0.00035 | R 5.23158 | entropy 0.4983 | loss -0.15158
| epoch   2 | lr 0.00035 | R 5.24553 | entropy 0.4997 | loss -1.62925
| epoch   2 | lr 0.00035 | R 5.21065 | entropy 0.4979 | loss -1.57013
| epoch   2 | lr 0.00035 | R 5.19163 | entropy 0.4934 | loss -1.46574
| epoch   2 | lr 0.00035 | R 5.21214 | entropy 0.4927 | loss -1.45444
| epoch   2 | lr 0.00035 | R 5.10055 | entropy 0.4905 | loss -2.31004
| epoch   2 | lr 0.00035 | R 5.31084 | entropy 0.4854 | loss -1.17354
| epoch   2 | lr 0.00035 | R 5.29658 | entropy 0.4860 | loss -0.99518
| epoch   2 | lr 0.00035 | R 5.32532 | entropy 0.4876 | loss -1.67163
| epoch   2 | lr 0.00035 | R 5.20374 | entropy 0.4873 | loss -2.37217
| epoch   2 | lr 0.00035 | R 5.35272 | entropy 0.4894 | loss -0.53781
| epoch   2 | lr 0.00035 | R 5.24553 | entropy 0.4889 | loss -2.05688
| epoch   2 | lr 0.00035 | R 5.20918 | entropy 0.4867 | loss -2.61633
| epoch   2 | lr 0.00035 | R 5.26918 | entropy 0.4842 | loss -1.19513
| epoch   2 | lr 0.00035 | R 5.39520 | entropy 0.4827 | loss -1.22554
| epoch   2 | lr 0.00035 | R 5.50417 | entropy 0.4819 | loss -0.57766
| epoch   2 | lr 0.00035 | R 5.20771 | entropy 0.4806 | loss -4.21419
| epoch   2 | lr 0.00035 | R 5.43806 | entropy 0.4711 | loss -0.08662
| epoch   2 | lr 0.00035 | R 5.38235 | entropy 0.4670 | loss -2.03087
| epoch   2 | lr 0.00035 | R 5.18253 | entropy 0.4641 | loss -2.95910
| epoch   2 | lr 0.00035 | R 5.39146 | entropy 0.4608 | loss -1.04986
| epoch   2 | lr 0.00035 | R 5.43245 | entropy 0.4611 | loss -0.52749
| epoch   2 | lr 0.00035 | R 5.42951 | entropy 0.4622 | loss -1.63969
| epoch   2 | lr 0.00035 | R 5.43664 | entropy 0.4605 | loss -1.31557
| epoch   2 | lr 0.00035 | R 5.40678 | entropy 0.4596 | loss -1.46265
loss,  
 1.5017
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 1.5017
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 31.1008
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 1.4810
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 60.5789
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 1.4775
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.50 | loss 1.50 | ppl     4.50
loss,  
 14.6442
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.2401
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 43.1006
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.5321
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 71.1193
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 0.7042
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.42 | loss 1.42 | ppl     4.15
loss,  
 28.0560
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.2319
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 57.4617
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.4075
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.43 | loss 1.43 | ppl     4.16
loss,  
 13.9011
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
1.00000e-02 *
  8.6342
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 42.2647
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.2335
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 72.3477
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.3599
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.45 | loss 1.45 | ppl     4.25
loss,  
 27.2432
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.1233
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 55.0991
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.2286
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.37 | loss 1.37 | ppl     3.92
loss,  
 13.3549
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  5.1168
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 39.7818
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.1416
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 65.2438
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.2168
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.30 | loss 1.30 | ppl     3.69
loss,  
 26.4006
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
1.00000e-02 *
  8.2245
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 52.1626
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.1530
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.30 | loss 1.30 | ppl     3.68
loss,  
 14.1751
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  3.9266
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 40.0113
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1050
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 64.5280
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.1609
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   3 | lr 0.10 | raw loss 1.29 | loss 1.29 | ppl     3.63
| epoch   3 | lr 0.00035 | R 6.08174 | entropy 0.4532 | loss -14.31363
| epoch   3 | lr 0.00035 | R 6.29930 | entropy 0.4461 | loss -2.30856
| epoch   3 | lr 0.00035 | R 6.39288 | entropy 0.4458 | loss 0.07760
| epoch   3 | lr 0.00035 | R 7.54706 | entropy 0.4419 | loss 0.10452
| epoch   3 | lr 0.00035 | R 8.18528 | entropy 0.4399 | loss -0.65976
| epoch   3 | lr 0.00035 | R 8.08120 | entropy 0.4413 | loss -4.20613
| epoch   3 | lr 0.00035 | R 8.28198 | entropy 0.4392 | loss -3.74396
| epoch   3 | lr 0.00035 | R 8.53650 | entropy 0.4335 | loss -1.62983
| epoch   3 | lr 0.00035 | R 8.31293 | entropy 0.4338 | loss -3.93029
| epoch   3 | lr 0.00035 | R 8.36642 | entropy 0.4346 | loss -3.51501
| epoch   3 | lr 0.00035 | R 8.35260 | entropy 0.4380 | loss -2.77594
| epoch   3 | lr 0.00035 | R 8.57452 | entropy 0.4392 | loss -2.06188
| epoch   3 | lr 0.00035 | R 8.62721 | entropy 0.4380 | loss -2.88865
| epoch   3 | lr 0.00035 | R 8.77361 | entropy 0.4386 | loss -2.43817
| epoch   3 | lr 0.00035 | R 8.59420 | entropy 0.4417 | loss -3.08969
| epoch   3 | lr 0.00035 | R 8.52587 | entropy 0.4394 | loss -1.91580
| epoch   3 | lr 0.00035 | R 8.74062 | entropy 0.4386 | loss -1.36877
| epoch   3 | lr 0.00035 | R 8.42949 | entropy 0.4381 | loss -4.12255
| epoch   3 | lr 0.00035 | R 8.64610 | entropy 0.4380 | loss -2.22460
| epoch   3 | lr 0.00035 | R 8.66140 | entropy 0.4405 | loss -3.44681
| epoch   3 | lr 0.00035 | R 8.53491 | entropy 0.4395 | loss -3.52198
| epoch   3 | lr 0.00035 | R 8.71383 | entropy 0.4352 | loss -1.95481
| epoch   3 | lr 0.00035 | R 9.04718 | entropy 0.4333 | loss -1.07858
| epoch   3 | lr 0.00035 | R 9.19930 | entropy 0.4340 | loss -0.57299
| epoch   3 | lr 0.00035 | R 8.53774 | entropy 0.4338 | loss -5.16199
| epoch   3 | lr 0.00035 | R 8.42311 | entropy 0.4342 | loss -3.81080
| epoch   3 | lr 0.00035 | R 8.73091 | entropy 0.4363 | loss -2.88636
srun: Force Terminated job 1542031
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: *** STEP 1542031.0 ON BJ-IDC1-10-10-30-223 CANCELLED AT 2018-03-26T12:33:05 ***
srun: error: BJ-IDC1-10-10-30-223: task 0: Terminated
srun: job 1542190 queued and waiting for resources
srun: job 1542190 has been allocated resources
srun: Force Terminated job 1542190
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: *** STEP 1542190.0 ON BJ-IDC1-10-10-30-130 CANCELLED AT 2018-03-26T12:36:41 ***
srun: error: BJ-IDC1-10-10-30-130: task 0: Terminated
[*] Make directories : logs/cifar10_2018-03-26_12-37-28
Files already downloaded and verified
regularizing:
----- begin to init cnn------
defaultdict(<class 'dict'>, {0: {'1x1': Sequential(
  (0): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(3, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 1: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 2: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 3: {'1x1': Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 4: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 5: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 6: {'1x1': Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 7: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 8: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 9: {'1x1': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 10: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 11: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}})
# of parameters: 145,759,370
---- begin to init controller-----
===begin to cuda
cuda
finish cuda
=======make optimizer========
=======make optimizer========
finish init
[*] MODEL dir: logs/cifar10_2018-03-26_12-37-28
[*] PARAM path: logs/cifar10_2018-03-26_12-37-28/params.json
loss,  
 2.3382
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 0 
 2.3382
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 55.7749
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 20 
 2.6559
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 108.2531
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 40 
 2.6403
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.71 | loss 2.71 | ppl    15.02
loss,  
 25.1587
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 60 
 0.4124
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 76.6787
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 80 
 0.9467
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 128.3461
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 100 
 1.2708
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.57 | loss 2.57 | ppl    13.03
loss,  
 50.8794
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 120 
 0.4205
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 99.7636
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 140 
 0.7075
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.49 | loss 2.49 | ppl    12.12
loss,  
 24.5397
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 160 
 0.1524
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 72.6101
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 180 
 0.4012
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 118.7930
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 200 
 0.5910
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.38 | loss 2.38 | ppl    10.76
loss,  
 48.1491
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 220 
 0.2179
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 95.0377
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 240 
 0.3943
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.37 | loss 2.37 | ppl    10.64
loss,  
 22.7656
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 260 
1.00000e-02 *
  8.7225
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 70.0836
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 280 
 0.2494
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 116.5805
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 300 
 0.3873
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.33 | loss 2.33 | ppl    10.29
loss,  
 44.1885
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 320 
 0.1377
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 91.0210
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 340 
 0.2669
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.29 | loss 2.29 | ppl     9.85
loss,  
 21.9506
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 360 
1.00000e-02 *
  6.0805
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 65.7709
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 380 
 0.1726
[torch.cuda.FloatTensor of size 1 (GPU 0)]

loss,  
 111.6047
[torch.cuda.FloatTensor of size 1 (GPU 0)]
 400 
 0.2783
[torch.cuda.FloatTensor of size 1 (GPU 0)]

| epoch   0 | lr 0.10 | raw loss 2.23 | loss 2.23 | ppl     9.32
| epoch   0 | lr 0.00035 | R 1.15304 | entropy 1.5922 | loss -0.04528
| epoch   0 | lr 0.00035 | R 1.21509 | entropy 1.5922 | loss 1.28395
| epoch   0 | lr 0.00035 | R 1.19258 | entropy 1.5921 | loss -0.88667
srun: Force Terminated job 1542192
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: *** STEP 1542192.0 ON BJ-IDC1-10-10-30-130 CANCELLED AT 2018-03-26T12:39:14 ***
srun: error: BJ-IDC1-10-10-30-130: task 0: Terminated
[*] Make directories : logs/cifar10_2018-03-26_12-39-40
Files already downloaded and verified
regularizing:
----- begin to init cnn------
defaultdict(<class 'dict'>, {0: {'1x1': Sequential(
  (0): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(3, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 1: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 2: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 3: {'1x1': Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 4: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 5: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 6: {'1x1': Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 7: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 8: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 9: {'1x1': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 10: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 11: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}})
# of parameters: 145,759,370
---- begin to init controller-----
===begin to cuda
cuda
finish cuda
=======make optimizer========
=======make optimizer========
finish init
[*] MODEL dir: logs/cifar10_2018-03-26_12-39-40
[*] PARAM path: logs/cifar10_2018-03-26_12-39-40/params.json
| epoch   0 | lr 0.10 | raw loss 2.66 | loss 2.66 | ppl    14.32
| epoch   0 | lr 0.10 | raw loss 2.59 | loss 2.59 | ppl    13.35
| epoch   0 | lr 0.10 | raw loss 2.42 | loss 2.42 | ppl    11.25
| epoch   0 | lr 0.10 | raw loss 2.42 | loss 2.42 | ppl    11.19
| epoch   0 | lr 0.10 | raw loss 2.38 | loss 2.38 | ppl    10.79
| epoch   0 | lr 0.10 | raw loss 2.30 | loss 2.30 | ppl    10.01
| epoch   0 | lr 0.10 | raw loss 2.32 | loss 2.32 | ppl    10.14
| epoch   0 | lr 0.10 | raw loss 2.26 | loss 2.26 | ppl     9.54
| epoch   0 | lr 0.00035 | R 0.87524 | entropy 1.5922 | loss 1.60493
| epoch   0 | lr 0.00035 | R 0.91535 | entropy 1.5922 | loss 0.83796
| epoch   0 | lr 0.00035 | R 0.93099 | entropy 1.5921 | loss 0.12703
| epoch   0 | lr 0.00035 | R 1.29167 | entropy 1.5920 | loss 4.67702
| epoch   0 | lr 0.00035 | R 1.37611 | entropy 1.5918 | loss 0.96778
| epoch   0 | lr 0.00035 | R 1.37351 | entropy 1.5915 | loss 0.39436
| epoch   0 | lr 0.00035 | R 1.32282 | entropy 1.5884 | loss -0.76984
| epoch   0 | lr 0.00035 | R 1.42421 | entropy 1.5506 | loss 1.62775
| epoch   0 | lr 0.00035 | R 1.51071 | entropy 1.5077 | loss 0.80814
| epoch   0 | lr 0.00035 | R 1.56589 | entropy 1.4844 | loss 0.37372
| epoch   0 | lr 0.00035 | R 1.61701 | entropy 1.4584 | loss 0.56760
| epoch   0 | lr 0.00035 | R 1.60563 | entropy 1.4398 | loss -0.34221
| epoch   0 | lr 0.00035 | R 1.67668 | entropy 1.4265 | loss 0.35761
| epoch   0 | lr 0.00035 | R 1.73772 | entropy 1.4042 | loss 0.77152
| epoch   0 | lr 0.00035 | R 1.72594 | entropy 1.3781 | loss -0.03462
| epoch   0 | lr 0.00035 | R 1.83181 | entropy 1.3376 | loss 0.46583
| epoch   0 | lr 0.00035 | R 1.82172 | entropy 1.3163 | loss -0.33190
| epoch   0 | lr 0.00035 | R 1.83083 | entropy 1.2852 | loss -0.34991
| epoch   0 | lr 0.00035 | R 1.86921 | entropy 1.2642 | loss 0.28390
| epoch   0 | lr 0.00035 | R 1.89401 | entropy 1.2438 | loss 0.15469
| epoch   0 | lr 0.00035 | R 1.89240 | entropy 1.2162 | loss -1.09072
| epoch   0 | lr 0.00035 | R 1.87520 | entropy 1.2034 | loss -0.21836
| epoch   0 | lr 0.00035 | R 1.95103 | entropy 1.1935 | loss 0.65959
| epoch   0 | lr 0.00035 | R 1.91237 | entropy 1.1770 | loss -0.82367
| epoch   0 | lr 0.00035 | R 1.94107 | entropy 1.1519 | loss -0.37213
| epoch   0 | lr 0.00035 | R 2.00072 | entropy 1.1331 | loss 0.50718
| epoch   0 | lr 0.00035 | R 1.97264 | entropy 1.1169 | loss -0.51077
| epoch   0 | lr 0.00035 | R 1.96675 | entropy 1.0974 | loss -0.98185
| epoch   0 | lr 0.00035 | R 1.97464 | entropy 1.0793 | loss -0.06889
| epoch   0 | lr 0.00035 | R 1.98146 | entropy 1.0687 | loss -0.08454
| epoch   0 | lr 0.00035 | R 2.02338 | entropy 1.0588 | loss 0.22765
| epoch   0 | lr 0.00035 | R 1.99793 | entropy 1.0564 | loss -0.63554
| epoch   0 | lr 0.00035 | R 1.97634 | entropy 1.0464 | loss -0.59569
| epoch   0 | lr 0.00035 | R 2.01162 | entropy 1.0265 | loss -0.52396
| epoch   0 | lr 0.00035 | R 2.03940 | entropy 1.0061 | loss 0.18982
| epoch   0 | lr 0.00035 | R 2.04036 | entropy 1.0014 | loss -0.11623
| epoch   0 | lr 0.00035 | R 2.07105 | entropy 0.9926 | loss -0.36864
| epoch   0 | lr 0.00035 | R 2.05056 | entropy 0.9853 | loss -0.64774
| epoch   0 | lr 0.00035 | R 2.09199 | entropy 0.9645 | loss 0.08685
| epoch   0 | lr 0.10 | raw loss 2.15 | loss 2.15 | ppl     8.55
| epoch   0 | lr 0.10 | raw loss 2.01 | loss 2.01 | ppl     7.48
| epoch   0 | lr 0.10 | raw loss 1.95 | loss 1.95 | ppl     7.05
| epoch   0 | lr 0.10 | raw loss 1.94 | loss 1.94 | ppl     6.99
| epoch   0 | lr 0.10 | raw loss 1.92 | loss 1.92 | ppl     6.79
| epoch   0 | lr 0.10 | raw loss 1.91 | loss 1.91 | ppl     6.75
| epoch   0 | lr 0.10 | raw loss 1.84 | loss 1.84 | ppl     6.30
| epoch   0 | lr 0.10 | raw loss 1.87 | loss 1.87 | ppl     6.50
| epoch   0 | lr 0.00035 | R 2.53504 | entropy 0.9670 | loss -5.99707
| epoch   0 | lr 0.00035 | R 2.57218 | entropy 0.9736 | loss -0.36532
| epoch   0 | lr 0.00035 | R 2.59869 | entropy 0.9582 | loss -0.43908
| epoch   0 | lr 0.00035 | R 3.17595 | entropy 0.9482 | loss 4.31806
| epoch   0 | lr 0.00035 | R 3.20003 | entropy 0.9418 | loss -0.66331
| epoch   0 | lr 0.00035 | R 3.27072 | entropy 0.9410 | loss 0.27630
| epoch   0 | lr 0.00035 | R 3.09796 | entropy 0.9409 | loss -2.08075
| epoch   0 | lr 0.00035 | R 3.28395 | entropy 0.9360 | loss 0.85851
| epoch   0 | lr 0.00035 | R 3.20689 | entropy 0.9303 | loss -1.22666
| epoch   0 | lr 0.00035 | R 3.16484 | entropy 0.9239 | loss -1.29748
| epoch   0 | lr 0.00035 | R 3.18620 | entropy 0.9140 | loss -0.03447
| epoch   0 | lr 0.00035 | R 3.21722 | entropy 0.9122 | loss -0.21369
| epoch   0 | lr 0.00035 | R 3.26383 | entropy 0.9085 | loss -0.53395
| epoch   0 | lr 0.00035 | R 3.24678 | entropy 0.8949 | loss -0.92812
| epoch   0 | lr 0.00035 | R 3.33890 | entropy 0.8833 | loss 0.03669
| epoch   0 | lr 0.00035 | R 3.32636 | entropy 0.8796 | loss -0.98880
| epoch   0 | lr 0.00035 | R 3.30264 | entropy 0.8806 | loss -0.43306
| epoch   0 | lr 0.00035 | R 3.33622 | entropy 0.8753 | loss -0.94595
| epoch   0 | lr 0.00035 | R 3.25918 | entropy 0.8672 | loss -1.31055
| epoch   0 | lr 0.00035 | R 3.31881 | entropy 0.8695 | loss 0.53639
| epoch   0 | lr 0.00035 | R 3.30243 | entropy 0.8732 | loss -0.89449
| epoch   0 | lr 0.00035 | R 3.42144 | entropy 0.8663 | loss -0.18514
| epoch   0 | lr 0.00035 | R 3.33108 | entropy 0.8620 | loss -1.86084
| epoch   0 | lr 0.00035 | R 3.34813 | entropy 0.8542 | loss 0.21161
| epoch   0 | lr 0.00035 | R 3.27401 | entropy 0.8480 | loss -1.78686
| epoch   0 | lr 0.00035 | R 3.35873 | entropy 0.8423 | loss -0.40436
| epoch   0 | lr 0.00035 | R 3.36218 | entropy 0.8331 | loss -0.18700
| epoch   0 | lr 0.00035 | R 3.38833 | entropy 0.8258 | loss -0.54613
| epoch   0 | lr 0.00035 | R 3.43160 | entropy 0.8225 | loss 0.11639
| epoch   0 | lr 0.00035 | R 3.31921 | entropy 0.8134 | loss -2.07096
| epoch   0 | lr 0.00035 | R 3.32850 | entropy 0.8022 | loss -0.68264
| epoch   0 | lr 0.00035 | R 3.41330 | entropy 0.7989 | loss -0.11597
| epoch   0 | lr 0.00035 | R 3.37386 | entropy 0.7964 | loss -0.47443
| epoch   0 | lr 0.00035 | R 3.41771 | entropy 0.8018 | loss 0.11112
| epoch   0 | lr 0.00035 | R 3.42109 | entropy 0.8055 | loss -0.46460
| epoch   0 | lr 0.00035 | R 3.43078 | entropy 0.8006 | loss -0.92603
| epoch   0 | lr 0.00035 | R 3.42584 | entropy 0.7971 | loss -0.24039
| epoch   0 | lr 0.00035 | R 3.41860 | entropy 0.7962 | loss -0.42837
| epoch   0 | lr 0.00035 | R 3.35618 | entropy 0.7945 | loss -1.10223
derive | max_R: 4.479449
Traceback (most recent call last):
  File "main.py", line 45, in <module>
    main(args)
  File "main.py", line 30, in main
    trnr.train()
  File "/mnt/lustre/lianqing/enas/cnn/ENAS-cnn/trainer.py", line 239, in train
    max_num=self.args.batch_size*100)
  File "/mnt/lustre/lianqing/enas/cnn/ENAS-cnn/trainer.py", line 491, in evaluate
    acc.update(utils.get_accuracy(targets, output))
  File "/mnt/lustre/lianqing/enas/cnn/ENAS-cnn/utils.py", line 263, in get_accuracy
    acc = np.sum(np.argmax(targets == outputs, axis =1)) / len(targets)
  File "/mnt/lustre/lianqing/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py", line 963, in argmax
    return _wrapfunc(a, 'argmax', axis=axis, out=out)
  File "/mnt/lustre/lianqing/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py", line 57, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
ValueError: axis(=1) out of bounds
srun: error: BJ-IDC1-10-10-30-130: task 0: Exited with exit code 1
[*] Make directories : logs/cifar10_2018-03-26_13-39-20
Files already downloaded and verified
regularizing:
----- begin to init cnn------
defaultdict(<class 'dict'>, {0: {'1x1': Sequential(
  (0): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(3, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 1: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 2: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 3: {'1x1': Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 4: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 5: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 6: {'1x1': Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 7: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 8: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 9: {'1x1': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 10: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 11: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}})
# of parameters: 145,759,370
---- begin to init controller-----
===begin to cuda
cuda
finish cuda
=======make optimizer========
=======make optimizer========
finish init
[*] MODEL dir: logs/cifar10_2018-03-26_13-39-20
[*] PARAM path: logs/cifar10_2018-03-26_13-39-20/params.json
| epoch   0 | lr 0.10 | raw loss 2.70 | loss 2.70 | ppl    14.87
| epoch   0 | lr 0.10 | raw loss 2.53 | loss 2.53 | ppl    12.61
| epoch   0 | lr 0.10 | raw loss 2.45 | loss 2.45 | ppl    11.56
| epoch   0 | lr 0.10 | raw loss 2.44 | loss 2.44 | ppl    11.49
| epoch   0 | lr 0.10 | raw loss 2.37 | loss 2.37 | ppl    10.65
| epoch   0 | lr 0.10 | raw loss 2.26 | loss 2.26 | ppl     9.62
| epoch   0 | lr 0.10 | raw loss 2.33 | loss 2.33 | ppl    10.31
| epoch   0 | lr 0.10 | raw loss 2.27 | loss 2.27 | ppl     9.70
| epoch   0 | lr 0.00035 | R 0.69542 | entropy 1.5922 | loss 1.92254
| epoch   0 | lr 0.00035 | R 0.68118 | entropy 1.5921 | loss -0.70359
| epoch   0 | lr 0.00035 | R 0.76239 | entropy 1.5920 | loss 2.34550
| epoch   0 | lr 0.00035 | R 0.83896 | entropy 1.5918 | loss 0.65798
| epoch   0 | lr 0.00035 | R 0.92167 | entropy 1.5828 | loss 1.11315
| epoch   0 | lr 0.00035 | R 1.16160 | entropy 1.5126 | loss 2.66325
| epoch   0 | lr 0.00035 | R 1.18177 | entropy 1.4854 | loss 0.09493
| epoch   0 | lr 0.00035 | R 1.22579 | entropy 1.4602 | loss 0.75750
| epoch   0 | lr 0.00035 | R 1.23998 | entropy 1.4317 | loss -0.22455
| epoch   0 | lr 0.00035 | R 1.29958 | entropy 1.4184 | loss 0.64228
| epoch   0 | lr 0.00035 | R 1.30039 | entropy 1.4098 | loss 0.11014
| epoch   0 | lr 0.00035 | R 1.30058 | entropy 1.3916 | loss -0.33851
| epoch   0 | lr 0.00035 | R 1.29936 | entropy 1.3708 | loss -0.41129
| epoch   0 | lr 0.00035 | R 1.30356 | entropy 1.3460 | loss 0.08712
| epoch   0 | lr 0.00035 | R 1.31252 | entropy 1.3216 | loss -0.10351
| epoch   0 | lr 0.00035 | R 1.35818 | entropy 1.3012 | loss -0.00680
| epoch   0 | lr 0.00035 | R 1.36639 | entropy 1.2796 | loss -0.12763
| epoch   0 | lr 0.00035 | R 1.36159 | entropy 1.2560 | loss -0.09398
| epoch   0 | lr 0.00035 | R 1.39946 | entropy 1.2322 | loss 0.20253
| epoch   0 | lr 0.00035 | R 1.37761 | entropy 1.2192 | loss -0.48736
| epoch   0 | lr 0.00035 | R 1.37934 | entropy 1.2049 | loss -0.12480
| epoch   0 | lr 0.00035 | R 1.37019 | entropy 1.2017 | loss -0.54315
| epoch   0 | lr 0.00035 | R 1.40323 | entropy 1.1832 | loss 0.30799
| epoch   0 | lr 0.00035 | R 1.42367 | entropy 1.1690 | loss -0.12236
| epoch   0 | lr 0.00035 | R 1.39482 | entropy 1.1595 | loss -0.46613
| epoch   0 | lr 0.00035 | R 1.43461 | entropy 1.1449 | loss 0.38642
| epoch   0 | lr 0.00035 | R 1.41841 | entropy 1.1371 | loss -0.36249
| epoch   0 | lr 0.00035 | R 1.42418 | entropy 1.1345 | loss -0.12163
| epoch   0 | lr 0.00035 | R 1.43279 | entropy 1.1230 | loss -0.08969
| epoch   0 | lr 0.00035 | R 1.41612 | entropy 1.1248 | loss -0.18263
| epoch   0 | lr 0.00035 | R 1.46878 | entropy 1.1127 | loss 0.11945
| epoch   0 | lr 0.00035 | R 1.42647 | entropy 1.0985 | loss -0.65138
| epoch   0 | lr 0.00035 | R 1.41616 | entropy 1.0782 | loss -0.38792
| epoch   0 | lr 0.00035 | R 1.45507 | entropy 1.0571 | loss -0.24135
| epoch   0 | lr 0.00035 | R 1.45988 | entropy 1.0424 | loss -0.00602
| epoch   0 | lr 0.00035 | R 1.47809 | entropy 1.0318 | loss 0.16718
| epoch   0 | lr 0.00035 | R 1.48004 | entropy 1.0170 | loss -0.25461
| epoch   0 | lr 0.00035 | R 1.49667 | entropy 1.0046 | loss -0.05778
| epoch   0 | lr 0.00035 | R 1.48154 | entropy 0.9954 | loss -0.18156
| epoch   0 | lr 0.10 | raw loss 2.21 | loss 2.21 | ppl     9.12
| epoch   0 | lr 0.10 | raw loss 2.07 | loss 2.07 | ppl     7.93
| epoch   0 | lr 0.10 | raw loss 1.93 | loss 1.93 | ppl     6.88
| epoch   0 | lr 0.10 | raw loss 1.97 | loss 1.97 | ppl     7.19
| epoch   0 | lr 0.10 | raw loss 1.93 | loss 1.93 | ppl     6.87
| epoch   0 | lr 0.10 | raw loss 1.95 | loss 1.95 | ppl     7.04
| epoch   0 | lr 0.10 | raw loss 1.91 | loss 1.91 | ppl     6.76
| epoch   0 | lr 0.10 | raw loss 1.88 | loss 1.88 | ppl     6.55
| epoch   0 | lr 0.00035 | R 2.18183 | entropy 0.9797 | loss -6.58531
| epoch   0 | lr 0.00035 | R 2.22774 | entropy 0.9499 | loss 0.13757
| epoch   0 | lr 0.00035 | R 2.25440 | entropy 0.9377 | loss -0.87797
| epoch   0 | lr 0.00035 | R 2.91285 | entropy 0.9216 | loss 4.24837
| epoch   0 | lr 0.00035 | R 3.00520 | entropy 0.9103 | loss -0.28254
| epoch   0 | lr 0.00035 | R 3.03716 | entropy 0.9000 | loss -0.24123
| epoch   0 | lr 0.00035 | R 2.88021 | entropy 0.8905 | loss -1.76201
| epoch   0 | lr 0.00035 | R 3.02543 | entropy 0.8899 | loss -0.14264
| epoch   0 | lr 0.00035 | R 2.97166 | entropy 0.8812 | loss -0.72424
| epoch   0 | lr 0.00035 | R 3.07715 | entropy 0.8812 | loss -0.53259
| epoch   0 | lr 0.00035 | R 3.06050 | entropy 0.8715 | loss -0.18964
| epoch   0 | lr 0.00035 | R 3.09769 | entropy 0.8648 | loss -0.43521
| epoch   0 | lr 0.00035 | R 3.17639 | entropy 0.8592 | loss -0.30114
| epoch   0 | lr 0.00035 | R 3.05772 | entropy 0.8555 | loss -1.23163
| epoch   0 | lr 0.00035 | R 3.05852 | entropy 0.8550 | loss -0.44093
| epoch   0 | lr 0.00035 | R 3.02545 | entropy 0.8477 | loss -1.45082
| epoch   0 | lr 0.00035 | R 3.01938 | entropy 0.8410 | loss -1.31172
| epoch   0 | lr 0.00035 | R 3.12472 | entropy 0.8341 | loss -0.37293
| epoch   0 | lr 0.00035 | R 3.03200 | entropy 0.8269 | loss -1.19338
| epoch   0 | lr 0.00035 | R 3.10598 | entropy 0.8296 | loss -0.04257
| epoch   0 | lr 0.00035 | R 3.10846 | entropy 0.8213 | loss -1.02224
| epoch   0 | lr 0.00035 | R 3.20189 | entropy 0.8162 | loss -0.69354
| epoch   0 | lr 0.00035 | R 3.17150 | entropy 0.8136 | loss -0.53433
| epoch   0 | lr 0.00035 | R 3.07141 | entropy 0.8029 | loss -1.94155
| epoch   0 | lr 0.00035 | R 3.13904 | entropy 0.7948 | loss -1.02967
| epoch   0 | lr 0.00035 | R 3.15982 | entropy 0.7884 | loss -0.60802
| epoch   0 | lr 0.00035 | R 3.15549 | entropy 0.7839 | loss -0.50085
| epoch   0 | lr 0.00035 | R 3.17735 | entropy 0.7785 | loss -1.00547
| epoch   0 | lr 0.00035 | R 3.28225 | entropy 0.7754 | loss 0.53998
| epoch   0 | lr 0.00035 | R 3.12448 | entropy 0.7688 | loss -1.80427
| epoch   0 | lr 0.00035 | R 3.21349 | entropy 0.7623 | loss -0.30792
| epoch   0 | lr 0.00035 | R 3.11884 | entropy 0.7600 | loss -1.55380
| epoch   0 | lr 0.00035 | R 3.19996 | entropy 0.7585 | loss -0.02212
| epoch   0 | lr 0.00035 | R 3.23884 | entropy 0.7579 | loss 0.03203
| epoch   0 | lr 0.00035 | R 3.15187 | entropy 0.7539 | loss -0.93520
| epoch   0 | lr 0.00035 | R 3.31253 | entropy 0.7541 | loss 0.29406
| epoch   0 | lr 0.00035 | R 3.29652 | entropy 0.7505 | loss -0.84922
| epoch   0 | lr 0.00035 | R 3.33552 | entropy 0.7458 | loss -0.41727
| epoch   0 | lr 0.00035 | R 3.19055 | entropy 0.7434 | loss -1.57115
derive | max_R: 4.675821
========> finish evaluate on one epoch<======
eval | loss:   112.41 | ppl: 6601980957689103395543829722409199218529658732544.00 | accuracy:     0.35
[*] SAVED: logs/cifar10_2018-03-26_13-39-20/shared_epoch0_step802.pth
[*] SAVED: logs/cifar10_2018-03-26_13-39-20/controller_epoch0_step4000.pth
derive | max_R: 4.325463
========> finish evaluate on one epoch<======
eval | loss:   107.08 | ppl: 31945956557155107674778865011651300547673194496.00 | accuracy:     0.38
[*] SAVED: logs/cifar10_2018-03-26_13-39-20/shared_epoch0_step802.pth
[*] SAVED: logs/cifar10_2018-03-26_13-39-20/controller_epoch0_step4000.pth
| epoch   1 | lr 0.10 | raw loss 1.78 | loss 1.78 | ppl     5.95
| epoch   1 | lr 0.10 | raw loss 1.74 | loss 1.74 | ppl     5.72
| epoch   1 | lr 0.10 | raw loss 1.75 | loss 1.75 | ppl     5.78
| epoch   1 | lr 0.10 | raw loss 1.74 | loss 1.74 | ppl     5.67
| epoch   1 | lr 0.10 | raw loss 1.69 | loss 1.69 | ppl     5.39
| epoch   1 | lr 0.10 | raw loss 1.69 | loss 1.69 | ppl     5.41
| epoch   1 | lr 0.10 | raw loss 1.65 | loss 1.65 | ppl     5.20
| epoch   1 | lr 0.10 | raw loss 1.66 | loss 1.66 | ppl     5.28
| epoch   1 | lr 0.00035 | R 3.29034 | entropy 0.7280 | loss -10.35131
| epoch   1 | lr 0.00035 | R 3.29875 | entropy 0.7172 | loss -1.82417
| epoch   1 | lr 0.00035 | R 3.11520 | entropy 0.7122 | loss -0.69980
| epoch   1 | lr 0.00035 | R 4.24651 | entropy 0.7012 | loss 4.62366
| epoch   1 | lr 0.00035 | R 4.30669 | entropy 0.6944 | loss -1.90156
| epoch   1 | lr 0.00035 | R 4.41391 | entropy 0.6880 | loss 0.42131
| epoch   1 | lr 0.00035 | R 4.35370 | entropy 0.6883 | loss -2.06909
| epoch   1 | lr 0.00035 | R 4.46300 | entropy 0.6823 | loss -0.83357
| epoch   1 | lr 0.00035 | R 4.47876 | entropy 0.6732 | loss -1.13190
| epoch   1 | lr 0.00035 | R 4.27684 | entropy 0.6639 | loss -3.94594
| epoch   1 | lr 0.00035 | R 4.36704 | entropy 0.6606 | loss -1.29446
| epoch   1 | lr 0.00035 | R 4.50485 | entropy 0.6577 | loss -0.24961
| epoch   1 | lr 0.00035 | R 4.45218 | entropy 0.6520 | loss -1.63652
| epoch   1 | lr 0.00035 | R 4.43961 | entropy 0.6460 | loss -3.69199
| epoch   1 | lr 0.00035 | R 4.47363 | entropy 0.6403 | loss -2.38810
| epoch   1 | lr 0.00035 | R 4.57858 | entropy 0.6363 | loss 0.17843
| epoch   1 | lr 0.00035 | R 4.35592 | entropy 0.6321 | loss -2.53600
| epoch   1 | lr 0.00035 | R 4.37496 | entropy 0.6323 | loss -3.38102
| epoch   1 | lr 0.00035 | R 4.58405 | entropy 0.6307 | loss -0.47178
| epoch   1 | lr 0.00035 | R 4.29827 | entropy 0.6256 | loss -4.05309
| epoch   1 | lr 0.00035 | R 4.66924 | entropy 0.6248 | loss 1.12771
| epoch   1 | lr 0.00035 | R 4.74113 | entropy 0.6249 | loss -0.69046
| epoch   1 | lr 0.00035 | R 4.58645 | entropy 0.6192 | loss -2.45738
| epoch   1 | lr 0.00035 | R 4.47328 | entropy 0.6101 | loss -3.67159
| epoch   1 | lr 0.00035 | R 4.59710 | entropy 0.6041 | loss -1.16803
| epoch   1 | lr 0.00035 | R 4.71373 | entropy 0.6060 | loss -0.05836
| epoch   1 | lr 0.00035 | R 4.66169 | entropy 0.6040 | loss -2.90249
| epoch   1 | lr 0.00035 | R 4.67830 | entropy 0.5960 | loss -2.11854
| epoch   1 | lr 0.00035 | R 4.88357 | entropy 0.5899 | loss 0.06164
| epoch   1 | lr 0.00035 | R 4.66710 | entropy 0.5914 | loss -1.88234
| epoch   1 | lr 0.00035 | R 4.84558 | entropy 0.5892 | loss -0.99775
| epoch   1 | lr 0.00035 | R 4.50316 | entropy 0.5850 | loss -3.45686
| epoch   1 | lr 0.00035 | R 4.62414 | entropy 0.5873 | loss -1.26590
| epoch   1 | lr 0.00035 | R 4.76285 | entropy 0.5827 | loss -1.18733
| epoch   1 | lr 0.00035 | R 4.71520 | entropy 0.5800 | loss -1.13248
| epoch   1 | lr 0.00035 | R 4.78746 | entropy 0.5755 | loss -0.86016
| epoch   1 | lr 0.00035 | R 4.63853 | entropy 0.5752 | loss -3.58924
| epoch   1 | lr 0.00035 | R 4.65717 | entropy 0.5802 | loss -1.11384
| epoch   1 | lr 0.00035 | R 4.84261 | entropy 0.5812 | loss -0.95247
| epoch   2 | lr 0.10 | raw loss 1.64 | loss 1.64 | ppl     5.17
| epoch   2 | lr 0.10 | raw loss 1.54 | loss 1.54 | ppl     4.65
| epoch   2 | lr 0.10 | raw loss 1.53 | loss 1.53 | ppl     4.63
| epoch   2 | lr 0.10 | raw loss 1.53 | loss 1.53 | ppl     4.63
| epoch   2 | lr 0.10 | raw loss 1.45 | loss 1.45 | ppl     4.28
| epoch   2 | lr 0.10 | raw loss 1.46 | loss 1.46 | ppl     4.33
| epoch   2 | lr 0.10 | raw loss 1.43 | loss 1.43 | ppl     4.20
| epoch   2 | lr 0.10 | raw loss 1.50 | loss 1.50 | ppl     4.48
| epoch   2 | lr 0.00035 | R 4.50455 | entropy 0.5763 | loss -6.23084
| epoch   2 | lr 0.00035 | R 4.30715 | entropy 0.5738 | loss -3.93591
| epoch   2 | lr 0.00035 | R 4.58380 | entropy 0.5690 | loss 1.36684
| epoch   2 | lr 0.00035 | R 5.13288 | entropy 0.5664 | loss 1.05826
| epoch   2 | lr 0.00035 | R 5.50301 | entropy 0.5659 | loss -1.21079
| epoch   2 | lr 0.00035 | R 5.63312 | entropy 0.5641 | loss 0.22014
| epoch   2 | lr 0.00035 | R 5.55326 | entropy 0.5605 | loss -1.96418
| epoch   2 | lr 0.00035 | R 5.54800 | entropy 0.5557 | loss -1.87686
| epoch   2 | lr 0.00035 | R 5.49485 | entropy 0.5546 | loss -1.99713
| epoch   2 | lr 0.00035 | R 5.56025 | entropy 0.5589 | loss -3.35743
| epoch   2 | lr 0.00035 | R 5.20458 | entropy 0.5549 | loss -4.62946
| epoch   2 | lr 0.00035 | R 5.37000 | entropy 0.5514 | loss -1.06767
| epoch   2 | lr 0.00035 | R 5.47844 | entropy 0.5575 | loss -0.92129
| epoch   2 | lr 0.00035 | R 5.49056 | entropy 0.5610 | loss -2.50426
| epoch   2 | lr 0.00035 | R 5.64185 | entropy 0.5582 | loss -0.41507
| epoch   2 | lr 0.00035 | R 5.60822 | entropy 0.5590 | loss -1.96816
| epoch   2 | lr 0.00035 | R 5.68315 | entropy 0.5585 | loss -1.32879
| epoch   2 | lr 0.00035 | R 5.58459 | entropy 0.5546 | loss -3.41181
| epoch   2 | lr 0.00035 | R 5.89894 | entropy 0.5497 | loss 0.59151
| epoch   2 | lr 0.00035 | R 5.50147 | entropy 0.5469 | loss -4.05647
| epoch   2 | lr 0.00035 | R 5.64412 | entropy 0.5493 | loss -2.23460
| epoch   2 | lr 0.00035 | R 5.60910 | entropy 0.5491 | loss -1.25814
| epoch   2 | lr 0.00035 | R 5.75152 | entropy 0.5504 | loss -0.59515
| epoch   2 | lr 0.00035 | R 5.81029 | entropy 0.5518 | loss -0.61985
| epoch   2 | lr 0.00035 | R 5.55364 | entropy 0.5492 | loss -3.84843
| epoch   2 | lr 0.00035 | R 5.62942 | entropy 0.5474 | loss -1.91292
| epoch   2 | lr 0.00035 | R 5.54298 | entropy 0.5480 | loss -3.01105
| epoch   2 | lr 0.00035 | R 5.51619 | entropy 0.5484 | loss -2.47759
| epoch   2 | lr 0.00035 | R 5.69268 | entropy 0.5475 | loss -1.86484
| epoch   2 | lr 0.00035 | R 5.64741 | entropy 0.5472 | loss -1.48776
| epoch   2 | lr 0.00035 | R 5.62501 | entropy 0.5435 | loss -2.54895
| epoch   2 | lr 0.00035 | R 5.71340 | entropy 0.5422 | loss -1.06356
| epoch   2 | lr 0.00035 | R 5.70201 | entropy 0.5414 | loss -2.75886
| epoch   2 | lr 0.00035 | R 5.45192 | entropy 0.5401 | loss -3.81285
| epoch   2 | lr 0.00035 | R 5.68011 | entropy 0.5400 | loss -1.47170
| epoch   2 | lr 0.00035 | R 5.73274 | entropy 0.5354 | loss -1.98524
| epoch   2 | lr 0.00035 | R 5.91233 | entropy 0.5334 | loss -1.02656
| epoch   2 | lr 0.00035 | R 5.65952 | entropy 0.5305 | loss -2.28523
| epoch   2 | lr 0.00035 | R 5.75588 | entropy 0.5293 | loss -1.53323
| epoch   3 | lr 0.10 | raw loss 1.40 | loss 1.40 | ppl     4.07
| epoch   3 | lr 0.10 | raw loss 1.37 | loss 1.37 | ppl     3.92
| epoch   3 | lr 0.10 | raw loss 1.33 | loss 1.33 | ppl     3.77
| epoch   3 | lr 0.10 | raw loss 1.37 | loss 1.37 | ppl     3.94
| epoch   3 | lr 0.10 | raw loss 1.36 | loss 1.36 | ppl     3.91
| epoch   3 | lr 0.10 | raw loss 1.27 | loss 1.27 | ppl     3.57
| epoch   3 | lr 0.10 | raw loss 1.29 | loss 1.29 | ppl     3.63
| epoch   3 | lr 0.10 | raw loss 1.23 | loss 1.23 | ppl     3.42
| epoch   3 | lr 0.00035 | R 7.00041 | entropy 0.5334 | loss -15.04770
| epoch   3 | lr 0.00035 | R 7.00755 | entropy 0.5313 | loss -2.45911
| epoch   3 | lr 0.00035 | R 6.80536 | entropy 0.5334 | loss -1.57169
| epoch   3 | lr 0.00035 | R 9.34130 | entropy 0.5310 | loss 9.96071
| epoch   3 | lr 0.00035 | R 8.99377 | entropy 0.5316 | loss -6.61297
| epoch   3 | lr 0.00035 | R 9.90209 | entropy 0.5355 | loss -0.13334
| epoch   3 | lr 0.00035 | R 9.77931 | entropy 0.5337 | loss -3.79887
| epoch   3 | lr 0.00035 | R 9.95732 | entropy 0.5342 | loss -1.19929
| epoch   3 | lr 0.00035 | R 9.58047 | entropy 0.5326 | loss -7.03021
| epoch   3 | lr 0.00035 | R 9.74005 | entropy 0.5305 | loss -2.98126
srun: Force Terminated job 1542217
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: *** STEP 1542217.0 ON BJ-IDC1-10-10-30-130 CANCELLED AT 2018-03-26T15:09:53 ***
srun: error: BJ-IDC1-10-10-30-130: task 0: Terminated
[*] Make directories : logs/cifar10_2018-03-26_15-10-32
Files already downloaded and verified
regularizing:
----- begin to init cnn------
defaultdict(<class 'dict'>, {0: {'1x1': Sequential(
  (0): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(3, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 1: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 2: {'1x1': Sequential(
  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 3: {'1x1': Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 4: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 5: {'1x1': Sequential(
  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 6: {'1x1': Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 7: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 8: {'1x1': Sequential(
  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 9: {'1x1': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 10: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}, 11: {'1x1': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '3x3': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '5x5': Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
), '1x3x1': Sequential(
  (0): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
    (1): Conv2d(1024, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
  )
  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
  (2): ReLU(inplace)
)}})
# of parameters: 145,759,370
---- begin to init controller-----
===begin to cuda
cuda
finish cuda
=======make optimizer========
=======make optimizer========
finish init
[*] MODEL dir: logs/cifar10_2018-03-26_15-10-32
[*] PARAM path: logs/cifar10_2018-03-26_15-10-32/params.json
| epoch   0 | lr 0.10 | raw loss 2.66 | loss 2.66 | ppl    14.34
| epoch   0 | lr 0.10 | raw loss 2.59 | loss 2.59 | ppl    13.32
| epoch   0 | lr 0.10 | raw loss 2.49 | loss 2.49 | ppl    12.09
| epoch   0 | lr 0.10 | raw loss 2.44 | loss 2.44 | ppl    11.51
| epoch   0 | lr 0.10 | raw loss 2.38 | loss 2.38 | ppl    10.80
| epoch   0 | lr 0.10 | raw loss 2.30 | loss 2.30 | ppl    10.00
| epoch   0 | lr 0.10 | raw loss 2.28 | loss 2.28 | ppl     9.78
| epoch   0 | lr 0.10 | raw loss 2.28 | loss 2.28 | ppl     9.82
| epoch   0 | lr 0.10 | raw loss 2.21 | loss 2.21 | ppl     9.10
| epoch   0 | lr 0.10 | raw loss 2.26 | loss 2.26 | ppl     9.55
| epoch   0 | lr 0.10 | raw loss 2.22 | loss 2.22 | ppl     9.23
| epoch   0 | lr 0.10 | raw loss 2.22 | loss 2.22 | ppl     9.21
| epoch   0 | lr 0.10 | raw loss 2.17 | loss 2.17 | ppl     8.77
| epoch   0 | lr 0.10 | raw loss 2.18 | loss 2.18 | ppl     8.83
| epoch   0 | lr 0.10 | raw loss 2.18 | loss 2.18 | ppl     8.83
====>train_shared<====== finish one epoch
| epoch   0 | lr 0.00035 | R 0.53345 | entropy 1.5922 | loss -3.49292
| epoch   0 | lr 0.00035 | R 0.52923 | entropy 1.5922 | loss -0.11095
| epoch   0 | lr 0.00035 | R 0.56925 | entropy 1.5922 | loss 0.63197
| epoch   0 | lr 0.00035 | R 0.82438 | entropy 1.5921 | loss 3.86980
| epoch   0 | lr 0.00035 | R 0.87215 | entropy 1.5918 | loss 0.51795
| epoch   0 | lr 0.00035 | R 0.99746 | entropy 1.5649 | loss 1.42285
| epoch   0 | lr 0.00035 | R 1.02196 | entropy 1.5046 | loss -0.24685
| epoch   0 | lr 0.00035 | R 1.10115 | entropy 1.4461 | loss 1.26681
| epoch   0 | lr 0.00035 | R 1.17443 | entropy 1.4080 | loss 0.71829
| epoch   0 | lr 0.00035 | R 1.16310 | entropy 1.3624 | loss -0.48581
| epoch   0 | lr 0.00035 | R 1.15695 | entropy 1.3458 | loss 0.39664
| epoch   0 | lr 0.00035 | R 1.18770 | entropy 1.3328 | loss -0.27339
| epoch   0 | lr 0.00035 | R 1.22233 | entropy 1.3073 | loss 0.20023
| epoch   0 | lr 0.00035 | R 1.22976 | entropy 1.2851 | loss 0.02006
| epoch   0 | lr 0.00035 | R 1.23896 | entropy 1.2644 | loss -0.08648
| epoch   0 | lr 0.00035 | R 1.27246 | entropy 1.2561 | loss 0.06801
| epoch   0 | lr 0.00035 | R 1.25372 | entropy 1.2493 | loss -0.49303
| epoch   0 | lr 0.00035 | R 1.28320 | entropy 1.2332 | loss 0.40931
| epoch   0 | lr 0.00035 | R 1.28101 | entropy 1.2142 | loss -0.48756
| epoch   0 | lr 0.00035 | R 1.27068 | entropy 1.1864 | loss -0.05181
| epoch   0 | lr 0.00035 | R 1.30309 | entropy 1.1659 | loss 0.11759
| epoch   0 | lr 0.00035 | R 1.29328 | entropy 1.1521 | loss -0.35660
| epoch   0 | lr 0.00035 | R 1.30969 | entropy 1.1321 | loss 0.00922
| epoch   0 | lr 0.00035 | R 1.33451 | entropy 1.1263 | loss -0.13572
| epoch   0 | lr 0.00035 | R 1.33928 | entropy 1.1122 | loss -0.21258
| epoch   0 | lr 0.00035 | R 1.34945 | entropy 1.0989 | loss 0.13181
| epoch   0 | lr 0.00035 | R 1.35097 | entropy 1.0800 | loss -0.38942
| epoch   0 | lr 0.00035 | R 1.38823 | entropy 1.0627 | loss 0.01068
| epoch   0 | lr 0.00035 | R 1.40614 | entropy 1.0548 | loss 0.09908
| epoch   0 | lr 0.00035 | R 1.39222 | entropy 1.0424 | loss -0.30890
| epoch   0 | lr 0.00035 | R 1.40107 | entropy 1.0301 | loss -0.22580
| epoch   0 | lr 0.00035 | R 1.41105 | entropy 1.0143 | loss -0.01289
| epoch   0 | lr 0.00035 | R 1.40975 | entropy 0.9966 | loss -0.36065
| epoch   0 | lr 0.00035 | R 1.40236 | entropy 0.9816 | loss -0.09716
| epoch   0 | lr 0.00035 | R 1.42169 | entropy 0.9776 | loss -0.01130
| epoch   0 | lr 0.00035 | R 1.42139 | entropy 0.9628 | loss -0.51259
| epoch   0 | lr 0.00035 | R 1.45963 | entropy 0.9495 | loss 0.22343
| epoch   0 | lr 0.00035 | R 1.44354 | entropy 0.9400 | loss -0.40263
| epoch   0 | lr 0.00035 | R 1.45130 | entropy 0.9323 | loss -0.13553
| epoch   0 | lr 0.10 | raw loss 2.08 | loss 2.08 | ppl     8.02
| epoch   0 | lr 0.10 | raw loss 1.99 | loss 1.99 | ppl     7.33
| epoch   0 | lr 0.10 | raw loss 1.94 | loss 1.94 | ppl     6.98
| epoch   0 | lr 0.10 | raw loss 1.90 | loss 1.90 | ppl     6.71
| epoch   0 | lr 0.10 | raw loss 1.91 | loss 1.91 | ppl     6.75
| epoch   0 | lr 0.10 | raw loss 1.87 | loss 1.87 | ppl     6.52
| epoch   0 | lr 0.10 | raw loss 1.85 | loss 1.85 | ppl     6.34
| epoch   0 | lr 0.10 | raw loss 1.80 | loss 1.80 | ppl     6.05
| epoch   0 | lr 0.10 | raw loss 1.82 | loss 1.82 | ppl     6.19
| epoch   0 | lr 0.10 | raw loss 1.80 | loss 1.80 | ppl     6.05
| epoch   0 | lr 0.10 | raw loss 1.78 | loss 1.78 | ppl     5.91
| epoch   0 | lr 0.10 | raw loss 1.77 | loss 1.77 | ppl     5.84
| epoch   0 | lr 0.10 | raw loss 1.75 | loss 1.75 | ppl     5.74
| epoch   0 | lr 0.10 | raw loss 1.74 | loss 1.74 | ppl     5.67
| epoch   0 | lr 0.10 | raw loss 1.69 | loss 1.69 | ppl     5.40
====>train_shared<====== finish one epoch
| epoch   0 | lr 0.00035 | R 1.30254 | entropy 0.8948 | loss 0.20326
| epoch   0 | lr 0.00035 | R 1.13659 | entropy 0.8543 | loss -1.60618
| epoch   0 | lr 0.00035 | R 1.25755 | entropy 0.8345 | loss 0.51753
| epoch   0 | lr 0.00035 | R 1.95193 | entropy 0.8274 | loss 4.89050
| epoch   0 | lr 0.00035 | R 2.04956 | entropy 0.8106 | loss -0.76292
| epoch   0 | lr 0.00035 | R 2.05465 | entropy 0.8077 | loss -0.32214
| epoch   0 | lr 0.00035 | R 2.14374 | entropy 0.7948 | loss -0.44977
| epoch   0 | lr 0.00035 | R 2.10178 | entropy 0.7749 | loss -0.76692
| epoch   0 | lr 0.00035 | R 2.07252 | entropy 0.7616 | loss -1.69511
| epoch   0 | lr 0.00035 | R 2.20705 | entropy 0.7508 | loss 0.29904
| epoch   0 | lr 0.00035 | R 2.30309 | entropy 0.7463 | loss 0.15343
| epoch   0 | lr 0.00035 | R 2.22263 | entropy 0.7434 | loss -1.26530
| epoch   0 | lr 0.00035 | R 2.33054 | entropy 0.7346 | loss 0.11737
| epoch   0 | lr 0.00035 | R 2.32595 | entropy 0.7297 | loss -1.02057
| epoch   0 | lr 0.00035 | R 2.39633 | entropy 0.7182 | loss -0.08356
| epoch   0 | lr 0.00035 | R 2.29163 | entropy 0.7061 | loss -2.20077
| epoch   0 | lr 0.00035 | R 2.36772 | entropy 0.6989 | loss -0.46932
| epoch   0 | lr 0.00035 | R 2.44168 | entropy 0.6913 | loss -0.51604
| epoch   0 | lr 0.00035 | R 2.46811 | entropy 0.6844 | loss -0.47894
| epoch   0 | lr 0.00035 | R 2.34048 | entropy 0.6801 | loss -1.64356
| epoch   0 | lr 0.00035 | R 2.53578 | entropy 0.6749 | loss 0.04823
| epoch   0 | lr 0.00035 | R 2.53633 | entropy 0.6737 | loss -0.57463
| epoch   0 | lr 0.00035 | R 2.49623 | entropy 0.6735 | loss -0.60004
| epoch   0 | lr 0.00035 | R 2.56893 | entropy 0.6703 | loss -0.32540
| epoch   0 | lr 0.00035 | R 2.40411 | entropy 0.6685 | loss -2.27002
| epoch   0 | lr 0.00035 | R 2.52313 | entropy 0.6639 | loss 0.07445
| epoch   0 | lr 0.00035 | R 2.48997 | entropy 0.6540 | loss -1.15970
| epoch   0 | lr 0.00035 | R 2.52768 | entropy 0.6483 | loss -0.46908
| epoch   0 | lr 0.00035 | R 2.56932 | entropy 0.6462 | loss -1.23991
| epoch   0 | lr 0.00035 | R 2.67091 | entropy 0.6432 | loss 0.04083
| epoch   0 | lr 0.00035 | R 2.65956 | entropy 0.6413 | loss -1.59978
| epoch   0 | lr 0.00035 | R 2.68904 | entropy 0.6324 | loss -0.99226
| epoch   0 | lr 0.00035 | R 2.63406 | entropy 0.6300 | loss -0.79546
| epoch   0 | lr 0.00035 | R 2.67744 | entropy 0.6277 | loss -1.01018
| epoch   0 | lr 0.00035 | R 2.68512 | entropy 0.6232 | loss -0.99231
| epoch   0 | lr 0.00035 | R 2.76115 | entropy 0.6193 | loss -0.21767
| epoch   0 | lr 0.00035 | R 2.72013 | entropy 0.6183 | loss -0.98609
| epoch   0 | lr 0.00035 | R 2.77308 | entropy 0.6126 | loss -0.48500
| epoch   0 | lr 0.00035 | R 2.77358 | entropy 0.6088 | loss -1.45003
derive | max_R: 3.220902
========> finish evaluate on one epoch<======
eval | loss:   122.57 | ppl: 169731942522168774320993556519785548835440968769470464.00 | accuracy:     0.29
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch0_step1564.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch0_step4000.pth
derive | max_R: 3.194705
========> finish evaluate on one epoch<======
eval | loss:   122.72 | ppl: 198579069102275959159767951032282499306903827793313792.00 | accuracy:     0.31
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch0_step1564.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch0_step4000.pth
| epoch   1 | lr 0.10 | raw loss 1.67 | loss 1.67 | ppl     5.33
| epoch   1 | lr 0.10 | raw loss 1.61 | loss 1.61 | ppl     4.98
| epoch   1 | lr 0.10 | raw loss 1.57 | loss 1.57 | ppl     4.83
| epoch   1 | lr 0.10 | raw loss 1.53 | loss 1.53 | ppl     4.61
| epoch   1 | lr 0.10 | raw loss 1.48 | loss 1.48 | ppl     4.41
| epoch   1 | lr 0.10 | raw loss 1.48 | loss 1.48 | ppl     4.37
| epoch   1 | lr 0.10 | raw loss 1.46 | loss 1.46 | ppl     4.29
| epoch   1 | lr 0.10 | raw loss 1.48 | loss 1.48 | ppl     4.40
| epoch   1 | lr 0.10 | raw loss 1.46 | loss 1.46 | ppl     4.31
| epoch   1 | lr 0.10 | raw loss 1.44 | loss 1.44 | ppl     4.20
| epoch   1 | lr 0.10 | raw loss 1.40 | loss 1.40 | ppl     4.07
| epoch   1 | lr 0.10 | raw loss 1.34 | loss 1.34 | ppl     3.83
| epoch   1 | lr 0.10 | raw loss 1.37 | loss 1.37 | ppl     3.92
| epoch   1 | lr 0.10 | raw loss 1.31 | loss 1.31 | ppl     3.69
| epoch   1 | lr 0.10 | raw loss 1.30 | loss 1.30 | ppl     3.67
====>train_shared<====== finish one epoch
| epoch   1 | lr 0.00035 | R 3.78319 | entropy 0.6051 | loss -11.95043
| epoch   1 | lr 0.00035 | R 3.83602 | entropy 0.6062 | loss -1.43547
| epoch   1 | lr 0.00035 | R 4.07358 | entropy 0.5986 | loss 0.39852
| epoch   1 | lr 0.00035 | R 5.50742 | entropy 0.5906 | loss 5.70236
| epoch   1 | lr 0.00035 | R 5.64642 | entropy 0.5766 | loss -1.95144
| epoch   1 | lr 0.00035 | R 5.67233 | entropy 0.5669 | loss -1.80913
| epoch   1 | lr 0.00035 | R 5.80130 | entropy 0.5593 | loss -1.87731
| epoch   1 | lr 0.00035 | R 5.76450 | entropy 0.5522 | loss -2.53089
| epoch   1 | lr 0.00035 | R 5.77528 | entropy 0.5459 | loss -3.90195
| epoch   1 | lr 0.00035 | R 5.98924 | entropy 0.5425 | loss -0.29006
| epoch   1 | lr 0.00035 | R 5.95047 | entropy 0.5379 | loss -2.59075
| epoch   1 | lr 0.00035 | R 6.20682 | entropy 0.5332 | loss -1.33922
| epoch   1 | lr 0.00035 | R 6.28973 | entropy 0.5277 | loss -1.71251
| epoch   1 | lr 0.00035 | R 6.09146 | entropy 0.5252 | loss -5.56408
| epoch   1 | lr 0.00035 | R 6.06951 | entropy 0.5253 | loss -2.48452
| epoch   1 | lr 0.00035 | R 6.45184 | entropy 0.5250 | loss -0.62352
| epoch   1 | lr 0.00035 | R 6.38430 | entropy 0.5201 | loss -2.33483
| epoch   1 | lr 0.00035 | R 5.99272 | entropy 0.5199 | loss -5.40541
| epoch   1 | lr 0.00035 | R 6.26022 | entropy 0.5186 | loss -1.80492
| epoch   1 | lr 0.00035 | R 6.27337 | entropy 0.5174 | loss -1.94986
| epoch   1 | lr 0.00035 | R 6.34492 | entropy 0.5157 | loss -2.45130
| epoch   1 | lr 0.00035 | R 6.44024 | entropy 0.5120 | loss -2.67203
| epoch   1 | lr 0.00035 | R 6.35667 | entropy 0.5086 | loss -2.45077
| epoch   1 | lr 0.00035 | R 6.25745 | entropy 0.5069 | loss -4.06064
| epoch   1 | lr 0.00035 | R 6.10896 | entropy 0.5045 | loss -6.31254
| epoch   1 | lr 0.00035 | R 6.46857 | entropy 0.5031 | loss 0.55623
| epoch   1 | lr 0.00035 | R 6.29833 | entropy 0.5006 | loss -4.32581
| epoch   1 | lr 0.00035 | R 6.47179 | entropy 0.4964 | loss -2.33740
| epoch   1 | lr 0.00035 | R 6.48811 | entropy 0.4894 | loss -2.37231
| epoch   1 | lr 0.00035 | R 6.36770 | entropy 0.4865 | loss -4.08921
| epoch   1 | lr 0.00035 | R 6.60043 | entropy 0.4795 | loss -1.53015
| epoch   1 | lr 0.00035 | R 6.66315 | entropy 0.4765 | loss -1.83518
| epoch   1 | lr 0.00035 | R 6.61885 | entropy 0.4756 | loss -2.51191
| epoch   1 | lr 0.00035 | R 6.64165 | entropy 0.4702 | loss -3.01452
| epoch   1 | lr 0.00035 | R 6.64665 | entropy 0.4659 | loss -4.10779
| epoch   1 | lr 0.00035 | R 6.39851 | entropy 0.4602 | loss -3.89436
| epoch   1 | lr 0.00035 | R 6.50158 | entropy 0.4527 | loss -4.33793
| epoch   1 | lr 0.00035 | R 6.76482 | entropy 0.4468 | loss 0.12143
| epoch   1 | lr 0.00035 | R 6.95394 | entropy 0.4455 | loss -1.24641
derive | max_R: 8.861788
========> finish evaluate on one epoch<======
eval | loss:    94.61 | ppl: 122258748734342560309862235533064563326976.00 | accuracy:     0.47
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch1_step2346.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch1_step6000.pth
| epoch   2 | lr 0.10 | raw loss 1.30 | loss 1.30 | ppl     3.68
| epoch   2 | lr 0.10 | raw loss 1.22 | loss 1.22 | ppl     3.40
| epoch   2 | lr 0.10 | raw loss 1.24 | loss 1.24 | ppl     3.45
| epoch   2 | lr 0.10 | raw loss 1.20 | loss 1.20 | ppl     3.33
| epoch   2 | lr 0.10 | raw loss 1.17 | loss 1.17 | ppl     3.21
| epoch   2 | lr 0.10 | raw loss 1.16 | loss 1.16 | ppl     3.20
| epoch   2 | lr 0.10 | raw loss 1.11 | loss 1.11 | ppl     3.03
| epoch   2 | lr 0.10 | raw loss 1.08 | loss 1.08 | ppl     2.95
| epoch   2 | lr 0.10 | raw loss 1.08 | loss 1.08 | ppl     2.96
| epoch   2 | lr 0.10 | raw loss 1.12 | loss 1.12 | ppl     3.05
| epoch   2 | lr 0.10 | raw loss 1.06 | loss 1.06 | ppl     2.88
| epoch   2 | lr 0.10 | raw loss 1.08 | loss 1.08 | ppl     2.95
| epoch   2 | lr 0.10 | raw loss 1.05 | loss 1.05 | ppl     2.86
| epoch   2 | lr 0.10 | raw loss 1.02 | loss 1.02 | ppl     2.77
| epoch   2 | lr 0.10 | raw loss 1.09 | loss 1.09 | ppl     2.96
====>train_shared<====== finish one epoch
| epoch   2 | lr 0.00035 | R 7.52859 | entropy 0.4444 | loss -7.12112
| epoch   2 | lr 0.00035 | R 7.72368 | entropy 0.4411 | loss -3.13665
| epoch   2 | lr 0.00035 | R 7.38904 | entropy 0.4452 | loss -6.68703
| epoch   2 | lr 0.00035 | R 8.71871 | entropy 0.4437 | loss 0.64241
| epoch   2 | lr 0.00035 | R 9.29131 | entropy 0.4444 | loss -0.02032
| epoch   2 | lr 0.00035 | R 9.45242 | entropy 0.4451 | loss -5.56074
| epoch   2 | lr 0.00035 | R 9.56888 | entropy 0.4435 | loss -2.79383
| epoch   2 | lr 0.00035 | R 9.55818 | entropy 0.4439 | loss -4.68766
| epoch   2 | lr 0.00035 | R 9.50652 | entropy 0.4390 | loss -5.76557
| epoch   2 | lr 0.00035 | R 9.30478 | entropy 0.4371 | loss -3.60446
| epoch   2 | lr 0.00035 | R 9.76688 | entropy 0.4375 | loss -4.83955
| epoch   2 | lr 0.00035 | R 9.99449 | entropy 0.4354 | loss -0.71012
| epoch   2 | lr 0.00035 | R 9.68594 | entropy 0.4332 | loss -5.54693
| epoch   2 | lr 0.00035 | R 9.76750 | entropy 0.4311 | loss -5.02487
| epoch   2 | lr 0.00035 | R 9.38074 | entropy 0.4307 | loss -7.96510
| epoch   2 | lr 0.00035 | R 9.90784 | entropy 0.4306 | loss -3.31914
| epoch   2 | lr 0.00035 | R 9.74020 | entropy 0.4311 | loss -3.86299
| epoch   2 | lr 0.00035 | R 10.16723 | entropy 0.4298 | loss -2.48913
| epoch   2 | lr 0.00035 | R 10.11338 | entropy 0.4288 | loss -2.54793
| epoch   2 | lr 0.00035 | R 10.24455 | entropy 0.4292 | loss -2.24579
| epoch   2 | lr 0.00035 | R 10.04698 | entropy 0.4294 | loss -5.69075
| epoch   2 | lr 0.00035 | R 9.57255 | entropy 0.4284 | loss -6.56696
| epoch   2 | lr 0.00035 | R 10.07293 | entropy 0.4272 | loss -2.33353
| epoch   2 | lr 0.00035 | R 10.06174 | entropy 0.4263 | loss -4.30884
| epoch   2 | lr 0.00035 | R 10.18672 | entropy 0.4265 | loss -3.38332
| epoch   2 | lr 0.00035 | R 10.13874 | entropy 0.4252 | loss -2.41702
| epoch   2 | lr 0.00035 | R 9.53609 | entropy 0.4229 | loss -10.91148
| epoch   2 | lr 0.00035 | R 10.29730 | entropy 0.4203 | loss 0.10114
| epoch   2 | lr 0.00035 | R 10.02166 | entropy 0.4212 | loss -2.47752
| epoch   2 | lr 0.00035 | R 10.02351 | entropy 0.4223 | loss -7.47833
| epoch   2 | lr 0.00035 | R 10.31685 | entropy 0.4212 | loss -2.20862
| epoch   2 | lr 0.00035 | R 10.22326 | entropy 0.4185 | loss -3.97719
| epoch   2 | lr 0.00035 | R 9.82752 | entropy 0.4150 | loss -8.55346
| epoch   2 | lr 0.00035 | R 10.62796 | entropy 0.4132 | loss 0.10485
| epoch   2 | lr 0.00035 | R 10.13960 | entropy 0.4129 | loss -5.35682
| epoch   2 | lr 0.00035 | R 10.27576 | entropy 0.4117 | loss -3.06155
| epoch   2 | lr 0.00035 | R 10.03397 | entropy 0.4124 | loss -6.47519
| epoch   2 | lr 0.00035 | R 10.45142 | entropy 0.4117 | loss -0.49481
| epoch   2 | lr 0.00035 | R 10.05635 | entropy 0.4100 | loss -8.66142
derive | max_R: 18.406675
========> finish evaluate on one epoch<======
eval | loss:    85.55 | ppl: 14191942721046042325907046430047993856.00 | accuracy:     0.59
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch2_step3128.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch2_step8000.pth
| epoch   3 | lr 0.10 | raw loss 1.02 | loss 1.02 | ppl     2.77
| epoch   3 | lr 0.10 | raw loss 0.99 | loss 0.99 | ppl     2.70
| epoch   3 | lr 0.10 | raw loss 1.00 | loss 1.00 | ppl     2.72
| epoch   3 | lr 0.10 | raw loss 0.99 | loss 0.99 | ppl     2.69
| epoch   3 | lr 0.10 | raw loss 0.99 | loss 0.99 | ppl     2.70
| epoch   3 | lr 0.10 | raw loss 0.90 | loss 0.90 | ppl     2.46
| epoch   3 | lr 0.10 | raw loss 0.92 | loss 0.92 | ppl     2.50
| epoch   3 | lr 0.10 | raw loss 0.94 | loss 0.94 | ppl     2.56
| epoch   3 | lr 0.10 | raw loss 0.93 | loss 0.93 | ppl     2.53
| epoch   3 | lr 0.10 | raw loss 0.89 | loss 0.89 | ppl     2.42
| epoch   3 | lr 0.10 | raw loss 0.89 | loss 0.89 | ppl     2.43
| epoch   3 | lr 0.10 | raw loss 0.93 | loss 0.93 | ppl     2.53
| epoch   3 | lr 0.10 | raw loss 0.90 | loss 0.90 | ppl     2.47
| epoch   3 | lr 0.10 | raw loss 0.82 | loss 0.82 | ppl     2.27
| epoch   3 | lr 0.10 | raw loss 0.87 | loss 0.87 | ppl     2.39
====>train_shared<====== finish one epoch
| epoch   3 | lr 0.00035 | R 8.36222 | entropy 0.4076 | loss -18.15256
| epoch   3 | lr 0.00035 | R 8.57783 | entropy 0.4042 | loss -4.70619
| epoch   3 | lr 0.00035 | R 8.29908 | entropy 0.3997 | loss -5.31755
| epoch   3 | lr 0.00035 | R 9.73045 | entropy 0.3962 | loss 3.14423
| epoch   3 | lr 0.00035 | R 9.64106 | entropy 0.3941 | loss -3.99835
| epoch   3 | lr 0.00035 | R 10.34623 | entropy 0.3922 | loss -0.36222
| epoch   3 | lr 0.00035 | R 10.36292 | entropy 0.3900 | loss -3.68034
| epoch   3 | lr 0.00035 | R 9.75850 | entropy 0.3873 | loss -8.56386
| epoch   3 | lr 0.00035 | R 9.86042 | entropy 0.3844 | loss -7.09438
| epoch   3 | lr 0.00035 | R 10.54256 | entropy 0.3811 | loss -2.39828
| epoch   3 | lr 0.00035 | R 10.57771 | entropy 0.3790 | loss -2.37428
| epoch   3 | lr 0.00035 | R 10.07779 | entropy 0.3764 | loss -8.96981
| epoch   3 | lr 0.00035 | R 9.96697 | entropy 0.3756 | loss -5.72262
| epoch   3 | lr 0.00035 | R 10.27326 | entropy 0.3732 | loss -1.69381
| epoch   3 | lr 0.00035 | R 10.14404 | entropy 0.3705 | loss -4.58628
| epoch   3 | lr 0.00035 | R 10.58595 | entropy 0.3683 | loss -1.28987
| epoch   3 | lr 0.00035 | R 10.11050 | entropy 0.3670 | loss -5.60023
| epoch   3 | lr 0.00035 | R 10.88680 | entropy 0.3652 | loss -1.25193
| epoch   3 | lr 0.00035 | R 10.23093 | entropy 0.3645 | loss -6.48760
| epoch   3 | lr 0.00035 | R 9.90756 | entropy 0.3622 | loss -6.27677
| epoch   3 | lr 0.00035 | R 10.50758 | entropy 0.3599 | loss -2.88837
| epoch   3 | lr 0.00035 | R 10.41440 | entropy 0.3575 | loss -5.24110
| epoch   3 | lr 0.00035 | R 10.54372 | entropy 0.3547 | loss -3.54457
| epoch   3 | lr 0.00035 | R 10.53099 | entropy 0.3512 | loss -4.70981
| epoch   3 | lr 0.00035 | R 10.87585 | entropy 0.3470 | loss -3.25444
| epoch   3 | lr 0.00035 | R 10.55854 | entropy 0.3441 | loss -7.26539
| epoch   3 | lr 0.00035 | R 10.19272 | entropy 0.3407 | loss -8.57089
| epoch   3 | lr 0.00035 | R 11.09036 | entropy 0.3358 | loss -1.02403
| epoch   3 | lr 0.00035 | R 10.63099 | entropy 0.3341 | loss -6.42904
| epoch   3 | lr 0.00035 | R 11.02443 | entropy 0.3321 | loss -1.36936
| epoch   3 | lr 0.00035 | R 10.57268 | entropy 0.3310 | loss -8.48928
| epoch   3 | lr 0.00035 | R 10.51414 | entropy 0.3295 | loss -5.10870
| epoch   3 | lr 0.00035 | R 10.86174 | entropy 0.3273 | loss -3.36331
| epoch   3 | lr 0.00035 | R 10.92495 | entropy 0.3253 | loss -3.16933
| epoch   3 | lr 0.00035 | R 10.85592 | entropy 0.3249 | loss -5.56724
| epoch   3 | lr 0.00035 | R 11.08126 | entropy 0.3248 | loss -3.06969
| epoch   3 | lr 0.00035 | R 10.72816 | entropy 0.3237 | loss -5.77743
| epoch   3 | lr 0.00035 | R 10.63197 | entropy 0.3228 | loss -5.41075
| epoch   3 | lr 0.00035 | R 10.78784 | entropy 0.3225 | loss -4.57171
derive | max_R: 21.176411
========> finish evaluate on one epoch<======
eval | loss:    73.34 | ppl: 71236519848748122971315044876288.00 | accuracy:     0.60
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch3_step3910.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch3_step10000.pth
| epoch   4 | lr 0.10 | raw loss 0.93 | loss 0.93 | ppl     2.53
| epoch   4 | lr 0.10 | raw loss 0.81 | loss 0.81 | ppl     2.24
| epoch   4 | lr 0.10 | raw loss 0.87 | loss 0.87 | ppl     2.38
| epoch   4 | lr 0.10 | raw loss 0.83 | loss 0.83 | ppl     2.28
| epoch   4 | lr 0.10 | raw loss 0.79 | loss 0.79 | ppl     2.19
| epoch   4 | lr 0.10 | raw loss 0.81 | loss 0.81 | ppl     2.24
| epoch   4 | lr 0.10 | raw loss 0.82 | loss 0.82 | ppl     2.27
| epoch   4 | lr 0.10 | raw loss 0.79 | loss 0.79 | ppl     2.21
| epoch   4 | lr 0.10 | raw loss 0.76 | loss 0.76 | ppl     2.13
| epoch   4 | lr 0.10 | raw loss 0.76 | loss 0.76 | ppl     2.15
| epoch   4 | lr 0.10 | raw loss 0.76 | loss 0.76 | ppl     2.13
| epoch   4 | lr 0.10 | raw loss 0.77 | loss 0.77 | ppl     2.16
| epoch   4 | lr 0.10 | raw loss 0.78 | loss 0.78 | ppl     2.19
| epoch   4 | lr 0.10 | raw loss 0.74 | loss 0.74 | ppl     2.10
| epoch   4 | lr 0.10 | raw loss 0.79 | loss 0.79 | ppl     2.21
====>train_shared<====== finish one epoch
| epoch   4 | lr 0.00035 | R 15.22459 | entropy 0.3160 | loss 6.20772
| epoch   4 | lr 0.00035 | R 14.82485 | entropy 0.3192 | loss -6.48706
| epoch   4 | lr 0.00035 | R 15.09319 | entropy 0.3176 | loss -6.44820
| epoch   4 | lr 0.00035 | R 17.39813 | entropy 0.3158 | loss -2.96073
| epoch   4 | lr 0.00035 | R 18.12268 | entropy 0.3136 | loss -5.59575
| epoch   4 | lr 0.00035 | R 17.46687 | entropy 0.3129 | loss -10.60547
| epoch   4 | lr 0.00035 | R 18.41784 | entropy 0.3136 | loss -2.04393
| epoch   4 | lr 0.00035 | R 18.28014 | entropy 0.3143 | loss -5.07153
| epoch   4 | lr 0.00035 | R 17.51478 | entropy 0.3148 | loss -8.91743
| epoch   4 | lr 0.00035 | R 17.83464 | entropy 0.3143 | loss -10.31883
| epoch   4 | lr 0.00035 | R 17.91808 | entropy 0.3141 | loss -4.58494
| epoch   4 | lr 0.00035 | R 18.35686 | entropy 0.3138 | loss -7.85730
| epoch   4 | lr 0.00035 | R 17.55227 | entropy 0.3138 | loss -12.73190
| epoch   4 | lr 0.00035 | R 18.43424 | entropy 0.3135 | loss -1.13530
| epoch   4 | lr 0.00035 | R 18.08114 | entropy 0.3153 | loss -7.29598
| epoch   4 | lr 0.00035 | R 17.94337 | entropy 0.3150 | loss -8.27755
| epoch   4 | lr 0.00035 | R 17.95981 | entropy 0.3145 | loss -10.12195
| epoch   4 | lr 0.00035 | R 18.17348 | entropy 0.3116 | loss -5.33991
| epoch   4 | lr 0.00035 | R 17.76785 | entropy 0.3105 | loss -6.13853
| epoch   4 | lr 0.00035 | R 17.73203 | entropy 0.3099 | loss -9.14447
| epoch   4 | lr 0.00035 | R 17.97419 | entropy 0.3080 | loss -5.06259
| epoch   4 | lr 0.00035 | R 18.65814 | entropy 0.3075 | loss -5.78119
| epoch   4 | lr 0.00035 | R 17.83606 | entropy 0.3075 | loss -7.50879
| epoch   4 | lr 0.00035 | R 18.05306 | entropy 0.3085 | loss -6.46685
| epoch   4 | lr 0.00035 | R 18.22851 | entropy 0.3081 | loss -8.06924
| epoch   4 | lr 0.00035 | R 17.67829 | entropy 0.3078 | loss -10.27913
| epoch   4 | lr 0.00035 | R 18.31848 | entropy 0.3082 | loss -3.24945
| epoch   4 | lr 0.00035 | R 17.90304 | entropy 0.3090 | loss -9.56928
| epoch   4 | lr 0.00035 | R 18.03237 | entropy 0.3100 | loss -5.14548
| epoch   4 | lr 0.00035 | R 17.95783 | entropy 0.3092 | loss -7.34722
| epoch   4 | lr 0.00035 | R 18.11316 | entropy 0.3074 | loss -7.27783
| epoch   4 | lr 0.00035 | R 17.95837 | entropy 0.3047 | loss -5.41862
| epoch   4 | lr 0.00035 | R 18.01665 | entropy 0.3064 | loss -6.78493
| epoch   4 | lr 0.00035 | R 17.90746 | entropy 0.3082 | loss -6.15361
| epoch   4 | lr 0.00035 | R 18.48837 | entropy 0.3088 | loss -2.81073
| epoch   4 | lr 0.00035 | R 17.41266 | entropy 0.3086 | loss -11.71906
| epoch   4 | lr 0.00035 | R 18.64204 | entropy 0.3071 | loss -1.61014
| epoch   4 | lr 0.00035 | R 17.59833 | entropy 0.3063 | loss -9.04133
| epoch   4 | lr 0.00035 | R 18.09635 | entropy 0.3070 | loss -7.00558
derive | max_R: 33.642960
========> finish evaluate on one epoch<======
eval | loss:    54.56 | ppl: 493144531056301952729088.00 | accuracy:     0.73
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch4_step4692.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch4_step12000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch0_step4000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch0_step1564.pth
| epoch   5 | lr 0.10 | raw loss 0.77 | loss 0.77 | ppl     2.17
| epoch   5 | lr 0.10 | raw loss 0.72 | loss 0.72 | ppl     2.05
| epoch   5 | lr 0.10 | raw loss 0.71 | loss 0.71 | ppl     2.03
| epoch   5 | lr 0.10 | raw loss 0.71 | loss 0.71 | ppl     2.04
| epoch   5 | lr 0.10 | raw loss 0.74 | loss 0.74 | ppl     2.10
| epoch   5 | lr 0.10 | raw loss 0.72 | loss 0.72 | ppl     2.05
| epoch   5 | lr 0.10 | raw loss 0.67 | loss 0.67 | ppl     1.96
| epoch   5 | lr 0.10 | raw loss 0.76 | loss 0.76 | ppl     2.13
| epoch   5 | lr 0.10 | raw loss 0.65 | loss 0.65 | ppl     1.92
| epoch   5 | lr 0.10 | raw loss 0.71 | loss 0.71 | ppl     2.03
| epoch   5 | lr 0.10 | raw loss 0.73 | loss 0.73 | ppl     2.07
| epoch   5 | lr 0.10 | raw loss 0.68 | loss 0.68 | ppl     1.97
| epoch   5 | lr 0.10 | raw loss 0.72 | loss 0.72 | ppl     2.05
| epoch   5 | lr 0.10 | raw loss 0.66 | loss 0.66 | ppl     1.94
| epoch   5 | lr 0.10 | raw loss 0.65 | loss 0.65 | ppl     1.92
====>train_shared<====== finish one epoch
| epoch   5 | lr 0.00035 | R 13.27606 | entropy 0.3092 | loss -19.87557
| epoch   5 | lr 0.00035 | R 13.39490 | entropy 0.3088 | loss -5.25331
| epoch   5 | lr 0.00035 | R 13.46553 | entropy 0.3044 | loss 1.79123
| epoch   5 | lr 0.00035 | R 15.15658 | entropy 0.3009 | loss -1.33679
| epoch   5 | lr 0.00035 | R 15.69207 | entropy 0.3001 | loss -6.90998
| epoch   5 | lr 0.00035 | R 16.14527 | entropy 0.2998 | loss -4.63492
| epoch   5 | lr 0.00035 | R 15.19549 | entropy 0.3000 | loss -7.50087
| epoch   5 | lr 0.00035 | R 15.48228 | entropy 0.2982 | loss -8.00662
| epoch   5 | lr 0.00035 | R 15.80203 | entropy 0.2973 | loss -4.54422
| epoch   5 | lr 0.00035 | R 15.96352 | entropy 0.2966 | loss -4.14889
| epoch   5 | lr 0.00035 | R 15.61922 | entropy 0.2962 | loss -7.44274
| epoch   5 | lr 0.00035 | R 15.82100 | entropy 0.2961 | loss -3.83468
| epoch   5 | lr 0.00035 | R 15.33880 | entropy 0.2961 | loss -8.66514
| epoch   5 | lr 0.00035 | R 16.04147 | entropy 0.2968 | loss -0.58440
| epoch   5 | lr 0.00035 | R 15.79970 | entropy 0.2969 | loss -5.69853
| epoch   5 | lr 0.00035 | R 15.73141 | entropy 0.2970 | loss -7.68600
| epoch   5 | lr 0.00035 | R 15.77460 | entropy 0.2979 | loss -7.53960
| epoch   5 | lr 0.00035 | R 16.01240 | entropy 0.2965 | loss -5.45398
| epoch   5 | lr 0.00035 | R 15.69931 | entropy 0.2969 | loss -4.79597
| epoch   5 | lr 0.00035 | R 15.19990 | entropy 0.2986 | loss -12.19275
| epoch   5 | lr 0.00035 | R 15.58317 | entropy 0.2991 | loss -10.11697
| epoch   5 | lr 0.00035 | R 15.41620 | entropy 0.3005 | loss -8.92672
| epoch   5 | lr 0.00035 | R 16.36737 | entropy 0.2999 | loss -2.55354
| epoch   5 | lr 0.00035 | R 15.58174 | entropy 0.3026 | loss -6.42077
| epoch   5 | lr 0.00035 | R 15.16440 | entropy 0.3027 | loss -12.56641
| epoch   5 | lr 0.00035 | R 15.86331 | entropy 0.3035 | loss -2.45712
| epoch   5 | lr 0.00035 | R 15.95651 | entropy 0.3036 | loss -7.09755
| epoch   5 | lr 0.00035 | R 16.41903 | entropy 0.3031 | loss -3.04590
| epoch   5 | lr 0.00035 | R 16.29142 | entropy 0.3059 | loss -4.56193
| epoch   5 | lr 0.00035 | R 15.57222 | entropy 0.3078 | loss -10.28643
| epoch   5 | lr 0.00035 | R 16.50535 | entropy 0.3110 | loss -5.30368
| epoch   5 | lr 0.00035 | R 16.10335 | entropy 0.3120 | loss -5.58396
| epoch   5 | lr 0.00035 | R 16.26256 | entropy 0.3102 | loss -5.02760
| epoch   5 | lr 0.00035 | R 16.66723 | entropy 0.3097 | loss -3.23197
| epoch   5 | lr 0.00035 | R 15.90342 | entropy 0.3077 | loss -11.88882
| epoch   5 | lr 0.00035 | R 16.04980 | entropy 0.3068 | loss -4.17458
| epoch   5 | lr 0.00035 | R 15.56751 | entropy 0.3071 | loss -11.73751
| epoch   5 | lr 0.00035 | R 16.75473 | entropy 0.3072 | loss -1.17111
| epoch   5 | lr 0.00035 | R 16.18144 | entropy 0.3096 | loss -8.54954
derive | max_R: 29.686403
========> finish evaluate on one epoch<======
eval | loss:    60.85 | ppl: 267075144396555338479828992.00 | accuracy:     0.68
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch5_step5474.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch5_step14000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch1_step6000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch1_step2346.pth
| epoch   6 | lr 0.10 | raw loss 0.67 | loss 0.67 | ppl     1.96
| epoch   6 | lr 0.10 | raw loss 0.66 | loss 0.66 | ppl     1.93
| epoch   6 | lr 0.10 | raw loss 0.66 | loss 0.66 | ppl     1.94
| epoch   6 | lr 0.10 | raw loss 0.67 | loss 0.67 | ppl     1.96
| epoch   6 | lr 0.10 | raw loss 0.66 | loss 0.66 | ppl     1.93
| epoch   6 | lr 0.10 | raw loss 0.63 | loss 0.63 | ppl     1.88
| epoch   6 | lr 0.10 | raw loss 0.66 | loss 0.66 | ppl     1.93
| epoch   6 | lr 0.10 | raw loss 0.63 | loss 0.63 | ppl     1.87
| epoch   6 | lr 0.10 | raw loss 0.62 | loss 0.62 | ppl     1.86
| epoch   6 | lr 0.10 | raw loss 0.65 | loss 0.65 | ppl     1.91
| epoch   6 | lr 0.10 | raw loss 0.65 | loss 0.65 | ppl     1.91
| epoch   6 | lr 0.10 | raw loss 0.62 | loss 0.62 | ppl     1.86
| epoch   6 | lr 0.10 | raw loss 0.64 | loss 0.64 | ppl     1.90
| epoch   6 | lr 0.10 | raw loss 0.61 | loss 0.61 | ppl     1.84
| epoch   6 | lr 0.10 | raw loss 0.59 | loss 0.59 | ppl     1.80
====>train_shared<====== finish one epoch
| epoch   6 | lr 0.00035 | R 16.09925 | entropy 0.3054 | loss -16.25756
| epoch   6 | lr 0.00035 | R 16.52721 | entropy 0.3012 | loss -6.55967
| epoch   6 | lr 0.00035 | R 15.33121 | entropy 0.2981 | loss -9.12672
| epoch   6 | lr 0.00035 | R 17.59342 | entropy 0.2960 | loss -4.45930
| epoch   6 | lr 0.00035 | R 17.78809 | entropy 0.2949 | loss -11.48616
| epoch   6 | lr 0.00035 | R 17.99610 | entropy 0.2935 | loss -5.50057
| epoch   6 | lr 0.00035 | R 18.31016 | entropy 0.2938 | loss -8.13707
| epoch   6 | lr 0.00035 | R 18.18342 | entropy 0.2936 | loss -9.93300
| epoch   6 | lr 0.00035 | R 18.42574 | entropy 0.2941 | loss -7.34874
| epoch   6 | lr 0.00035 | R 18.38394 | entropy 0.2936 | loss -10.83040
| epoch   6 | lr 0.00035 | R 18.00109 | entropy 0.2927 | loss -14.22885
| epoch   6 | lr 0.00035 | R 18.27349 | entropy 0.2906 | loss -9.42460
| epoch   6 | lr 0.00035 | R 18.55103 | entropy 0.2893 | loss -6.00636
| epoch   6 | lr 0.00035 | R 18.28524 | entropy 0.2901 | loss -6.72909
| epoch   6 | lr 0.00035 | R 18.53349 | entropy 0.2896 | loss -10.38170
| epoch   6 | lr 0.00035 | R 18.28197 | entropy 0.2895 | loss -10.10106
| epoch   6 | lr 0.00035 | R 18.78111 | entropy 0.2897 | loss -7.03294
| epoch   6 | lr 0.00035 | R 18.28177 | entropy 0.2880 | loss -10.48825
| epoch   6 | lr 0.00035 | R 18.85302 | entropy 0.2876 | loss -4.90619
| epoch   6 | lr 0.00035 | R 17.12494 | entropy 0.2878 | loss -22.42684
| epoch   6 | lr 0.00035 | R 18.36546 | entropy 0.2867 | loss -7.63736
| epoch   6 | lr 0.00035 | R 18.15586 | entropy 0.2867 | loss -6.21278
| epoch   6 | lr 0.00035 | R 18.95207 | entropy 0.2871 | loss -5.72449
| epoch   6 | lr 0.00035 | R 18.89411 | entropy 0.2855 | loss -9.30235
| epoch   6 | lr 0.00035 | R 18.25365 | entropy 0.2852 | loss -14.69185
| epoch   6 | lr 0.00035 | R 17.84088 | entropy 0.2844 | loss -15.67887
| epoch   6 | lr 0.00035 | R 18.69442 | entropy 0.2837 | loss -3.96425
| epoch   6 | lr 0.00035 | R 18.79514 | entropy 0.2833 | loss -4.71679
| epoch   6 | lr 0.00035 | R 19.30932 | entropy 0.2830 | loss -6.78947
| epoch   6 | lr 0.00035 | R 19.40908 | entropy 0.2828 | loss -7.54386
| epoch   6 | lr 0.00035 | R 18.97474 | entropy 0.2832 | loss -10.57022
| epoch   6 | lr 0.00035 | R 19.13376 | entropy 0.2807 | loss -10.63255
| epoch   6 | lr 0.00035 | R 18.79708 | entropy 0.2788 | loss -8.73530
| epoch   6 | lr 0.00035 | R 18.79062 | entropy 0.2782 | loss -13.77912
| epoch   6 | lr 0.00035 | R 19.74764 | entropy 0.2761 | loss -4.80424
| epoch   6 | lr 0.00035 | R 18.08947 | entropy 0.2754 | loss -12.90308
| epoch   6 | lr 0.00035 | R 19.23426 | entropy 0.2754 | loss -7.05220
| epoch   6 | lr 0.00035 | R 18.01057 | entropy 0.2745 | loss -18.93791
| epoch   6 | lr 0.00035 | R 18.98536 | entropy 0.2733 | loss -7.27124
derive | max_R: 32.757820
========> finish evaluate on one epoch<======
eval | loss:    44.10 | ppl: 14274129812746981376.00 | accuracy:     0.76
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch6_step6256.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch6_step16000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch2_step3128.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch2_step8000.pth
| epoch   7 | lr 0.10 | raw loss 0.60 | loss 0.60 | ppl     1.82
| epoch   7 | lr 0.10 | raw loss 0.59 | loss 0.59 | ppl     1.80
| epoch   7 | lr 0.10 | raw loss 0.57 | loss 0.57 | ppl     1.77
| epoch   7 | lr 0.10 | raw loss 0.56 | loss 0.56 | ppl     1.76
| epoch   7 | lr 0.10 | raw loss 0.60 | loss 0.60 | ppl     1.82
| epoch   7 | lr 0.10 | raw loss 0.57 | loss 0.57 | ppl     1.77
| epoch   7 | lr 0.10 | raw loss 0.57 | loss 0.57 | ppl     1.77
| epoch   7 | lr 0.10 | raw loss 0.56 | loss 0.56 | ppl     1.76
| epoch   7 | lr 0.10 | raw loss 0.62 | loss 0.62 | ppl     1.86
| epoch   7 | lr 0.10 | raw loss 0.62 | loss 0.62 | ppl     1.86
| epoch   7 | lr 0.10 | raw loss 0.55 | loss 0.55 | ppl     1.73
| epoch   7 | lr 0.10 | raw loss 0.59 | loss 0.59 | ppl     1.81
| epoch   7 | lr 0.10 | raw loss 0.59 | loss 0.59 | ppl     1.80
| epoch   7 | lr 0.10 | raw loss 0.56 | loss 0.56 | ppl     1.76
| epoch   7 | lr 0.10 | raw loss 0.59 | loss 0.59 | ppl     1.81
====>train_shared<====== finish one epoch
| epoch   7 | lr 0.00035 | R 17.31482 | entropy 0.2722 | loss 0.68782
| epoch   7 | lr 0.00035 | R 17.21132 | entropy 0.2726 | loss -3.82159
| epoch   7 | lr 0.00035 | R 16.96182 | entropy 0.2743 | loss -7.44801
| epoch   7 | lr 0.00035 | R 17.49376 | entropy 0.2718 | loss -7.30338
| epoch   7 | lr 0.00035 | R 17.86616 | entropy 0.2701 | loss -6.63173
| epoch   7 | lr 0.00035 | R 17.36334 | entropy 0.2694 | loss -6.48368
| epoch   7 | lr 0.00035 | R 17.85475 | entropy 0.2685 | loss -4.49378
| epoch   7 | lr 0.00035 | R 17.44021 | entropy 0.2696 | loss -6.08482
| epoch   7 | lr 0.00035 | R 17.49295 | entropy 0.2690 | loss -8.70088
| epoch   7 | lr 0.00035 | R 17.71304 | entropy 0.2683 | loss -7.09404
| epoch   7 | lr 0.00035 | R 18.12335 | entropy 0.2683 | loss -7.06585
| epoch   7 | lr 0.00035 | R 17.80153 | entropy 0.2679 | loss -9.80052
| epoch   7 | lr 0.00035 | R 17.91850 | entropy 0.2687 | loss -5.63591
| epoch   7 | lr 0.00035 | R 17.86871 | entropy 0.2687 | loss -6.35362
| epoch   7 | lr 0.00035 | R 17.28735 | entropy 0.2698 | loss -9.69137
| epoch   7 | lr 0.00035 | R 17.51509 | entropy 0.2701 | loss -8.34492
| epoch   7 | lr 0.00035 | R 17.64572 | entropy 0.2706 | loss -8.47219
| epoch   7 | lr 0.00035 | R 17.68689 | entropy 0.2713 | loss -9.58626
| epoch   7 | lr 0.00035 | R 18.03694 | entropy 0.2707 | loss -2.01216
| epoch   7 | lr 0.00035 | R 17.97198 | entropy 0.2725 | loss -7.57931
| epoch   7 | lr 0.00035 | R 18.28092 | entropy 0.2747 | loss -5.44116
| epoch   7 | lr 0.00035 | R 18.20565 | entropy 0.2751 | loss -4.83956
| epoch   7 | lr 0.00035 | R 17.18487 | entropy 0.2772 | loss -11.88469
| epoch   7 | lr 0.00035 | R 17.59340 | entropy 0.2795 | loss -4.68332
| epoch   7 | lr 0.00035 | R 17.70613 | entropy 0.2785 | loss -8.06279
| epoch   7 | lr 0.00035 | R 18.03329 | entropy 0.2785 | loss -5.16246
| epoch   7 | lr 0.00035 | R 17.68781 | entropy 0.2807 | loss -8.67057
| epoch   7 | lr 0.00035 | R 17.50021 | entropy 0.2818 | loss -7.26986
| epoch   7 | lr 0.00035 | R 18.28921 | entropy 0.2830 | loss -3.23761
| epoch   7 | lr 0.00035 | R 18.24624 | entropy 0.2832 | loss -5.07775
| epoch   7 | lr 0.00035 | R 18.19030 | entropy 0.2832 | loss -6.02700
| epoch   7 | lr 0.00035 | R 17.27522 | entropy 0.2832 | loss -14.17162
| epoch   7 | lr 0.00035 | R 17.47578 | entropy 0.2820 | loss -11.16732
| epoch   7 | lr 0.00035 | R 17.60837 | entropy 0.2834 | loss -5.82175
| epoch   7 | lr 0.00035 | R 18.05342 | entropy 0.2843 | loss -7.38072
| epoch   7 | lr 0.00035 | R 16.77617 | entropy 0.2859 | loss -10.80255
| epoch   7 | lr 0.00035 | R 17.72060 | entropy 0.2877 | loss -6.60313
| epoch   7 | lr 0.00035 | R 17.59904 | entropy 0.2870 | loss -8.66192
| epoch   7 | lr 0.00035 | R 17.89289 | entropy 0.2889 | loss -9.41542
derive | max_R: 31.043636
========> finish evaluate on one epoch<======
eval | loss:    51.74 | ppl: 29503817537583170191360.00 | accuracy:     0.72
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch7_step7038.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch7_step18000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch3_step10000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch3_step3910.pth
| epoch   8 | lr 0.10 | raw loss 0.57 | loss 0.57 | ppl     1.76
| epoch   8 | lr 0.10 | raw loss 0.57 | loss 0.57 | ppl     1.77
| epoch   8 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.70
| epoch   8 | lr 0.10 | raw loss 0.57 | loss 0.57 | ppl     1.77
| epoch   8 | lr 0.10 | raw loss 0.55 | loss 0.55 | ppl     1.73
| epoch   8 | lr 0.10 | raw loss 0.57 | loss 0.57 | ppl     1.76
| epoch   8 | lr 0.10 | raw loss 0.55 | loss 0.55 | ppl     1.73
| epoch   8 | lr 0.10 | raw loss 0.54 | loss 0.54 | ppl     1.71
| epoch   8 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.70
| epoch   8 | lr 0.10 | raw loss 0.55 | loss 0.55 | ppl     1.73
| epoch   8 | lr 0.10 | raw loss 0.55 | loss 0.55 | ppl     1.74
| epoch   8 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.69
| epoch   8 | lr 0.10 | raw loss 0.54 | loss 0.54 | ppl     1.71
| epoch   8 | lr 0.10 | raw loss 0.54 | loss 0.54 | ppl     1.71
| epoch   8 | lr 0.10 | raw loss 0.52 | loss 0.52 | ppl     1.68
====>train_shared<====== finish one epoch
| epoch   8 | lr 0.00035 | R 20.89698 | entropy 0.2906 | loss -9.56234
| epoch   8 | lr 0.00035 | R 21.80692 | entropy 0.2920 | loss -0.86527
| epoch   8 | lr 0.00035 | R 21.44066 | entropy 0.2917 | loss -8.17941
| epoch   8 | lr 0.00035 | R 21.53941 | entropy 0.2905 | loss -5.49034
| epoch   8 | lr 0.00035 | R 22.02941 | entropy 0.2926 | loss -4.06164
| epoch   8 | lr 0.00035 | R 22.34834 | entropy 0.2923 | loss -3.48045
| epoch   8 | lr 0.00035 | R 22.76709 | entropy 0.2928 | loss -0.30512
| epoch   8 | lr 0.00035 | R 21.72276 | entropy 0.2945 | loss -12.52592
| epoch   8 | lr 0.00035 | R 21.73750 | entropy 0.2969 | loss -7.00258
| epoch   8 | lr 0.00035 | R 21.58504 | entropy 0.2985 | loss -7.45607
| epoch   8 | lr 0.00035 | R 22.41605 | entropy 0.2980 | loss -1.84312
| epoch   8 | lr 0.00035 | R 22.34468 | entropy 0.2976 | loss -5.05936
| epoch   8 | lr 0.00035 | R 22.52719 | entropy 0.2984 | loss -6.19971
| epoch   8 | lr 0.00035 | R 22.10226 | entropy 0.2979 | loss -7.27283
| epoch   8 | lr 0.00035 | R 22.76471 | entropy 0.2981 | loss -2.56985
| epoch   8 | lr 0.00035 | R 21.95814 | entropy 0.2983 | loss -5.82982
| epoch   8 | lr 0.00035 | R 22.22085 | entropy 0.2993 | loss -8.19377
| epoch   8 | lr 0.00035 | R 22.36131 | entropy 0.3003 | loss -7.09320
| epoch   8 | lr 0.00035 | R 22.53722 | entropy 0.3007 | loss -3.30602
| epoch   8 | lr 0.00035 | R 22.54301 | entropy 0.3015 | loss -5.70519
| epoch   8 | lr 0.00035 | R 22.38638 | entropy 0.3021 | loss -8.16254
| epoch   8 | lr 0.00035 | R 23.47743 | entropy 0.3017 | loss 1.80496
| epoch   8 | lr 0.00035 | R 22.59385 | entropy 0.3011 | loss -8.70047
| epoch   8 | lr 0.00035 | R 22.30008 | entropy 0.3002 | loss -9.04250
| epoch   8 | lr 0.00035 | R 22.56754 | entropy 0.3006 | loss -5.24168
| epoch   8 | lr 0.00035 | R 22.72404 | entropy 0.2988 | loss -4.88554
| epoch   8 | lr 0.00035 | R 23.16580 | entropy 0.2984 | loss -6.09013
| epoch   8 | lr 0.00035 | R 23.79764 | entropy 0.2957 | loss -2.14334
| epoch   8 | lr 0.00035 | R 23.04426 | entropy 0.2942 | loss -8.23580
| epoch   8 | lr 0.00035 | R 22.62568 | entropy 0.2925 | loss -7.18660
| epoch   8 | lr 0.00035 | R 22.45150 | entropy 0.2898 | loss -8.65029
| epoch   8 | lr 0.00035 | R 23.63591 | entropy 0.2857 | loss -2.61993
| epoch   8 | lr 0.00035 | R 23.67416 | entropy 0.2848 | loss -4.62510
| epoch   8 | lr 0.00035 | R 23.25222 | entropy 0.2821 | loss -9.76429
| epoch   8 | lr 0.00035 | R 22.83999 | entropy 0.2805 | loss -6.80720
| epoch   8 | lr 0.00035 | R 23.18029 | entropy 0.2831 | loss -4.99190
| epoch   8 | lr 0.00035 | R 22.79400 | entropy 0.2843 | loss -8.69627
| epoch   8 | lr 0.00035 | R 23.47190 | entropy 0.2812 | loss -6.01320
| epoch   8 | lr 0.00035 | R 23.20207 | entropy 0.2803 | loss -7.58431
derive | max_R: 45.976589
========> finish evaluate on one epoch<======
eval | loss:    41.41 | ppl: 961647493268351360.00 | accuracy:     0.79
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch8_step7820.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch8_step20000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch4_step12000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch4_step4692.pth
| epoch   9 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.71
| epoch   9 | lr 0.10 | raw loss 0.51 | loss 0.51 | ppl     1.67
| epoch   9 | lr 0.10 | raw loss 0.50 | loss 0.50 | ppl     1.65
| epoch   9 | lr 0.10 | raw loss 0.51 | loss 0.51 | ppl     1.67
| epoch   9 | lr 0.10 | raw loss 0.49 | loss 0.49 | ppl     1.64
| epoch   9 | lr 0.10 | raw loss 0.50 | loss 0.50 | ppl     1.65
| epoch   9 | lr 0.10 | raw loss 0.52 | loss 0.52 | ppl     1.68
| epoch   9 | lr 0.10 | raw loss 0.52 | loss 0.52 | ppl     1.68
| epoch   9 | lr 0.10 | raw loss 0.52 | loss 0.52 | ppl     1.69
| epoch   9 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.70
| epoch   9 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.70
| epoch   9 | lr 0.10 | raw loss 0.52 | loss 0.52 | ppl     1.67
| epoch   9 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.69
| epoch   9 | lr 0.10 | raw loss 0.52 | loss 0.52 | ppl     1.68
| epoch   9 | lr 0.10 | raw loss 0.52 | loss 0.52 | ppl     1.68
====>train_shared<====== finish one epoch
| epoch   9 | lr 0.00035 | R 24.21947 | entropy 0.2772 | loss -20.80558
| epoch   9 | lr 0.00035 | R 24.81567 | entropy 0.2748 | loss -11.29563
| epoch   9 | lr 0.00035 | R 23.24150 | entropy 0.2724 | loss -6.13950
| epoch   9 | lr 0.00035 | R 24.33918 | entropy 0.2686 | loss -11.22123
| epoch   9 | lr 0.00035 | R 24.07340 | entropy 0.2679 | loss -7.41422
| epoch   9 | lr 0.00035 | R 24.67903 | entropy 0.2688 | loss -7.65335
| epoch   9 | lr 0.00035 | R 24.16020 | entropy 0.2689 | loss -13.70149
| epoch   9 | lr 0.00035 | R 24.04212 | entropy 0.2696 | loss -10.00269
| epoch   9 | lr 0.00035 | R 23.95920 | entropy 0.2688 | loss -14.89284
| epoch   9 | lr 0.00035 | R 24.50638 | entropy 0.2688 | loss -8.23444
| epoch   9 | lr 0.00035 | R 25.52728 | entropy 0.2703 | loss -0.92177
| epoch   9 | lr 0.00035 | R 25.17645 | entropy 0.2709 | loss -6.96303
| epoch   9 | lr 0.00035 | R 23.99982 | entropy 0.2720 | loss -12.53758
| epoch   9 | lr 0.00035 | R 24.24976 | entropy 0.2735 | loss -10.88071
| epoch   9 | lr 0.00035 | R 24.98823 | entropy 0.2734 | loss -10.16902
| epoch   9 | lr 0.00035 | R 25.24601 | entropy 0.2735 | loss -8.17976
| epoch   9 | lr 0.00035 | R 24.61212 | entropy 0.2740 | loss -10.64566
| epoch   9 | lr 0.00035 | R 24.90208 | entropy 0.2749 | loss -6.89560
| epoch   9 | lr 0.00035 | R 25.11127 | entropy 0.2768 | loss -4.37867
| epoch   9 | lr 0.00035 | R 24.99091 | entropy 0.2787 | loss -6.66069
| epoch   9 | lr 0.00035 | R 24.54064 | entropy 0.2808 | loss -7.74775
| epoch   9 | lr 0.00035 | R 24.02999 | entropy 0.2822 | loss -12.24203
| epoch   9 | lr 0.00035 | R 25.27412 | entropy 0.2836 | loss -1.23330
| epoch   9 | lr 0.00035 | R 25.18845 | entropy 0.2841 | loss -5.77963
| epoch   9 | lr 0.00035 | R 24.44454 | entropy 0.2816 | loss -8.94626
| epoch   9 | lr 0.00035 | R 24.71195 | entropy 0.2804 | loss -8.26321
| epoch   9 | lr 0.00035 | R 25.24109 | entropy 0.2804 | loss -2.69083
| epoch   9 | lr 0.00035 | R 25.47857 | entropy 0.2809 | loss -9.17161
| epoch   9 | lr 0.00035 | R 25.01797 | entropy 0.2816 | loss -5.86634
| epoch   9 | lr 0.00035 | R 25.06222 | entropy 0.2812 | loss -5.65828
| epoch   9 | lr 0.00035 | R 25.02394 | entropy 0.2819 | loss -8.59166
| epoch   9 | lr 0.00035 | R 24.82556 | entropy 0.2824 | loss -12.88702
| epoch   9 | lr 0.00035 | R 24.25764 | entropy 0.2824 | loss -8.02935
| epoch   9 | lr 0.00035 | R 24.99105 | entropy 0.2819 | loss -5.03460
| epoch   9 | lr 0.00035 | R 25.07320 | entropy 0.2810 | loss -5.26278
| epoch   9 | lr 0.00035 | R 23.11490 | entropy 0.2800 | loss -19.89072
| epoch   9 | lr 0.00035 | R 24.52543 | entropy 0.2789 | loss -5.07378
| epoch   9 | lr 0.00035 | R 24.51136 | entropy 0.2790 | loss -11.97398
| epoch   9 | lr 0.00035 | R 24.61047 | entropy 0.2773 | loss -4.79416
derive | max_R: 48.293346
========> finish evaluate on one epoch<======
eval | loss:    35.35 | ppl: 2239708530772126.75 | accuracy:     0.81
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch9_step8602.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch9_step22000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch5_step5474.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch5_step14000.pth
| epoch  10 | lr 0.10 | raw loss 0.53 | loss 0.53 | ppl     1.69
| epoch  10 | lr 0.10 | raw loss 0.50 | loss 0.50 | ppl     1.65
| epoch  10 | lr 0.10 | raw loss 0.51 | loss 0.51 | ppl     1.66
| epoch  10 | lr 0.10 | raw loss 0.51 | loss 0.51 | ppl     1.66
| epoch  10 | lr 0.10 | raw loss 0.48 | loss 0.48 | ppl     1.62
| epoch  10 | lr 0.10 | raw loss 0.46 | loss 0.46 | ppl     1.58
| epoch  10 | lr 0.10 | raw loss 0.50 | loss 0.50 | ppl     1.66
| epoch  10 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.57
| epoch  10 | lr 0.10 | raw loss 0.47 | loss 0.47 | ppl     1.60
| epoch  10 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.57
| epoch  10 | lr 0.10 | raw loss 0.47 | loss 0.47 | ppl     1.59
| epoch  10 | lr 0.10 | raw loss 0.49 | loss 0.49 | ppl     1.63
| epoch  10 | lr 0.10 | raw loss 0.49 | loss 0.49 | ppl     1.63
| epoch  10 | lr 0.10 | raw loss 0.50 | loss 0.50 | ppl     1.64
| epoch  10 | lr 0.10 | raw loss 0.48 | loss 0.48 | ppl     1.62
====>train_shared<====== finish one epoch
| epoch  10 | lr 0.00035 | R 21.24291 | entropy 0.2819 | loss -6.58353
| epoch  10 | lr 0.00035 | R 21.31310 | entropy 0.2849 | loss -4.99477
| epoch  10 | lr 0.00035 | R 20.40752 | entropy 0.2834 | loss -8.07618
| epoch  10 | lr 0.00035 | R 22.73433 | entropy 0.2818 | loss -0.83522
| epoch  10 | lr 0.00035 | R 22.57854 | entropy 0.2821 | loss -3.19436
| epoch  10 | lr 0.00035 | R 22.43586 | entropy 0.2817 | loss -4.26610
| epoch  10 | lr 0.00035 | R 22.50382 | entropy 0.2814 | loss -6.27598
| epoch  10 | lr 0.00035 | R 21.87048 | entropy 0.2812 | loss -7.00463
| epoch  10 | lr 0.00035 | R 21.95480 | entropy 0.2811 | loss -5.88955
| epoch  10 | lr 0.00035 | R 21.53980 | entropy 0.2810 | loss -8.29577
| epoch  10 | lr 0.00035 | R 21.80737 | entropy 0.2817 | loss -7.32131
| epoch  10 | lr 0.00035 | R 22.22344 | entropy 0.2821 | loss -5.45453
| epoch  10 | lr 0.00035 | R 21.85612 | entropy 0.2829 | loss -7.61462
| epoch  10 | lr 0.00035 | R 22.16509 | entropy 0.2836 | loss -5.59654
| epoch  10 | lr 0.00035 | R 22.28782 | entropy 0.2839 | loss -3.58567
| epoch  10 | lr 0.00035 | R 21.79866 | entropy 0.2844 | loss -9.62103
| epoch  10 | lr 0.00035 | R 22.00122 | entropy 0.2848 | loss -5.50156
| epoch  10 | lr 0.00035 | R 21.04911 | entropy 0.2850 | loss -20.89934
| epoch  10 | lr 0.00035 | R 22.79182 | entropy 0.2839 | loss -2.43458
| epoch  10 | lr 0.00035 | R 22.45656 | entropy 0.2846 | loss -5.75317
| epoch  10 | lr 0.00035 | R 22.63831 | entropy 0.2840 | loss -5.89881
| epoch  10 | lr 0.00035 | R 21.49107 | entropy 0.2832 | loss -14.25908
| epoch  10 | lr 0.00035 | R 22.35701 | entropy 0.2831 | loss -5.06828
| epoch  10 | lr 0.00035 | R 22.04011 | entropy 0.2838 | loss -6.35335
| epoch  10 | lr 0.00035 | R 21.80980 | entropy 0.2843 | loss -6.96377
| epoch  10 | lr 0.00035 | R 22.22621 | entropy 0.2849 | loss -5.06363
| epoch  10 | lr 0.00035 | R 23.10454 | entropy 0.2852 | loss -0.48326
| epoch  10 | lr 0.00035 | R 22.02227 | entropy 0.2854 | loss -10.21490
| epoch  10 | lr 0.00035 | R 22.36658 | entropy 0.2863 | loss -3.84197
| epoch  10 | lr 0.00035 | R 22.34230 | entropy 0.2869 | loss -5.74261
| epoch  10 | lr 0.00035 | R 22.37570 | entropy 0.2863 | loss -6.10478
| epoch  10 | lr 0.00035 | R 22.57111 | entropy 0.2848 | loss -7.14765
| epoch  10 | lr 0.00035 | R 21.82034 | entropy 0.2839 | loss -12.72476
| epoch  10 | lr 0.00035 | R 21.98843 | entropy 0.2844 | loss -6.09503
| epoch  10 | lr 0.00035 | R 21.94679 | entropy 0.2837 | loss -8.71882
| epoch  10 | lr 0.00035 | R 22.05120 | entropy 0.2828 | loss -6.74943
| epoch  10 | lr 0.00035 | R 21.69334 | entropy 0.2838 | loss -8.94866
| epoch  10 | lr 0.00035 | R 22.26099 | entropy 0.2855 | loss -4.94917
| epoch  10 | lr 0.00035 | R 22.13768 | entropy 0.2871 | loss -5.57148
derive | max_R: 47.908024
========> finish evaluate on one epoch<======
eval | loss:    42.51 | ppl: 2900781026264668160.00 | accuracy:     0.78
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch10_step9384.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch10_step24000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch6_step16000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch6_step6256.pth
| epoch  11 | lr 0.10 | raw loss 0.44 | loss 0.44 | ppl     1.55
| epoch  11 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.54
| epoch  11 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.53
| epoch  11 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.58
| epoch  11 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.57
| epoch  11 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  11 | lr 0.10 | raw loss 0.51 | loss 0.51 | ppl     1.66
| epoch  11 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.53
| epoch  11 | lr 0.10 | raw loss 0.48 | loss 0.48 | ppl     1.61
| epoch  11 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.57
| epoch  11 | lr 0.10 | raw loss 0.47 | loss 0.47 | ppl     1.60
| epoch  11 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.57
| epoch  11 | lr 0.10 | raw loss 0.44 | loss 0.44 | ppl     1.55
| epoch  11 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  11 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.57
====>train_shared<====== finish one epoch
| epoch  11 | lr 0.00035 | R 25.51335 | entropy 0.2878 | loss -7.09496
| epoch  11 | lr 0.00035 | R 26.60166 | entropy 0.2873 | loss -6.26911
| epoch  11 | lr 0.00035 | R 25.76724 | entropy 0.2867 | loss -11.06150
| epoch  11 | lr 0.00035 | R 22.69276 | entropy 0.2888 | loss -11.08421
| epoch  11 | lr 0.00035 | R 21.13247 | entropy 0.2888 | loss -12.03315
| epoch  11 | lr 0.00035 | R 21.08629 | entropy 0.2864 | loss -11.08649
| epoch  11 | lr 0.00035 | R 20.84889 | entropy 0.2860 | loss -9.14320
| epoch  11 | lr 0.00035 | R 20.99344 | entropy 0.2857 | loss -5.14286
| epoch  11 | lr 0.00035 | R 21.07881 | entropy 0.2850 | loss -7.35753
| epoch  11 | lr 0.00035 | R 21.25402 | entropy 0.2841 | loss -9.75773
| epoch  11 | lr 0.00035 | R 20.88027 | entropy 0.2838 | loss -11.73259
| epoch  11 | lr 0.00035 | R 21.73254 | entropy 0.2838 | loss -3.16139
| epoch  11 | lr 0.00035 | R 21.47851 | entropy 0.2833 | loss -5.75373
| epoch  11 | lr 0.00035 | R 21.00926 | entropy 0.2833 | loss -11.60246
| epoch  11 | lr 0.00035 | R 21.04809 | entropy 0.2835 | loss -7.11537
| epoch  11 | lr 0.00035 | R 21.23347 | entropy 0.2837 | loss -4.30896
| epoch  11 | lr 0.00035 | R 20.22837 | entropy 0.2840 | loss -16.47562
| epoch  11 | lr 0.00035 | R 21.27505 | entropy 0.2833 | loss -4.59205
| epoch  11 | lr 0.00035 | R 20.81532 | entropy 0.2817 | loss -10.75395
| epoch  11 | lr 0.00035 | R 21.38033 | entropy 0.2807 | loss -4.88505
| epoch  11 | lr 0.00035 | R 21.02241 | entropy 0.2798 | loss -8.76720
| epoch  11 | lr 0.00035 | R 20.60358 | entropy 0.2788 | loss -10.42577
| epoch  11 | lr 0.00035 | R 20.92263 | entropy 0.2784 | loss -8.20005
| epoch  11 | lr 0.00035 | R 21.43752 | entropy 0.2790 | loss -4.27963
| epoch  11 | lr 0.00035 | R 21.39753 | entropy 0.2791 | loss -3.75218
| epoch  11 | lr 0.00035 | R 20.78862 | entropy 0.2785 | loss -11.23424
| epoch  11 | lr 0.00035 | R 20.94970 | entropy 0.2768 | loss -6.09010
| epoch  11 | lr 0.00035 | R 20.65158 | entropy 0.2769 | loss -11.28363
| epoch  11 | lr 0.00035 | R 21.00288 | entropy 0.2756 | loss -9.08491
| epoch  11 | lr 0.00035 | R 21.38198 | entropy 0.2756 | loss -4.20505
| epoch  11 | lr 0.00035 | R 21.24559 | entropy 0.2760 | loss -10.24114
| epoch  11 | lr 0.00035 | R 21.25245 | entropy 0.2755 | loss -4.92919
| epoch  11 | lr 0.00035 | R 21.61389 | entropy 0.2748 | loss -7.72420
| epoch  11 | lr 0.00035 | R 21.08916 | entropy 0.2739 | loss -10.33822
| epoch  11 | lr 0.00035 | R 20.72995 | entropy 0.2744 | loss -10.83190
| epoch  11 | lr 0.00035 | R 21.01135 | entropy 0.2743 | loss -6.44583
| epoch  11 | lr 0.00035 | R 21.06007 | entropy 0.2740 | loss -7.58172
| epoch  11 | lr 0.00035 | R 20.69732 | entropy 0.2738 | loss -10.88253
| epoch  11 | lr 0.00035 | R 21.71913 | entropy 0.2743 | loss -3.30422
derive | max_R: 49.237488
========> finish evaluate on one epoch<======
eval | loss:    39.40 | ppl: 129114881940678576.00 | accuracy:     0.80
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch11_step10166.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch11_step26000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch7_step7038.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch7_step18000.pth
| epoch  12 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.54
| epoch  12 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.51
| epoch  12 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.53
| epoch  12 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.53
| epoch  12 | lr 0.10 | raw loss 0.46 | loss 0.46 | ppl     1.59
| epoch  12 | lr 0.10 | raw loss 0.48 | loss 0.48 | ppl     1.62
| epoch  12 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  12 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.53
| epoch  12 | lr 0.10 | raw loss 0.44 | loss 0.44 | ppl     1.55
| epoch  12 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.53
| epoch  12 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  12 | lr 0.10 | raw loss 0.40 | loss 0.40 | ppl     1.49
| epoch  12 | lr 0.10 | raw loss 0.47 | loss 0.47 | ppl     1.60
| epoch  12 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.54
| epoch  12 | lr 0.10 | raw loss 0.45 | loss 0.45 | ppl     1.57
====>train_shared<====== finish one epoch
| epoch  12 | lr 0.00035 | R 26.19821 | entropy 0.2729 | loss 6.38341
| epoch  12 | lr 0.00035 | R 26.85496 | entropy 0.2744 | loss -3.24431
| epoch  12 | lr 0.00035 | R 26.91558 | entropy 0.2765 | loss -19.25515
| epoch  12 | lr 0.00035 | R 21.98702 | entropy 0.2756 | loss -25.57706
| epoch  12 | lr 0.00035 | R 22.12360 | entropy 0.2749 | loss -5.88802
| epoch  12 | lr 0.00035 | R 22.35891 | entropy 0.2748 | loss -3.19084
| epoch  12 | lr 0.00035 | R 22.43034 | entropy 0.2749 | loss -3.10399
| epoch  12 | lr 0.00035 | R 21.79557 | entropy 0.2748 | loss -9.83407
| epoch  12 | lr 0.00035 | R 22.31514 | entropy 0.2743 | loss -2.58307
| epoch  12 | lr 0.00035 | R 22.14775 | entropy 0.2744 | loss -4.50645
| epoch  12 | lr 0.00035 | R 21.85972 | entropy 0.2745 | loss -8.28813
| epoch  12 | lr 0.00035 | R 21.97220 | entropy 0.2742 | loss -6.03507
| epoch  12 | lr 0.00035 | R 22.13290 | entropy 0.2743 | loss -3.45764
| epoch  12 | lr 0.00035 | R 21.54246 | entropy 0.2742 | loss -10.08467
| epoch  12 | lr 0.00035 | R 21.97573 | entropy 0.2743 | loss -5.25268
| epoch  12 | lr 0.00035 | R 22.14271 | entropy 0.2735 | loss -2.64502
| epoch  12 | lr 0.00035 | R 22.12978 | entropy 0.2735 | loss -2.87140
| epoch  12 | lr 0.00035 | R 21.78969 | entropy 0.2736 | loss -8.48288
| epoch  12 | lr 0.00035 | R 22.45250 | entropy 0.2743 | loss -5.81028
| epoch  12 | lr 0.00035 | R 22.14930 | entropy 0.2748 | loss -5.62256
| epoch  12 | lr 0.00035 | R 22.18443 | entropy 0.2747 | loss -5.39962
| epoch  12 | lr 0.00035 | R 22.41359 | entropy 0.2739 | loss -4.88251
| epoch  12 | lr 0.00035 | R 22.54890 | entropy 0.2737 | loss -3.17290
| epoch  12 | lr 0.00035 | R 21.92857 | entropy 0.2730 | loss -7.00615
| epoch  12 | lr 0.00035 | R 22.24556 | entropy 0.2733 | loss -5.61980
| epoch  12 | lr 0.00035 | R 22.46276 | entropy 0.2742 | loss -3.82713
| epoch  12 | lr 0.00035 | R 22.93222 | entropy 0.2743 | loss -3.09958
| epoch  12 | lr 0.00035 | R 21.48040 | entropy 0.2721 | loss -13.04143
| epoch  12 | lr 0.00035 | R 22.35253 | entropy 0.2721 | loss -4.48033
| epoch  12 | lr 0.00035 | R 22.94889 | entropy 0.2719 | loss -1.65878
| epoch  12 | lr 0.00035 | R 22.86445 | entropy 0.2729 | loss -1.81146
| epoch  12 | lr 0.00035 | R 22.02967 | entropy 0.2756 | loss -14.42760
| epoch  12 | lr 0.00035 | R 22.90544 | entropy 0.2776 | loss -2.53579
| epoch  12 | lr 0.00035 | R 22.27036 | entropy 0.2787 | loss -8.50890
| epoch  12 | lr 0.00035 | R 22.87289 | entropy 0.2804 | loss -3.10320
| epoch  12 | lr 0.00035 | R 21.95857 | entropy 0.2818 | loss -8.74556
| epoch  12 | lr 0.00035 | R 22.46824 | entropy 0.2819 | loss -7.81197
| epoch  12 | lr 0.00035 | R 22.49631 | entropy 0.2803 | loss -6.02871
| epoch  12 | lr 0.00035 | R 22.53561 | entropy 0.2743 | loss -7.06840
derive | max_R: 49.820824
========> finish evaluate on one epoch<======
eval | loss:    32.57 | ppl: 140302856950137.56 | accuracy:     0.83
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch12_step10948.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch12_step28000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch8_step20000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch8_step7820.pth
| epoch  13 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.54
| epoch  13 | lr 0.10 | raw loss 0.37 | loss 0.37 | ppl     1.45
| epoch  13 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  13 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  13 | lr 0.10 | raw loss 0.41 | loss 0.41 | ppl     1.51
| epoch  13 | lr 0.10 | raw loss 0.41 | loss 0.41 | ppl     1.51
| epoch  13 | lr 0.10 | raw loss 0.41 | loss 0.41 | ppl     1.50
| epoch  13 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  13 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.48
| epoch  13 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.54
| epoch  13 | lr 0.10 | raw loss 0.41 | loss 0.41 | ppl     1.50
| epoch  13 | lr 0.10 | raw loss 0.44 | loss 0.44 | ppl     1.55
| epoch  13 | lr 0.10 | raw loss 0.41 | loss 0.41 | ppl     1.51
| epoch  13 | lr 0.10 | raw loss 0.43 | loss 0.43 | ppl     1.54
| epoch  13 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.53
====>train_shared<====== finish one epoch
| epoch  13 | lr 0.00035 | R 26.53238 | entropy 0.2683 | loss -23.55121
| epoch  13 | lr 0.00035 | R 26.86604 | entropy 0.2687 | loss -0.75289
| epoch  13 | lr 0.00035 | R 25.97980 | entropy 0.2740 | loss 3.84950
| epoch  13 | lr 0.00035 | R 29.94320 | entropy 0.2757 | loss -1.81268
| epoch  13 | lr 0.00035 | R 30.65740 | entropy 0.2773 | loss -5.19588
| epoch  13 | lr 0.00035 | R 29.54018 | entropy 0.2776 | loss -17.77121
| epoch  13 | lr 0.00035 | R 29.69391 | entropy 0.2765 | loss -12.98184
| epoch  13 | lr 0.00035 | R 30.68851 | entropy 0.2773 | loss -4.79588
| epoch  13 | lr 0.00035 | R 30.16664 | entropy 0.2785 | loss -13.10754
| epoch  13 | lr 0.00035 | R 29.77090 | entropy 0.2808 | loss -7.60617
| epoch  13 | lr 0.00035 | R 30.22382 | entropy 0.2830 | loss -9.34214
| epoch  13 | lr 0.00035 | R 30.22158 | entropy 0.2828 | loss -8.99217
| epoch  13 | lr 0.00035 | R 29.53798 | entropy 0.2827 | loss -16.52245
| epoch  13 | lr 0.00035 | R 30.38245 | entropy 0.2832 | loss -4.76673
| epoch  13 | lr 0.00035 | R 29.82124 | entropy 0.2841 | loss -13.72209
| epoch  13 | lr 0.00035 | R 30.07915 | entropy 0.2830 | loss -6.76733
| epoch  13 | lr 0.00035 | R 30.00479 | entropy 0.2823 | loss -12.68050
| epoch  13 | lr 0.00035 | R 29.66816 | entropy 0.2837 | loss -13.52200
| epoch  13 | lr 0.00035 | R 30.49499 | entropy 0.2830 | loss -5.07309
| epoch  13 | lr 0.00035 | R 30.40463 | entropy 0.2825 | loss -6.39561
| epoch  13 | lr 0.00035 | R 29.77500 | entropy 0.2818 | loss -17.68409
| epoch  13 | lr 0.00035 | R 30.32311 | entropy 0.2820 | loss -5.41119
| epoch  13 | lr 0.00035 | R 29.71767 | entropy 0.2824 | loss -10.96498
| epoch  13 | lr 0.00035 | R 30.19995 | entropy 0.2822 | loss -5.63727
| epoch  13 | lr 0.00035 | R 29.66174 | entropy 0.2810 | loss -14.57056
| epoch  13 | lr 0.00035 | R 30.91385 | entropy 0.2823 | loss -5.22228
| epoch  13 | lr 0.00035 | R 29.90077 | entropy 0.2823 | loss -14.64053
| epoch  13 | lr 0.00035 | R 29.54283 | entropy 0.2835 | loss -15.48021
| epoch  13 | lr 0.00035 | R 30.06314 | entropy 0.2860 | loss -11.79834
| epoch  13 | lr 0.00035 | R 31.15405 | entropy 0.2867 | loss -3.07572
| epoch  13 | lr 0.00035 | R 30.25081 | entropy 0.2876 | loss -11.99215
| epoch  13 | lr 0.00035 | R 30.83573 | entropy 0.2898 | loss -6.18895
| epoch  13 | lr 0.00035 | R 30.38369 | entropy 0.2900 | loss -9.13035
| epoch  13 | lr 0.00035 | R 30.63406 | entropy 0.2901 | loss -7.66288
| epoch  13 | lr 0.00035 | R 30.24103 | entropy 0.2900 | loss -9.20714
| epoch  13 | lr 0.00035 | R 29.58719 | entropy 0.2885 | loss -19.13125
| epoch  13 | lr 0.00035 | R 30.28628 | entropy 0.2898 | loss -7.75863
| epoch  13 | lr 0.00035 | R 30.08573 | entropy 0.2892 | loss -15.55840
| epoch  13 | lr 0.00035 | R 30.33564 | entropy 0.2911 | loss -8.70273
derive | max_R: 57.263992
========> finish evaluate on one epoch<======
eval | loss:    63.26 | ppl: 2966541842518194112008552448.00 | accuracy:     0.77
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch13_step11730.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch13_step30000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch9_step8602.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch9_step22000.pth
| epoch  14 | lr 0.10 | raw loss 0.37 | loss 0.37 | ppl     1.45
| epoch  14 | lr 0.10 | raw loss 0.44 | loss 0.44 | ppl     1.55
| epoch  14 | lr 0.10 | raw loss 0.36 | loss 0.36 | ppl     1.43
| epoch  14 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
| epoch  14 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.48
| epoch  14 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.48
| epoch  14 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  14 | lr 0.10 | raw loss 0.41 | loss 0.41 | ppl     1.50
| epoch  14 | lr 0.10 | raw loss 0.38 | loss 0.38 | ppl     1.47
| epoch  14 | lr 0.10 | raw loss 0.40 | loss 0.40 | ppl     1.50
| epoch  14 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
| epoch  14 | lr 0.10 | raw loss 0.42 | loss 0.42 | ppl     1.52
| epoch  14 | lr 0.10 | raw loss 0.44 | loss 0.44 | ppl     1.55
| epoch  14 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
| epoch  14 | lr 0.10 | raw loss 0.37 | loss 0.37 | ppl     1.45
====>train_shared<====== finish one epoch
| epoch  14 | lr 0.00035 | R 25.59603 | entropy 0.2882 | loss -5.49855
| epoch  14 | lr 0.00035 | R 26.66802 | entropy 0.2866 | loss 0.93147
| epoch  14 | lr 0.00035 | R 26.01769 | entropy 0.2862 | loss -10.79868
| epoch  14 | lr 0.00035 | R 26.43309 | entropy 0.2853 | loss -7.12443
| epoch  14 | lr 0.00035 | R 26.34649 | entropy 0.2858 | loss -9.69188
| epoch  14 | lr 0.00035 | R 26.28995 | entropy 0.2886 | loss -12.00451
| epoch  14 | lr 0.00035 | R 25.88512 | entropy 0.2906 | loss -8.72715
| epoch  14 | lr 0.00035 | R 26.70914 | entropy 0.2893 | loss -18.93510
| epoch  14 | lr 0.00035 | R 26.22270 | entropy 0.2875 | loss -13.20650
| epoch  14 | lr 0.00035 | R 27.76313 | entropy 0.2867 | loss -2.16421
| epoch  14 | lr 0.00035 | R 27.19506 | entropy 0.2870 | loss -9.97596
| epoch  14 | lr 0.00035 | R 26.39115 | entropy 0.2862 | loss -15.24278
| epoch  14 | lr 0.00035 | R 27.03549 | entropy 0.2860 | loss -13.49029
| epoch  14 | lr 0.00035 | R 27.09675 | entropy 0.2855 | loss -9.25329
| epoch  14 | lr 0.00035 | R 26.70048 | entropy 0.2844 | loss -15.17657
| epoch  14 | lr 0.00035 | R 26.68982 | entropy 0.2825 | loss -18.75676
| epoch  14 | lr 0.00035 | R 27.48178 | entropy 0.2793 | loss -7.83985
| epoch  14 | lr 0.00035 | R 26.85209 | entropy 0.2775 | loss -18.46414
| epoch  14 | lr 0.00035 | R 26.66210 | entropy 0.2751 | loss -10.29369
| epoch  14 | lr 0.00035 | R 27.93353 | entropy 0.2723 | loss -6.23825
| epoch  14 | lr 0.00035 | R 27.03318 | entropy 0.2693 | loss -10.33148
| epoch  14 | lr 0.00035 | R 26.44583 | entropy 0.2670 | loss -24.46145
| epoch  14 | lr 0.00035 | R 28.39078 | entropy 0.2656 | loss -4.07659
| epoch  14 | lr 0.00035 | R 28.19286 | entropy 0.2655 | loss -7.27279
| epoch  14 | lr 0.00035 | R 27.16827 | entropy 0.2650 | loss -14.39485
| epoch  14 | lr 0.00035 | R 27.57944 | entropy 0.2635 | loss -9.48369
| epoch  14 | lr 0.00035 | R 27.23966 | entropy 0.2614 | loss -17.93770
| epoch  14 | lr 0.00035 | R 27.70114 | entropy 0.2613 | loss -10.18543
| epoch  14 | lr 0.00035 | R 27.49878 | entropy 0.2611 | loss -16.33926
| epoch  14 | lr 0.00035 | R 25.90808 | entropy 0.2586 | loss -16.83611
| epoch  14 | lr 0.00035 | R 27.98310 | entropy 0.2588 | loss -8.39800
| epoch  14 | lr 0.00035 | R 28.16769 | entropy 0.2578 | loss -6.35097
| epoch  14 | lr 0.00035 | R 27.36915 | entropy 0.2581 | loss -10.11558
| epoch  14 | lr 0.00035 | R 27.75404 | entropy 0.2568 | loss -10.11910
| epoch  14 | lr 0.00035 | R 28.58745 | entropy 0.2556 | loss -6.44540
| epoch  14 | lr 0.00035 | R 27.07599 | entropy 0.2543 | loss -13.96429
| epoch  14 | lr 0.00035 | R 27.80780 | entropy 0.2539 | loss -8.40989
| epoch  14 | lr 0.00035 | R 27.97366 | entropy 0.2538 | loss -7.82727
| epoch  14 | lr 0.00035 | R 27.67129 | entropy 0.2531 | loss -19.21543
derive | max_R: 52.467617
========> finish evaluate on one epoch<======
eval | loss:   100.36 | ppl: 38607496446971399826916197627520611094888448.00 | accuracy:     0.58
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch14_step12512.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch14_step32000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch10_step9384.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch10_step24000.pth
| epoch  15 | lr 0.10 | raw loss 0.38 | loss 0.38 | ppl     1.46
| epoch  15 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
| epoch  15 | lr 0.10 | raw loss 0.37 | loss 0.37 | ppl     1.45
| epoch  15 | lr 0.10 | raw loss 0.36 | loss 0.36 | ppl     1.43
| epoch  15 | lr 0.10 | raw loss 0.36 | loss 0.36 | ppl     1.44
| epoch  15 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
| epoch  15 | lr 0.10 | raw loss 0.40 | loss 0.40 | ppl     1.49
| epoch  15 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
| epoch  15 | lr 0.10 | raw loss 0.35 | loss 0.35 | ppl     1.42
| epoch  15 | lr 0.10 | raw loss 0.37 | loss 0.37 | ppl     1.45
| epoch  15 | lr 0.10 | raw loss 0.38 | loss 0.38 | ppl     1.47
| epoch  15 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
| epoch  15 | lr 0.10 | raw loss 0.38 | loss 0.38 | ppl     1.46
| epoch  15 | lr 0.10 | raw loss 0.38 | loss 0.38 | ppl     1.46
| epoch  15 | lr 0.10 | raw loss 0.39 | loss 0.39 | ppl     1.47
====>train_shared<====== finish one epoch
| epoch  15 | lr 0.00035 | R 21.38931 | entropy 0.2506 | loss 7.29045
| epoch  15 | lr 0.00035 | R 23.31616 | entropy 0.2498 | loss -0.55592
| epoch  15 | lr 0.00035 | R 21.88208 | entropy 0.2502 | loss -10.58848
| epoch  15 | lr 0.00035 | R 24.44500 | entropy 0.2515 | loss -4.28512
| epoch  15 | lr 0.00035 | R 24.19489 | entropy 0.2527 | loss -14.16368
| epoch  15 | lr 0.00035 | R 24.82654 | entropy 0.2545 | loss -8.41966
| epoch  15 | lr 0.00035 | R 25.49609 | entropy 0.2551 | loss -5.66012
| epoch  15 | lr 0.00035 | R 25.24356 | entropy 0.2558 | loss -5.50602
| epoch  15 | lr 0.00035 | R 24.38563 | entropy 0.2562 | loss -15.49359
| epoch  15 | lr 0.00035 | R 24.85732 | entropy 0.2564 | loss -6.53741
| epoch  15 | lr 0.00035 | R 24.49889 | entropy 0.2561 | loss -9.65354
| epoch  15 | lr 0.00035 | R 24.46916 | entropy 0.2568 | loss -14.04793
| epoch  15 | lr 0.00035 | R 25.11014 | entropy 0.2569 | loss -9.17065
| epoch  15 | lr 0.00035 | R 24.21200 | entropy 0.2560 | loss -17.28928
| epoch  15 | lr 0.00035 | R 24.09792 | entropy 0.2547 | loss -15.54890
| epoch  15 | lr 0.00035 | R 24.41448 | entropy 0.2534 | loss -8.63705
| epoch  15 | lr 0.00035 | R 24.66379 | entropy 0.2519 | loss -9.75562
| epoch  15 | lr 0.00035 | R 23.99796 | entropy 0.2523 | loss -17.04438
| epoch  15 | lr 0.00035 | R 25.53829 | entropy 0.2525 | loss -2.24167
| epoch  15 | lr 0.00035 | R 24.64201 | entropy 0.2534 | loss -12.50811
| epoch  15 | lr 0.00035 | R 24.37902 | entropy 0.2553 | loss -11.34620
| epoch  15 | lr 0.00035 | R 24.69955 | entropy 0.2575 | loss -10.02906
| epoch  15 | lr 0.00035 | R 24.78616 | entropy 0.2588 | loss -11.61605
| epoch  15 | lr 0.00035 | R 25.34429 | entropy 0.2586 | loss -7.93379
| epoch  15 | lr 0.00035 | R 25.27691 | entropy 0.2583 | loss -9.60673
| epoch  15 | lr 0.00035 | R 24.08305 | entropy 0.2586 | loss -16.55362
| epoch  15 | lr 0.00035 | R 24.13528 | entropy 0.2599 | loss -18.69713
| epoch  15 | lr 0.00035 | R 25.50982 | entropy 0.2606 | loss -3.85235
| epoch  15 | lr 0.00035 | R 25.56778 | entropy 0.2615 | loss -7.14898
| epoch  15 | lr 0.00035 | R 24.76129 | entropy 0.2620 | loss -9.20901
| epoch  15 | lr 0.00035 | R 25.74428 | entropy 0.2616 | loss -5.89512
| epoch  15 | lr 0.00035 | R 24.50174 | entropy 0.2628 | loss -14.04433
| epoch  15 | lr 0.00035 | R 25.14922 | entropy 0.2641 | loss -12.19728
| epoch  15 | lr 0.00035 | R 25.75492 | entropy 0.2665 | loss -3.57140
| epoch  15 | lr 0.00035 | R 25.48259 | entropy 0.2665 | loss -14.90085
| epoch  15 | lr 0.00035 | R 26.27024 | entropy 0.2660 | loss -4.77217
| epoch  15 | lr 0.00035 | R 26.35904 | entropy 0.2655 | loss -7.13517
| epoch  15 | lr 0.00035 | R 25.14407 | entropy 0.2646 | loss -20.24342
| epoch  15 | lr 0.00035 | R 25.39505 | entropy 0.2608 | loss -12.96206
derive | max_R: 44.970043
========> finish evaluate on one epoch<======
eval | loss:    42.21 | ppl: 2136213207489790720.00 | accuracy:     0.81
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch15_step13294.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch15_step34000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch11_step10166.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch11_step26000.pth
| epoch  16 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.40
| epoch  16 | lr 0.09 | raw loss 0.37 | loss 0.37 | ppl     1.45
| epoch  16 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.40
| epoch  16 | lr 0.09 | raw loss 0.32 | loss 0.32 | ppl     1.37
| epoch  16 | lr 0.09 | raw loss 0.31 | loss 0.31 | ppl     1.37
| epoch  16 | lr 0.09 | raw loss 0.38 | loss 0.38 | ppl     1.46
| epoch  16 | lr 0.09 | raw loss 0.35 | loss 0.35 | ppl     1.42
| epoch  16 | lr 0.09 | raw loss 0.36 | loss 0.36 | ppl     1.43
| epoch  16 | lr 0.09 | raw loss 0.35 | loss 0.35 | ppl     1.42
| epoch  16 | lr 0.09 | raw loss 0.35 | loss 0.35 | ppl     1.42
| epoch  16 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.41
| epoch  16 | lr 0.09 | raw loss 0.32 | loss 0.32 | ppl     1.38
| epoch  16 | lr 0.09 | raw loss 0.35 | loss 0.35 | ppl     1.43
| epoch  16 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.40
| epoch  16 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.40
====>train_shared<====== finish one epoch
| epoch  16 | lr 0.00035 | R 25.69952 | entropy 0.2589 | loss -11.88541
| epoch  16 | lr 0.00035 | R 25.41426 | entropy 0.2602 | loss -17.56248
| epoch  16 | lr 0.00035 | R 23.61375 | entropy 0.2584 | loss -8.96251
| epoch  16 | lr 0.00035 | R 23.50858 | entropy 0.2620 | loss -9.47809
| epoch  16 | lr 0.00035 | R 23.05851 | entropy 0.2626 | loss -5.73332
| epoch  16 | lr 0.00035 | R 23.79361 | entropy 0.2621 | loss -1.40823
| epoch  16 | lr 0.00035 | R 23.69861 | entropy 0.2621 | loss -3.38260
| epoch  16 | lr 0.00035 | R 23.88717 | entropy 0.2619 | loss -4.41985
| epoch  16 | lr 0.00035 | R 23.73347 | entropy 0.2600 | loss -6.17055
| epoch  16 | lr 0.00035 | R 23.03751 | entropy 0.2599 | loss -9.25230
| epoch  16 | lr 0.00035 | R 23.30276 | entropy 0.2584 | loss -4.56234
| epoch  16 | lr 0.00035 | R 23.58074 | entropy 0.2575 | loss -3.82174
| epoch  16 | lr 0.00035 | R 23.50608 | entropy 0.2570 | loss -8.09446
| epoch  16 | lr 0.00035 | R 23.24323 | entropy 0.2576 | loss -12.99161
| epoch  16 | lr 0.00035 | R 24.12684 | entropy 0.2579 | loss -0.60211
| epoch  16 | lr 0.00035 | R 23.46248 | entropy 0.2569 | loss -7.58285
| epoch  16 | lr 0.00035 | R 23.29398 | entropy 0.2555 | loss -4.13841
| epoch  16 | lr 0.00035 | R 24.57076 | entropy 0.2552 | loss 0.29702
| epoch  16 | lr 0.00035 | R 23.61551 | entropy 0.2549 | loss -8.64748
| epoch  16 | lr 0.00035 | R 24.01665 | entropy 0.2535 | loss -1.77356
| epoch  16 | lr 0.00035 | R 23.72450 | entropy 0.2537 | loss -4.50000
| epoch  16 | lr 0.00035 | R 23.17767 | entropy 0.2553 | loss -7.01390
| epoch  16 | lr 0.00035 | R 24.38952 | entropy 0.2556 | loss -0.43635
| epoch  16 | lr 0.00035 | R 23.87634 | entropy 0.2552 | loss -6.03052
| epoch  16 | lr 0.00035 | R 23.71927 | entropy 0.2558 | loss -7.57094
| epoch  16 | lr 0.00035 | R 23.27847 | entropy 0.2558 | loss -7.79355
| epoch  16 | lr 0.00035 | R 23.53753 | entropy 0.2558 | loss -3.86405
| epoch  16 | lr 0.00035 | R 24.24790 | entropy 0.2543 | loss -4.10576
| epoch  16 | lr 0.00035 | R 23.62606 | entropy 0.2528 | loss -7.00883
| epoch  16 | lr 0.00035 | R 24.27377 | entropy 0.2517 | loss -2.87922
| epoch  16 | lr 0.00035 | R 23.32872 | entropy 0.2527 | loss -10.84714
| epoch  16 | lr 0.00035 | R 24.36949 | entropy 0.2534 | loss 0.27282
| epoch  16 | lr 0.00035 | R 23.47117 | entropy 0.2540 | loss -5.43461
| epoch  16 | lr 0.00035 | R 23.32974 | entropy 0.2535 | loss -6.15108
| epoch  16 | lr 0.00035 | R 24.47938 | entropy 0.2529 | loss 0.76454
| epoch  16 | lr 0.00035 | R 23.53073 | entropy 0.2535 | loss -8.68429
| epoch  16 | lr 0.00035 | R 23.11212 | entropy 0.2535 | loss -9.85865
| epoch  16 | lr 0.00035 | R 24.13421 | entropy 0.2522 | loss -0.75208
| epoch  16 | lr 0.00035 | R 23.61827 | entropy 0.2523 | loss -7.18393
derive | max_R: 42.466263
========> finish evaluate on one epoch<======
eval | loss:    39.83 | ppl: 198616513836048736.00 | accuracy:     0.79
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch16_step14076.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch16_step36000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch12_step10948.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch12_step28000.pth
| epoch  17 | lr 0.09 | raw loss 0.37 | loss 0.37 | ppl     1.45
| epoch  17 | lr 0.09 | raw loss 0.30 | loss 0.30 | ppl     1.35
| epoch  17 | lr 0.09 | raw loss 0.30 | loss 0.30 | ppl     1.35
| epoch  17 | lr 0.09 | raw loss 0.30 | loss 0.30 | ppl     1.35
| epoch  17 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.40
| epoch  17 | lr 0.09 | raw loss 0.33 | loss 0.33 | ppl     1.39
| epoch  17 | lr 0.09 | raw loss 0.33 | loss 0.33 | ppl     1.40
| epoch  17 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.41
| epoch  17 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.41
| epoch  17 | lr 0.09 | raw loss 0.31 | loss 0.31 | ppl     1.36
| epoch  17 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.40
| epoch  17 | lr 0.09 | raw loss 0.33 | loss 0.33 | ppl     1.39
| epoch  17 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.40
| epoch  17 | lr 0.09 | raw loss 0.34 | loss 0.34 | ppl     1.41
| epoch  17 | lr 0.09 | raw loss 0.33 | loss 0.33 | ppl     1.39
====>train_shared<====== finish one epoch
| epoch  17 | lr 0.00035 | R 27.87921 | entropy 0.2543 | loss 15.49335
| epoch  17 | lr 0.00035 | R 26.70373 | entropy 0.2550 | loss -20.68932
| epoch  17 | lr 0.00035 | R 24.73710 | entropy 0.2563 | loss -8.86873
| epoch  17 | lr 0.00035 | R 23.84915 | entropy 0.2599 | loss -15.43951
| epoch  17 | lr 0.00035 | R 23.39314 | entropy 0.2607 | loss -7.70091
| epoch  17 | lr 0.00035 | R 24.00471 | entropy 0.2628 | loss 1.22262
| epoch  17 | lr 0.00035 | R 23.47025 | entropy 0.2642 | loss -4.38762
| epoch  17 | lr 0.00035 | R 23.32013 | entropy 0.2660 | loss -4.56477
| epoch  17 | lr 0.00035 | R 23.45857 | entropy 0.2684 | loss -3.09087
| epoch  17 | lr 0.00035 | R 22.80930 | entropy 0.2680 | loss -8.87310
| epoch  17 | lr 0.00035 | R 23.95620 | entropy 0.2674 | loss -0.94864
| epoch  17 | lr 0.00035 | R 22.99225 | entropy 0.2675 | loss -6.16739
| epoch  17 | lr 0.00035 | R 22.68832 | entropy 0.2696 | loss -13.89423
| epoch  17 | lr 0.00035 | R 23.19392 | entropy 0.2713 | loss -8.47200
| epoch  17 | lr 0.00035 | R 23.60092 | entropy 0.2737 | loss -4.79994
| epoch  17 | lr 0.00035 | R 24.09736 | entropy 0.2778 | loss -0.93742
| epoch  17 | lr 0.00035 | R 23.37624 | entropy 0.2811 | loss -12.36039
| epoch  17 | lr 0.00035 | R 23.99345 | entropy 0.2827 | loss -8.98197
| epoch  17 | lr 0.00035 | R 23.60813 | entropy 0.2847 | loss -5.80957
| epoch  17 | lr 0.00035 | R 23.62163 | entropy 0.2878 | loss -12.76108
| epoch  17 | lr 0.00035 | R 24.63478 | entropy 0.2908 | loss -5.44945
| epoch  17 | lr 0.00035 | R 24.30703 | entropy 0.2909 | loss -9.32069
| epoch  17 | lr 0.00035 | R 24.79133 | entropy 0.2891 | loss -6.19201
| epoch  17 | lr 0.00035 | R 25.04157 | entropy 0.2853 | loss -2.72145
| epoch  17 | lr 0.00035 | R 24.70927 | entropy 0.2820 | loss -9.60765
| epoch  17 | lr 0.00035 | R 24.18954 | entropy 0.2782 | loss -18.81815
| epoch  17 | lr 0.00035 | R 25.09571 | entropy 0.2760 | loss -3.22933
| epoch  17 | lr 0.00035 | R 25.57935 | entropy 0.2744 | loss -2.37454
| epoch  17 | lr 0.00035 | R 24.05908 | entropy 0.2736 | loss -14.83064
| epoch  17 | lr 0.00035 | R 24.80551 | entropy 0.2754 | loss -2.48628
| epoch  17 | lr 0.00035 | R 25.76661 | entropy 0.2748 | loss -2.47380
| epoch  17 | lr 0.00035 | R 24.59272 | entropy 0.2739 | loss -13.67646
| epoch  17 | lr 0.00035 | R 25.52501 | entropy 0.2696 | loss -6.19106
| epoch  17 | lr 0.00035 | R 25.81004 | entropy 0.2680 | loss -4.43646
| epoch  17 | lr 0.00035 | R 25.49280 | entropy 0.2670 | loss -10.65909
| epoch  17 | lr 0.00035 | R 25.78806 | entropy 0.2663 | loss -7.81457
| epoch  17 | lr 0.00035 | R 25.67154 | entropy 0.2669 | loss -4.54732
| epoch  17 | lr 0.00035 | R 24.49608 | entropy 0.2686 | loss -11.63752
| epoch  17 | lr 0.00035 | R 25.17276 | entropy 0.2711 | loss -11.34153
derive | max_R: 54.950619
========> finish evaluate on one epoch<======
eval | loss:    35.13 | ppl: 1812600122441958.75 | accuracy:     0.82
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch17_step14858.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch17_step38000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch13_step11730.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch13_step30000.pth
| epoch  18 | lr 0.08 | raw loss 0.33 | loss 0.33 | ppl     1.40
| epoch  18 | lr 0.08 | raw loss 0.32 | loss 0.32 | ppl     1.38
| epoch  18 | lr 0.08 | raw loss 0.33 | loss 0.33 | ppl     1.39
| epoch  18 | lr 0.08 | raw loss 0.32 | loss 0.32 | ppl     1.38
| epoch  18 | lr 0.08 | raw loss 0.32 | loss 0.32 | ppl     1.37
| epoch  18 | lr 0.08 | raw loss 0.29 | loss 0.29 | ppl     1.34
| epoch  18 | lr 0.08 | raw loss 0.30 | loss 0.30 | ppl     1.36
| epoch  18 | lr 0.08 | raw loss 0.32 | loss 0.32 | ppl     1.37
| epoch  18 | lr 0.08 | raw loss 0.34 | loss 0.34 | ppl     1.41
| epoch  18 | lr 0.08 | raw loss 0.34 | loss 0.34 | ppl     1.40
| epoch  18 | lr 0.08 | raw loss 0.30 | loss 0.30 | ppl     1.35
| epoch  18 | lr 0.08 | raw loss 0.31 | loss 0.31 | ppl     1.37
| epoch  18 | lr 0.08 | raw loss 0.33 | loss 0.33 | ppl     1.38
| epoch  18 | lr 0.08 | raw loss 0.33 | loss 0.33 | ppl     1.40
| epoch  18 | lr 0.08 | raw loss 0.32 | loss 0.32 | ppl     1.37
====>train_shared<====== finish one epoch
| epoch  18 | lr 0.00035 | R 31.83517 | entropy 0.2711 | loss 11.24005
| epoch  18 | lr 0.00035 | R 30.79751 | entropy 0.2655 | loss -15.87722
| epoch  18 | lr 0.00035 | R 31.95623 | entropy 0.2653 | loss -3.14133
| epoch  18 | lr 0.00035 | R 32.98614 | entropy 0.2733 | loss -9.83078
| epoch  18 | lr 0.00035 | R 33.72528 | entropy 0.2709 | loss -12.58331
| epoch  18 | lr 0.00035 | R 34.22038 | entropy 0.2703 | loss -10.16539
| epoch  18 | lr 0.00035 | R 34.76427 | entropy 0.2705 | loss -3.20167
| epoch  18 | lr 0.00035 | R 35.13422 | entropy 0.2684 | loss -4.68767
| epoch  18 | lr 0.00035 | R 34.37198 | entropy 0.2672 | loss -9.06962
| epoch  18 | lr 0.00035 | R 34.29217 | entropy 0.2676 | loss -7.39942
| epoch  18 | lr 0.00035 | R 34.50894 | entropy 0.2687 | loss -7.52493
| epoch  18 | lr 0.00035 | R 33.06605 | entropy 0.2698 | loss -19.58043
| epoch  18 | lr 0.00035 | R 33.66545 | entropy 0.2732 | loss -8.01481
| epoch  18 | lr 0.00035 | R 34.34383 | entropy 0.2754 | loss -9.67337
| epoch  18 | lr 0.00035 | R 34.39703 | entropy 0.2747 | loss -7.37693
| epoch  18 | lr 0.00035 | R 34.33835 | entropy 0.2759 | loss -8.40795
| epoch  18 | lr 0.00035 | R 34.19696 | entropy 0.2746 | loss -12.50610
| epoch  18 | lr 0.00035 | R 33.93859 | entropy 0.2740 | loss -10.36833
| epoch  18 | lr 0.00035 | R 34.37532 | entropy 0.2729 | loss -4.20017
| epoch  18 | lr 0.00035 | R 34.32244 | entropy 0.2714 | loss -5.54493
| epoch  18 | lr 0.00035 | R 34.07964 | entropy 0.2721 | loss -9.03202
| epoch  18 | lr 0.00035 | R 35.05603 | entropy 0.2719 | loss -6.34741
| epoch  18 | lr 0.00035 | R 34.17275 | entropy 0.2698 | loss -14.46498
| epoch  18 | lr 0.00035 | R 33.78315 | entropy 0.2696 | loss -4.93043
| epoch  18 | lr 0.00035 | R 33.38844 | entropy 0.2723 | loss -18.39580
| epoch  18 | lr 0.00035 | R 33.17570 | entropy 0.2714 | loss -9.77264
| epoch  18 | lr 0.00035 | R 35.05788 | entropy 0.2721 | loss -1.04160
| epoch  18 | lr 0.00035 | R 34.02882 | entropy 0.2716 | loss -10.56977
| epoch  18 | lr 0.00035 | R 34.69056 | entropy 0.2715 | loss -6.94932
| epoch  18 | lr 0.00035 | R 34.58352 | entropy 0.2730 | loss -8.97450
| epoch  18 | lr 0.00035 | R 34.95268 | entropy 0.2740 | loss -6.64868
| epoch  18 | lr 0.00035 | R 33.89573 | entropy 0.2753 | loss -10.20280
| epoch  18 | lr 0.00035 | R 34.30149 | entropy 0.2756 | loss -7.97555
| epoch  18 | lr 0.00035 | R 33.98133 | entropy 0.2757 | loss -6.68066
| epoch  18 | lr 0.00035 | R 33.71665 | entropy 0.2771 | loss -9.81466
| epoch  18 | lr 0.00035 | R 34.29676 | entropy 0.2784 | loss -14.82331
| epoch  18 | lr 0.00035 | R 34.18476 | entropy 0.2792 | loss -8.83422
| epoch  18 | lr 0.00035 | R 33.63065 | entropy 0.2781 | loss -18.04545
| epoch  18 | lr 0.00035 | R 33.40197 | entropy 0.2794 | loss -12.02657
derive | max_R: 54.842480
========> finish evaluate on one epoch<======
eval | loss:    30.76 | ppl: 22873537258530.50 | accuracy:     0.85
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch18_step15640.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch18_step40000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch14_step32000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch14_step12512.pth
| epoch  19 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.33
| epoch  19 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  19 | lr 0.08 | raw loss 0.31 | loss 0.31 | ppl     1.36
| epoch  19 | lr 0.08 | raw loss 0.30 | loss 0.30 | ppl     1.35
| epoch  19 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.33
| epoch  19 | lr 0.08 | raw loss 0.30 | loss 0.30 | ppl     1.35
| epoch  19 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  19 | lr 0.08 | raw loss 0.27 | loss 0.27 | ppl     1.31
| epoch  19 | lr 0.08 | raw loss 0.29 | loss 0.29 | ppl     1.33
| epoch  19 | lr 0.08 | raw loss 0.33 | loss 0.33 | ppl     1.38
| epoch  19 | lr 0.08 | raw loss 0.30 | loss 0.30 | ppl     1.35
| epoch  19 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.33
| epoch  19 | lr 0.08 | raw loss 0.32 | loss 0.32 | ppl     1.38
| epoch  19 | lr 0.08 | raw loss 0.31 | loss 0.31 | ppl     1.36
| epoch  19 | lr 0.08 | raw loss 0.29 | loss 0.29 | ppl     1.34
====>train_shared<====== finish one epoch
| epoch  19 | lr 0.00035 | R 30.79702 | entropy 0.2813 | loss -32.94288
| epoch  19 | lr 0.00035 | R 32.82177 | entropy 0.2774 | loss -1.47451
| epoch  19 | lr 0.00035 | R 31.88152 | entropy 0.2792 | loss -4.82125
| epoch  19 | lr 0.00035 | R 36.10158 | entropy 0.2777 | loss -11.08920
| epoch  19 | lr 0.00035 | R 36.37291 | entropy 0.2764 | loss -12.13611
| epoch  19 | lr 0.00035 | R 36.96020 | entropy 0.2761 | loss -8.20347
| epoch  19 | lr 0.00035 | R 37.03227 | entropy 0.2763 | loss -6.82756
| epoch  19 | lr 0.00035 | R 36.43000 | entropy 0.2763 | loss -7.01286
| epoch  19 | lr 0.00035 | R 36.16125 | entropy 0.2772 | loss -9.70546
| epoch  19 | lr 0.00035 | R 36.76823 | entropy 0.2780 | loss -9.81955
| epoch  19 | lr 0.00035 | R 37.01396 | entropy 0.2761 | loss -12.05525
| epoch  19 | lr 0.00035 | R 36.52019 | entropy 0.2765 | loss -12.01695
| epoch  19 | lr 0.00035 | R 37.18203 | entropy 0.2787 | loss -8.66578
| epoch  19 | lr 0.00035 | R 37.10678 | entropy 0.2778 | loss -7.96304
| epoch  19 | lr 0.00035 | R 37.56582 | entropy 0.2771 | loss -9.93237
| epoch  19 | lr 0.00035 | R 36.38495 | entropy 0.2754 | loss -14.25907
| epoch  19 | lr 0.00035 | R 36.61874 | entropy 0.2748 | loss -10.21823
| epoch  19 | lr 0.00035 | R 37.46669 | entropy 0.2750 | loss -5.36628
| epoch  19 | lr 0.00035 | R 36.84232 | entropy 0.2747 | loss -9.42687
| epoch  19 | lr 0.00035 | R 36.28287 | entropy 0.2743 | loss -16.65556
| epoch  19 | lr 0.00035 | R 37.50527 | entropy 0.2714 | loss -2.05228
| epoch  19 | lr 0.00035 | R 38.26168 | entropy 0.2708 | loss -0.17335
| epoch  19 | lr 0.00035 | R 35.98154 | entropy 0.2703 | loss -18.23323
| epoch  19 | lr 0.00035 | R 36.06900 | entropy 0.2692 | loss -13.91425
| epoch  19 | lr 0.00035 | R 36.78022 | entropy 0.2698 | loss -10.02805
| epoch  19 | lr 0.00035 | R 36.49106 | entropy 0.2685 | loss -10.77401
| epoch  19 | lr 0.00035 | R 36.51215 | entropy 0.2684 | loss -15.49820
| epoch  19 | lr 0.00035 | R 37.32256 | entropy 0.2680 | loss -12.46791
| epoch  19 | lr 0.00035 | R 36.46788 | entropy 0.2685 | loss -9.37378
| epoch  19 | lr 0.00035 | R 36.57393 | entropy 0.2685 | loss -12.18008
| epoch  19 | lr 0.00035 | R 37.18509 | entropy 0.2671 | loss -5.63758
| epoch  19 | lr 0.00035 | R 36.69600 | entropy 0.2679 | loss -6.64069
| epoch  19 | lr 0.00035 | R 37.47069 | entropy 0.2677 | loss -5.52538
| epoch  19 | lr 0.00035 | R 37.01038 | entropy 0.2677 | loss -7.67768
| epoch  19 | lr 0.00035 | R 37.35207 | entropy 0.2683 | loss -8.68391
| epoch  19 | lr 0.00035 | R 36.98820 | entropy 0.2689 | loss -7.91033
| epoch  19 | lr 0.00035 | R 36.76815 | entropy 0.2688 | loss -12.78737
| epoch  19 | lr 0.00035 | R 38.22262 | entropy 0.2692 | loss 0.66239
| epoch  19 | lr 0.00035 | R 36.26593 | entropy 0.2686 | loss -14.81828
derive | max_R: 54.826305
========> finish evaluate on one epoch<======
eval | loss:    31.00 | ppl: 28956710274138.61 | accuracy:     0.84
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch19_step16422.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch19_step42000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch15_step34000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch15_step13294.pth
| epoch  20 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.30
| epoch  20 | lr 0.08 | raw loss 0.29 | loss 0.29 | ppl     1.33
| epoch  20 | lr 0.08 | raw loss 0.27 | loss 0.27 | ppl     1.31
| epoch  20 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.30
| epoch  20 | lr 0.08 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  20 | lr 0.08 | raw loss 0.31 | loss 0.31 | ppl     1.36
| epoch  20 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.30
| epoch  20 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.33
| epoch  20 | lr 0.08 | raw loss 0.29 | loss 0.29 | ppl     1.34
| epoch  20 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  20 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.30
| epoch  20 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.33
| epoch  20 | lr 0.08 | raw loss 0.29 | loss 0.29 | ppl     1.33
| epoch  20 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  20 | lr 0.08 | raw loss 0.27 | loss 0.27 | ppl     1.31
====>train_shared<====== finish one epoch
| epoch  20 | lr 0.00035 | R 28.43908 | entropy 0.2740 | loss -21.03796
| epoch  20 | lr 0.00035 | R 28.52690 | entropy 0.2741 | loss -13.31671
| epoch  20 | lr 0.00035 | R 29.32492 | entropy 0.2764 | loss -6.29866
| epoch  20 | lr 0.00035 | R 28.73937 | entropy 0.2763 | loss -15.47446
| epoch  20 | lr 0.00035 | R 28.39675 | entropy 0.2757 | loss -6.27346
| epoch  20 | lr 0.00035 | R 29.28871 | entropy 0.2764 | loss -2.54314
| epoch  20 | lr 0.00035 | R 28.54057 | entropy 0.2773 | loss -8.09791
| epoch  20 | lr 0.00035 | R 28.66880 | entropy 0.2770 | loss -6.61182
| epoch  20 | lr 0.00035 | R 27.40994 | entropy 0.2771 | loss -12.93920
| epoch  20 | lr 0.00035 | R 28.21551 | entropy 0.2759 | loss -9.43943
| epoch  20 | lr 0.00035 | R 28.33467 | entropy 0.2747 | loss -8.74604
| epoch  20 | lr 0.00035 | R 28.84591 | entropy 0.2752 | loss -11.28777
| epoch  20 | lr 0.00035 | R 28.17616 | entropy 0.2749 | loss -9.96344
| epoch  20 | lr 0.00035 | R 29.43206 | entropy 0.2760 | loss -2.04921
| epoch  20 | lr 0.00035 | R 28.57387 | entropy 0.2766 | loss -14.11325
| epoch  20 | lr 0.00035 | R 28.28423 | entropy 0.2756 | loss -10.83124
| epoch  20 | lr 0.00035 | R 28.27654 | entropy 0.2764 | loss -11.36093
| epoch  20 | lr 0.00035 | R 28.80234 | entropy 0.2756 | loss -11.24127
| epoch  20 | lr 0.00035 | R 28.59853 | entropy 0.2759 | loss -14.42277
| epoch  20 | lr 0.00035 | R 28.77942 | entropy 0.2776 | loss -4.88656
| epoch  20 | lr 0.00035 | R 28.35879 | entropy 0.2786 | loss -6.84135
| epoch  20 | lr 0.00035 | R 29.00766 | entropy 0.2810 | loss -6.53442
| epoch  20 | lr 0.00035 | R 29.26809 | entropy 0.2813 | loss -6.37026
| epoch  20 | lr 0.00035 | R 28.02441 | entropy 0.2813 | loss -16.02424
| epoch  20 | lr 0.00035 | R 28.42348 | entropy 0.2829 | loss -4.70130
| epoch  20 | lr 0.00035 | R 29.18943 | entropy 0.2857 | loss -5.00259
| epoch  20 | lr 0.00035 | R 27.59730 | entropy 0.2865 | loss -18.15339
| epoch  20 | lr 0.00035 | R 29.53872 | entropy 0.2866 | loss -0.45621
| epoch  20 | lr 0.00035 | R 28.88853 | entropy 0.2873 | loss -5.84258
| epoch  20 | lr 0.00035 | R 28.39807 | entropy 0.2867 | loss -13.01394
| epoch  20 | lr 0.00035 | R 28.03355 | entropy 0.2865 | loss -17.37191
| epoch  20 | lr 0.00035 | R 28.88089 | entropy 0.2871 | loss -6.38746
| epoch  20 | lr 0.00035 | R 28.47110 | entropy 0.2867 | loss -11.25057
| epoch  20 | lr 0.00035 | R 29.29304 | entropy 0.2851 | loss -6.04359
| epoch  20 | lr 0.00035 | R 29.10196 | entropy 0.2841 | loss -10.57254
| epoch  20 | lr 0.00035 | R 27.50395 | entropy 0.2843 | loss -24.76045
| epoch  20 | lr 0.00035 | R 28.94896 | entropy 0.2848 | loss -1.54452
| epoch  20 | lr 0.00035 | R 28.80846 | entropy 0.2866 | loss -8.19737
| epoch  20 | lr 0.00035 | R 29.29262 | entropy 0.2882 | loss -5.66982
derive | max_R: 52.129028
========> finish evaluate on one epoch<======
eval | loss:    43.41 | ppl: 7098599908327417856.00 | accuracy:     0.78
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch20_step17204.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch20_step44000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch16_step14076.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch16_step36000.pth
| epoch  21 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  21 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.30
| epoch  21 | lr 0.08 | raw loss 0.27 | loss 0.27 | ppl     1.31
| epoch  21 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  21 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.29
| epoch  21 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.33
| epoch  21 | lr 0.08 | raw loss 0.27 | loss 0.27 | ppl     1.31
| epoch  21 | lr 0.08 | raw loss 0.25 | loss 0.25 | ppl     1.28
| epoch  21 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.29
| epoch  21 | lr 0.08 | raw loss 0.25 | loss 0.25 | ppl     1.29
| epoch  21 | lr 0.08 | raw loss 0.28 | loss 0.28 | ppl     1.33
| epoch  21 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.30
| epoch  21 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.29
| epoch  21 | lr 0.08 | raw loss 0.26 | loss 0.26 | ppl     1.29
| epoch  21 | lr 0.08 | raw loss 0.24 | loss 0.24 | ppl     1.27
====>train_shared<====== finish one epoch
| epoch  21 | lr 0.00035 | R 28.77362 | entropy 0.2928 | loss -14.76679
| epoch  21 | lr 0.00035 | R 29.09984 | entropy 0.2933 | loss -6.57199
| epoch  21 | lr 0.00035 | R 28.25443 | entropy 0.2934 | loss -8.61201
| epoch  21 | lr 0.00035 | R 36.31561 | entropy 0.2905 | loss 6.28605
| epoch  21 | lr 0.00035 | R 38.02554 | entropy 0.2918 | loss -6.54197
| epoch  21 | lr 0.00035 | R 37.41128 | entropy 0.2920 | loss -15.21010
| epoch  21 | lr 0.00035 | R 36.66967 | entropy 0.2926 | loss -16.40780
| epoch  21 | lr 0.00035 | R 37.92694 | entropy 0.2931 | loss -7.84368
| epoch  21 | lr 0.00035 | R 35.77880 | entropy 0.2935 | loss -28.46891
| epoch  21 | lr 0.00035 | R 35.62055 | entropy 0.2925 | loss -18.58480
| epoch  21 | lr 0.00035 | R 37.84613 | entropy 0.2905 | loss -11.36178
| epoch  21 | lr 0.00035 | R 38.50234 | entropy 0.2903 | loss -11.27231
| epoch  21 | lr 0.00035 | R 37.47007 | entropy 0.2904 | loss -21.52161
| epoch  21 | lr 0.00035 | R 36.37681 | entropy 0.2894 | loss -23.71751
| epoch  21 | lr 0.00035 | R 37.55075 | entropy 0.2903 | loss -10.71614
| epoch  21 | lr 0.00035 | R 37.59259 | entropy 0.2912 | loss -10.42920
| epoch  21 | lr 0.00035 | R 37.41861 | entropy 0.2916 | loss -10.54658
| epoch  21 | lr 0.00035 | R 35.79813 | entropy 0.2908 | loss -25.93041
| epoch  21 | lr 0.00035 | R 38.86980 | entropy 0.2885 | loss -6.84858
| epoch  21 | lr 0.00035 | R 38.81100 | entropy 0.2883 | loss -14.65173
| epoch  21 | lr 0.00035 | R 38.00185 | entropy 0.2880 | loss -7.18925
| epoch  21 | lr 0.00035 | R 37.85933 | entropy 0.2880 | loss -11.85526
| epoch  21 | lr 0.00035 | R 37.88266 | entropy 0.2876 | loss -8.28701
| epoch  21 | lr 0.00035 | R 36.95573 | entropy 0.2873 | loss -18.59269
| epoch  21 | lr 0.00035 | R 38.04945 | entropy 0.2862 | loss -9.30588
| epoch  21 | lr 0.00035 | R 37.86614 | entropy 0.2858 | loss -8.10012
| epoch  21 | lr 0.00035 | R 37.27139 | entropy 0.2858 | loss -17.43939
| epoch  21 | lr 0.00035 | R 37.52737 | entropy 0.2860 | loss -18.87615
| epoch  21 | lr 0.00035 | R 37.69912 | entropy 0.2867 | loss -13.56038
| epoch  21 | lr 0.00035 | R 36.65173 | entropy 0.2866 | loss -18.40533
| epoch  21 | lr 0.00035 | R 36.54874 | entropy 0.2863 | loss -20.96028
| epoch  21 | lr 0.00035 | R 36.29962 | entropy 0.2873 | loss -25.42624
| epoch  21 | lr 0.00035 | R 37.82333 | entropy 0.2864 | loss -16.08423
| epoch  21 | lr 0.00035 | R 37.23707 | entropy 0.2854 | loss -13.07408
| epoch  21 | lr 0.00035 | R 37.76156 | entropy 0.2848 | loss -13.80942
| epoch  21 | lr 0.00035 | R 38.32107 | entropy 0.2848 | loss -7.43919
| epoch  21 | lr 0.00035 | R 38.41153 | entropy 0.2850 | loss -4.43333
| epoch  21 | lr 0.00035 | R 36.54374 | entropy 0.2850 | loss -25.88263
| epoch  21 | lr 0.00035 | R 37.82760 | entropy 0.2842 | loss -13.92093
derive | max_R: 57.475807
========> finish evaluate on one epoch<======
eval | loss:    33.15 | ppl: 250433999057474.34 | accuracy:     0.82
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch21_step17986.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch21_step46000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch17_step38000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch17_step14858.pth
| epoch  22 | lr 0.07 | raw loss 0.25 | loss 0.25 | ppl     1.29
| epoch  22 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.25
| epoch  22 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  22 | lr 0.07 | raw loss 0.25 | loss 0.25 | ppl     1.28
| epoch  22 | lr 0.07 | raw loss 0.25 | loss 0.25 | ppl     1.28
| epoch  22 | lr 0.07 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  22 | lr 0.07 | raw loss 0.27 | loss 0.27 | ppl     1.31
| epoch  22 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.25
| epoch  22 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.28
| epoch  22 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  22 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.25
| epoch  22 | lr 0.07 | raw loss 0.21 | loss 0.21 | ppl     1.24
| epoch  22 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  22 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  22 | lr 0.07 | raw loss 0.29 | loss 0.29 | ppl     1.33
====>train_shared<====== finish one epoch
| epoch  22 | lr 0.00035 | R 33.32371 | entropy 0.2838 | loss -30.11165
| epoch  22 | lr 0.00035 | R 36.11680 | entropy 0.2833 | loss 6.62436
| epoch  22 | lr 0.00035 | R 35.63125 | entropy 0.2818 | loss -12.13230
| epoch  22 | lr 0.00035 | R 37.58821 | entropy 0.2810 | loss -11.84627
| epoch  22 | lr 0.00035 | R 38.70732 | entropy 0.2813 | loss -6.60902
| epoch  22 | lr 0.00035 | R 38.20087 | entropy 0.2821 | loss -6.56693
| epoch  22 | lr 0.00035 | R 37.61677 | entropy 0.2830 | loss -12.79309
| epoch  22 | lr 0.00035 | R 37.54838 | entropy 0.2835 | loss -20.28004
| epoch  22 | lr 0.00035 | R 38.30047 | entropy 0.2837 | loss -8.77416
| epoch  22 | lr 0.00035 | R 38.22973 | entropy 0.2836 | loss -11.16802
| epoch  22 | lr 0.00035 | R 38.23264 | entropy 0.2835 | loss -12.86461
| epoch  22 | lr 0.00035 | R 37.25777 | entropy 0.2837 | loss -19.52197
| epoch  22 | lr 0.00035 | R 39.07519 | entropy 0.2841 | loss -3.24936
| epoch  22 | lr 0.00035 | R 38.14268 | entropy 0.2845 | loss -19.08962
| epoch  22 | lr 0.00035 | R 37.68079 | entropy 0.2847 | loss -15.91949
| epoch  22 | lr 0.00035 | R 38.29234 | entropy 0.2853 | loss -7.87652
| epoch  22 | lr 0.00035 | R 38.01477 | entropy 0.2855 | loss -16.28352
| epoch  22 | lr 0.00035 | R 38.27082 | entropy 0.2849 | loss -15.34324
| epoch  22 | lr 0.00035 | R 37.98032 | entropy 0.2848 | loss -6.81683
| epoch  22 | lr 0.00035 | R 38.26729 | entropy 0.2847 | loss -9.44556
| epoch  22 | lr 0.00035 | R 37.78522 | entropy 0.2849 | loss -12.31974
| epoch  22 | lr 0.00035 | R 38.61069 | entropy 0.2848 | loss -10.62016
| epoch  22 | lr 0.00035 | R 37.12186 | entropy 0.2845 | loss -18.09865
| epoch  22 | lr 0.00035 | R 37.82965 | entropy 0.2851 | loss -10.12477
| epoch  22 | lr 0.00035 | R 38.79759 | entropy 0.2871 | loss -7.31348
| epoch  22 | lr 0.00035 | R 40.03550 | entropy 0.2880 | loss -3.34921
| epoch  22 | lr 0.00035 | R 37.53098 | entropy 0.2885 | loss -26.08116
| epoch  22 | lr 0.00035 | R 39.19366 | entropy 0.2901 | loss -5.00472
| epoch  22 | lr 0.00035 | R 37.56287 | entropy 0.2909 | loss -22.04772
| epoch  22 | lr 0.00035 | R 38.03951 | entropy 0.2924 | loss -21.36658
| epoch  22 | lr 0.00035 | R 38.13818 | entropy 0.2908 | loss -9.35049
| epoch  22 | lr 0.00035 | R 39.00023 | entropy 0.2904 | loss -13.97588
| epoch  22 | lr 0.00035 | R 38.71671 | entropy 0.2901 | loss -19.30826
| epoch  22 | lr 0.00035 | R 38.53731 | entropy 0.2904 | loss -12.86023
| epoch  22 | lr 0.00035 | R 38.80166 | entropy 0.2901 | loss -18.58764
| epoch  22 | lr 0.00035 | R 38.83658 | entropy 0.2885 | loss -11.05452
| epoch  22 | lr 0.00035 | R 37.89984 | entropy 0.2881 | loss -16.52365
| epoch  22 | lr 0.00035 | R 38.01000 | entropy 0.2884 | loss -16.33972
| epoch  22 | lr 0.00035 | R 39.43979 | entropy 0.2875 | loss -8.64989
derive | max_R: 60.498486
========> finish evaluate on one epoch<======
eval | loss:    27.31 | ppl: 723831337886.77 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch22_step18768.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch22_step48000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch18_step40000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch18_step15640.pth
| epoch  23 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  23 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  23 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.28
| epoch  23 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.24
| epoch  23 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  23 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.25
| epoch  23 | lr 0.07 | raw loss 0.25 | loss 0.25 | ppl     1.28
| epoch  23 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  23 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  23 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  23 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  23 | lr 0.07 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  23 | lr 0.07 | raw loss 0.28 | loss 0.28 | ppl     1.32
| epoch  23 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.24
| epoch  23 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.25
====>train_shared<====== finish one epoch
| epoch  23 | lr 0.00035 | R 32.24551 | entropy 0.2874 | loss -11.53945
| epoch  23 | lr 0.00035 | R 34.69860 | entropy 0.2874 | loss 0.43374
| epoch  23 | lr 0.00035 | R 33.23704 | entropy 0.2869 | loss -15.63134
| epoch  23 | lr 0.00035 | R 37.15745 | entropy 0.2874 | loss -10.60593
| epoch  23 | lr 0.00035 | R 35.92229 | entropy 0.2885 | loss -21.16086
| epoch  23 | lr 0.00035 | R 37.69104 | entropy 0.2883 | loss -5.12066
| epoch  23 | lr 0.00035 | R 37.14314 | entropy 0.2882 | loss -18.88022
| epoch  23 | lr 0.00035 | R 38.14193 | entropy 0.2880 | loss -3.02994
| epoch  23 | lr 0.00035 | R 37.54615 | entropy 0.2880 | loss -15.72684
| epoch  23 | lr 0.00035 | R 37.53761 | entropy 0.2873 | loss -12.84743
| epoch  23 | lr 0.00035 | R 36.01715 | entropy 0.2867 | loss -16.54914
| epoch  23 | lr 0.00035 | R 37.79469 | entropy 0.2862 | loss -6.92084
| epoch  23 | lr 0.00035 | R 38.04926 | entropy 0.2851 | loss -10.42069
| epoch  23 | lr 0.00035 | R 38.14041 | entropy 0.2845 | loss -8.56142
| epoch  23 | lr 0.00035 | R 37.20959 | entropy 0.2839 | loss -20.97197
| epoch  23 | lr 0.00035 | R 38.04741 | entropy 0.2838 | loss -6.99378
| epoch  23 | lr 0.00035 | R 38.19833 | entropy 0.2845 | loss -9.48723
| epoch  23 | lr 0.00035 | R 38.37287 | entropy 0.2838 | loss -13.31908
| epoch  23 | lr 0.00035 | R 38.45259 | entropy 0.2832 | loss -9.32850
| epoch  23 | lr 0.00035 | R 36.85537 | entropy 0.2829 | loss -31.99244
| epoch  23 | lr 0.00035 | R 37.66092 | entropy 0.2818 | loss -10.56304
| epoch  23 | lr 0.00035 | R 37.45770 | entropy 0.2810 | loss -5.74647
| epoch  23 | lr 0.00035 | R 39.47571 | entropy 0.2819 | loss -1.02483
| epoch  23 | lr 0.00035 | R 38.80864 | entropy 0.2826 | loss -5.14777
| epoch  23 | lr 0.00035 | R 37.68202 | entropy 0.2826 | loss -16.60714
| epoch  23 | lr 0.00035 | R 38.49201 | entropy 0.2824 | loss -10.24958
| epoch  23 | lr 0.00035 | R 38.19173 | entropy 0.2824 | loss -7.72470
| epoch  23 | lr 0.00035 | R 38.93194 | entropy 0.2826 | loss -7.86330
| epoch  23 | lr 0.00035 | R 38.56645 | entropy 0.2826 | loss -15.92200
| epoch  23 | lr 0.00035 | R 36.55568 | entropy 0.2829 | loss -27.53553
| epoch  23 | lr 0.00035 | R 38.71206 | entropy 0.2829 | loss -3.88409
| epoch  23 | lr 0.00035 | R 37.90338 | entropy 0.2813 | loss -19.84489
| epoch  23 | lr 0.00035 | R 36.99306 | entropy 0.2822 | loss -10.48150
| epoch  23 | lr 0.00035 | R 36.39871 | entropy 0.2809 | loss -15.28408
| epoch  23 | lr 0.00035 | R 37.47985 | entropy 0.2788 | loss -14.37822
| epoch  23 | lr 0.00035 | R 38.44398 | entropy 0.2781 | loss -11.27382
| epoch  23 | lr 0.00035 | R 38.28974 | entropy 0.2769 | loss -10.46610
| epoch  23 | lr 0.00035 | R 38.21847 | entropy 0.2760 | loss -9.43986
| epoch  23 | lr 0.00035 | R 38.12751 | entropy 0.2761 | loss -12.76236
derive | max_R: 65.591446
========> finish evaluate on one epoch<======
eval | loss:    27.87 | ppl: 1270406746220.57 | accuracy:     0.85
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch23_step19550.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch23_step50000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch19_step42000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch19_step16422.pth
| epoch  24 | lr 0.07 | raw loss 0.21 | loss 0.21 | ppl     1.24
| epoch  24 | lr 0.07 | raw loss 0.21 | loss 0.21 | ppl     1.24
| epoch  24 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  24 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  24 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.25
| epoch  24 | lr 0.07 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  24 | lr 0.07 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  24 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  24 | lr 0.07 | raw loss 0.24 | loss 0.24 | ppl     1.27
| epoch  24 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  24 | lr 0.07 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  24 | lr 0.07 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  24 | lr 0.07 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  24 | lr 0.07 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  24 | lr 0.07 | raw loss 0.22 | loss 0.22 | ppl     1.25
====>train_shared<====== finish one epoch
| epoch  24 | lr 0.00035 | R 33.40109 | entropy 0.2718 | loss -30.17830
| epoch  24 | lr 0.00035 | R 34.51468 | entropy 0.2732 | loss -11.73447
| epoch  24 | lr 0.00035 | R 34.51112 | entropy 0.2721 | loss -5.46205
| epoch  24 | lr 0.00035 | R 37.26152 | entropy 0.2725 | loss -7.83535
| epoch  24 | lr 0.00035 | R 37.30077 | entropy 0.2733 | loss -17.61297
| epoch  24 | lr 0.00035 | R 36.94671 | entropy 0.2743 | loss -11.43931
| epoch  24 | lr 0.00035 | R 36.25956 | entropy 0.2732 | loss -20.63758
| epoch  24 | lr 0.00035 | R 37.66030 | entropy 0.2712 | loss -8.92877
| epoch  24 | lr 0.00035 | R 37.46649 | entropy 0.2706 | loss -13.31643
| epoch  24 | lr 0.00035 | R 36.76877 | entropy 0.2695 | loss -18.49778
| epoch  24 | lr 0.00035 | R 35.57759 | entropy 0.2693 | loss -17.34424
| epoch  24 | lr 0.00035 | R 37.55936 | entropy 0.2670 | loss -4.62954
| epoch  24 | lr 0.00035 | R 36.97182 | entropy 0.2656 | loss -9.15289
| epoch  24 | lr 0.00035 | R 37.37427 | entropy 0.2646 | loss -11.79699
| epoch  24 | lr 0.00035 | R 36.68543 | entropy 0.2629 | loss -17.40125
| epoch  24 | lr 0.00035 | R 36.94809 | entropy 0.2613 | loss -14.95765
| epoch  24 | lr 0.00035 | R 36.64594 | entropy 0.2606 | loss -15.69861
| epoch  24 | lr 0.00035 | R 36.57917 | entropy 0.2604 | loss -11.48415
| epoch  24 | lr 0.00035 | R 36.68032 | entropy 0.2590 | loss -16.03011
| epoch  24 | lr 0.00035 | R 36.97928 | entropy 0.2592 | loss -8.47265
| epoch  24 | lr 0.00035 | R 36.69804 | entropy 0.2582 | loss -16.72161
| epoch  24 | lr 0.00035 | R 37.69433 | entropy 0.2579 | loss -11.51329
| epoch  24 | lr 0.00035 | R 37.18209 | entropy 0.2547 | loss -12.66264
| epoch  24 | lr 0.00035 | R 38.18143 | entropy 0.2524 | loss -9.51038
| epoch  24 | lr 0.00035 | R 36.56360 | entropy 0.2489 | loss -21.50329
| epoch  24 | lr 0.00035 | R 36.69889 | entropy 0.2484 | loss -17.40432
| epoch  24 | lr 0.00035 | R 36.09323 | entropy 0.2470 | loss -14.78998
| epoch  24 | lr 0.00035 | R 37.05939 | entropy 0.2461 | loss -10.53585
| epoch  24 | lr 0.00035 | R 37.58741 | entropy 0.2455 | loss -8.98614
| epoch  24 | lr 0.00035 | R 38.14563 | entropy 0.2437 | loss -9.86253
| epoch  24 | lr 0.00035 | R 37.17577 | entropy 0.2425 | loss -12.29165
| epoch  24 | lr 0.00035 | R 37.38470 | entropy 0.2418 | loss -9.42747
| epoch  24 | lr 0.00035 | R 37.80532 | entropy 0.2401 | loss -8.03038
| epoch  24 | lr 0.00035 | R 36.39140 | entropy 0.2400 | loss -16.91386
| epoch  24 | lr 0.00035 | R 36.14098 | entropy 0.2401 | loss -6.25401
| epoch  24 | lr 0.00035 | R 36.64126 | entropy 0.2400 | loss -12.92345
| epoch  24 | lr 0.00035 | R 36.40644 | entropy 0.2392 | loss -16.74330
| epoch  24 | lr 0.00035 | R 37.09700 | entropy 0.2380 | loss -14.90766
| epoch  24 | lr 0.00035 | R 37.61441 | entropy 0.2373 | loss -5.74810
derive | max_R: 56.514126
========> finish evaluate on one epoch<======
eval | loss:    38.62 | ppl: 59087895657957720.00 | accuracy:     0.80
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch24_step20332.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch24_step52000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch20_step44000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch20_step17204.pth
| epoch  25 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  25 | lr 0.06 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  25 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  25 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  25 | lr 0.06 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  25 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  25 | lr 0.06 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  25 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  25 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  25 | lr 0.06 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  25 | lr 0.06 | raw loss 0.23 | loss 0.23 | ppl     1.26
| epoch  25 | lr 0.06 | raw loss 0.24 | loss 0.24 | ppl     1.28
| epoch  25 | lr 0.06 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  25 | lr 0.06 | raw loss 0.22 | loss 0.22 | ppl     1.24
| epoch  25 | lr 0.06 | raw loss 0.22 | loss 0.22 | ppl     1.25
====>train_shared<====== finish one epoch
| epoch  25 | lr 0.00035 | R 32.08103 | entropy 0.2370 | loss -33.09385
| epoch  25 | lr 0.00035 | R 33.64004 | entropy 0.2372 | loss -2.16777
| epoch  25 | lr 0.00035 | R 31.92935 | entropy 0.2393 | loss -22.42740
| epoch  25 | lr 0.00035 | R 34.41590 | entropy 0.2365 | loss -3.86207
| epoch  25 | lr 0.00035 | R 34.07169 | entropy 0.2351 | loss -15.75575
| epoch  25 | lr 0.00035 | R 34.43636 | entropy 0.2354 | loss -13.42461
| epoch  25 | lr 0.00035 | R 34.93458 | entropy 0.2355 | loss -7.23079
| epoch  25 | lr 0.00035 | R 34.08917 | entropy 0.2352 | loss -15.88853
| epoch  25 | lr 0.00035 | R 35.18125 | entropy 0.2357 | loss -5.95055
| epoch  25 | lr 0.00035 | R 35.25341 | entropy 0.2363 | loss -9.04984
| epoch  25 | lr 0.00035 | R 35.27727 | entropy 0.2368 | loss -10.67880
| epoch  25 | lr 0.00035 | R 34.67066 | entropy 0.2373 | loss -13.26479
| epoch  25 | lr 0.00035 | R 35.91396 | entropy 0.2373 | loss -4.85662
| epoch  25 | lr 0.00035 | R 35.24789 | entropy 0.2372 | loss -12.51394
| epoch  25 | lr 0.00035 | R 35.51704 | entropy 0.2378 | loss -4.77612
| epoch  25 | lr 0.00035 | R 36.68612 | entropy 0.2388 | loss -1.21625
| epoch  25 | lr 0.00035 | R 34.33023 | entropy 0.2391 | loss -19.15446
| epoch  25 | lr 0.00035 | R 35.71694 | entropy 0.2395 | loss -4.09029
| epoch  25 | lr 0.00035 | R 34.77342 | entropy 0.2403 | loss -14.73099
| epoch  25 | lr 0.00035 | R 34.96388 | entropy 0.2401 | loss -6.93344
| epoch  25 | lr 0.00035 | R 35.81975 | entropy 0.2403 | loss -7.35156
| epoch  25 | lr 0.00035 | R 35.16033 | entropy 0.2410 | loss -13.16719
| epoch  25 | lr 0.00035 | R 34.99558 | entropy 0.2417 | loss -12.06010
| epoch  25 | lr 0.00035 | R 35.93812 | entropy 0.2417 | loss -3.67575
| epoch  25 | lr 0.00035 | R 34.92850 | entropy 0.2414 | loss -15.98962
| epoch  25 | lr 0.00035 | R 35.08789 | entropy 0.2417 | loss -6.90899
| epoch  25 | lr 0.00035 | R 35.72523 | entropy 0.2416 | loss -10.06891
| epoch  25 | lr 0.00035 | R 35.06717 | entropy 0.2411 | loss -16.36038
| epoch  25 | lr 0.00035 | R 36.02121 | entropy 0.2407 | loss -7.88263
| epoch  25 | lr 0.00035 | R 34.78426 | entropy 0.2417 | loss -20.09989
| epoch  25 | lr 0.00035 | R 36.39286 | entropy 0.2426 | loss -2.20171
| epoch  25 | lr 0.00035 | R 36.87376 | entropy 0.2417 | loss -4.80371
| epoch  25 | lr 0.00035 | R 34.96103 | entropy 0.2415 | loss -23.39963
| epoch  25 | lr 0.00035 | R 33.71323 | entropy 0.2418 | loss -18.23166
| epoch  25 | lr 0.00035 | R 36.54123 | entropy 0.2420 | loss -3.19760
| epoch  25 | lr 0.00035 | R 35.32219 | entropy 0.2406 | loss -22.10703
| epoch  25 | lr 0.00035 | R 35.99448 | entropy 0.2392 | loss -14.49905
| epoch  25 | lr 0.00035 | R 35.76108 | entropy 0.2387 | loss -9.23551
| epoch  25 | lr 0.00035 | R 37.35745 | entropy 0.2390 | loss -4.63926
derive | max_R: 61.093422
========> finish evaluate on one epoch<======
eval | loss:    26.93 | ppl: 496147300599.79 | accuracy:     0.87
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch25_step21114.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch25_step54000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch21_step46000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch21_step17986.pth
| epoch  26 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  26 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  26 | lr 0.06 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  26 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  26 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  26 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  26 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  26 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  26 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  26 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  26 | lr 0.06 | raw loss 0.22 | loss 0.22 | ppl     1.25
| epoch  26 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  26 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  26 | lr 0.06 | raw loss 0.22 | loss 0.22 | ppl     1.24
| epoch  26 | lr 0.06 | raw loss 0.21 | loss 0.21 | ppl     1.24
====>train_shared<====== finish one epoch
| epoch  26 | lr 0.00035 | R 32.47314 | entropy 0.2359 | loss 4.87203
| epoch  26 | lr 0.00035 | R 34.02762 | entropy 0.2352 | loss -3.12762
| epoch  26 | lr 0.00035 | R 34.20440 | entropy 0.2357 | loss -6.93783
| epoch  26 | lr 0.00035 | R 34.41880 | entropy 0.2343 | loss -12.22891
| epoch  26 | lr 0.00035 | R 34.32870 | entropy 0.2319 | loss -9.69663
| epoch  26 | lr 0.00035 | R 34.10677 | entropy 0.2313 | loss -11.18787
| epoch  26 | lr 0.00035 | R 33.35094 | entropy 0.2299 | loss -10.07183
| epoch  26 | lr 0.00035 | R 33.36377 | entropy 0.2294 | loss -16.29113
| epoch  26 | lr 0.00035 | R 33.06775 | entropy 0.2294 | loss -10.51491
| epoch  26 | lr 0.00035 | R 33.08968 | entropy 0.2286 | loss -12.02907
| epoch  26 | lr 0.00035 | R 34.87244 | entropy 0.2281 | loss -2.24087
| epoch  26 | lr 0.00035 | R 35.06200 | entropy 0.2275 | loss -2.23311
| epoch  26 | lr 0.00035 | R 33.56800 | entropy 0.2266 | loss -18.22416
| epoch  26 | lr 0.00035 | R 33.75782 | entropy 0.2259 | loss -9.27492
| epoch  26 | lr 0.00035 | R 34.01228 | entropy 0.2253 | loss -9.45209
| epoch  26 | lr 0.00035 | R 34.01858 | entropy 0.2253 | loss -10.36738
| epoch  26 | lr 0.00035 | R 33.13452 | entropy 0.2253 | loss -12.21751
| epoch  26 | lr 0.00035 | R 33.39144 | entropy 0.2245 | loss -13.92305
| epoch  26 | lr 0.00035 | R 35.06280 | entropy 0.2234 | loss -4.61576
| epoch  26 | lr 0.00035 | R 33.89232 | entropy 0.2236 | loss -9.18785
| epoch  26 | lr 0.00035 | R 34.06147 | entropy 0.2232 | loss -9.86457
| epoch  26 | lr 0.00035 | R 33.46664 | entropy 0.2225 | loss -13.20783
| epoch  26 | lr 0.00035 | R 34.01245 | entropy 0.2224 | loss -7.62361
| epoch  26 | lr 0.00035 | R 34.01078 | entropy 0.2219 | loss -11.57090
| epoch  26 | lr 0.00035 | R 34.51181 | entropy 0.2212 | loss -6.58596
| epoch  26 | lr 0.00035 | R 33.73121 | entropy 0.2205 | loss -21.17703
| epoch  26 | lr 0.00035 | R 33.67788 | entropy 0.2206 | loss -13.47207
| epoch  26 | lr 0.00035 | R 34.30413 | entropy 0.2208 | loss -10.33817
| epoch  26 | lr 0.00035 | R 34.13677 | entropy 0.2201 | loss -10.26535
| epoch  26 | lr 0.00035 | R 34.30200 | entropy 0.2188 | loss -9.17401
| epoch  26 | lr 0.00035 | R 34.03907 | entropy 0.2184 | loss -10.25251
| epoch  26 | lr 0.00035 | R 34.85548 | entropy 0.2183 | loss -7.64922
| epoch  26 | lr 0.00035 | R 34.37942 | entropy 0.2184 | loss -7.48183
| epoch  26 | lr 0.00035 | R 33.88574 | entropy 0.2186 | loss -8.43924
| epoch  26 | lr 0.00035 | R 34.26822 | entropy 0.2182 | loss -7.49419
| epoch  26 | lr 0.00035 | R 33.65646 | entropy 0.2164 | loss -10.75370
| epoch  26 | lr 0.00035 | R 34.27792 | entropy 0.2165 | loss -9.41822
| epoch  26 | lr 0.00035 | R 32.68851 | entropy 0.2166 | loss -20.05938
| epoch  26 | lr 0.00035 | R 33.84551 | entropy 0.2165 | loss -9.48782
derive | max_R: 59.021194
========> finish evaluate on one epoch<======
eval | loss:    27.44 | ppl: 823133610435.81 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch26_step21896.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch26_step56000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch22_step48000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch22_step18768.pth
| epoch  27 | lr 0.06 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  27 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  27 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  27 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  27 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  27 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  27 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  27 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  27 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  27 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  27 | lr 0.06 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  27 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.19
| epoch  27 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  27 | lr 0.06 | raw loss 0.21 | loss 0.21 | ppl     1.23
| epoch  27 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
====>train_shared<====== finish one epoch
| epoch  27 | lr 0.00035 | R 34.49555 | entropy 0.2141 | loss -7.80428
| epoch  27 | lr 0.00035 | R 36.10330 | entropy 0.2137 | loss -16.43654
| epoch  27 | lr 0.00035 | R 33.67173 | entropy 0.2144 | loss -6.23481
| epoch  27 | lr 0.00035 | R 38.01522 | entropy 0.2138 | loss -15.96604
| epoch  27 | lr 0.00035 | R 40.31094 | entropy 0.2134 | loss -8.10665
| epoch  27 | lr 0.00035 | R 39.60632 | entropy 0.2133 | loss -12.06814
| epoch  27 | lr 0.00035 | R 40.79106 | entropy 0.2130 | loss -6.78623
| epoch  27 | lr 0.00035 | R 38.52720 | entropy 0.2122 | loss -27.01605
| epoch  27 | lr 0.00035 | R 39.59068 | entropy 0.2116 | loss -14.42751
| epoch  27 | lr 0.00035 | R 39.70064 | entropy 0.2111 | loss -12.91474
| epoch  27 | lr 0.00035 | R 39.48411 | entropy 0.2110 | loss -14.65973
| epoch  27 | lr 0.00035 | R 39.17624 | entropy 0.2108 | loss -22.65595
| epoch  27 | lr 0.00035 | R 39.72680 | entropy 0.2102 | loss -13.23194
| epoch  27 | lr 0.00035 | R 40.75025 | entropy 0.2103 | loss -5.76170
| epoch  27 | lr 0.00035 | R 39.53367 | entropy 0.2103 | loss -21.27637
| epoch  27 | lr 0.00035 | R 39.83384 | entropy 0.2106 | loss -15.35419
| epoch  27 | lr 0.00035 | R 40.56020 | entropy 0.2113 | loss -9.96419
| epoch  27 | lr 0.00035 | R 40.10095 | entropy 0.2109 | loss -13.45491
| epoch  27 | lr 0.00035 | R 40.37409 | entropy 0.2101 | loss -6.81530
| epoch  27 | lr 0.00035 | R 40.73985 | entropy 0.2096 | loss -11.19645
| epoch  27 | lr 0.00035 | R 40.42380 | entropy 0.2093 | loss -10.64368
| epoch  27 | lr 0.00035 | R 39.35995 | entropy 0.2091 | loss -17.90420
| epoch  27 | lr 0.00035 | R 40.27295 | entropy 0.2089 | loss -8.84180
| epoch  27 | lr 0.00035 | R 39.06865 | entropy 0.2082 | loss -19.43945
| epoch  27 | lr 0.00035 | R 39.53825 | entropy 0.2073 | loss -19.80116
| epoch  27 | lr 0.00035 | R 40.16264 | entropy 0.2065 | loss -8.67460
| epoch  27 | lr 0.00035 | R 39.47174 | entropy 0.2061 | loss -18.12548
| epoch  27 | lr 0.00035 | R 39.74539 | entropy 0.2064 | loss -16.58518
| epoch  27 | lr 0.00035 | R 41.05696 | entropy 0.2059 | loss -3.83660
| epoch  27 | lr 0.00035 | R 39.62280 | entropy 0.2058 | loss -19.79271
| epoch  27 | lr 0.00035 | R 39.21763 | entropy 0.2055 | loss -16.68856
| epoch  27 | lr 0.00035 | R 38.74635 | entropy 0.2056 | loss -19.05518
| epoch  27 | lr 0.00035 | R 39.29932 | entropy 0.2060 | loss -15.92774
| epoch  27 | lr 0.00035 | R 39.91618 | entropy 0.2070 | loss -13.41154
| epoch  27 | lr 0.00035 | R 38.70631 | entropy 0.2071 | loss -17.04859
| epoch  27 | lr 0.00035 | R 39.28594 | entropy 0.2071 | loss -13.98285
| epoch  27 | lr 0.00035 | R 40.34098 | entropy 0.2071 | loss -4.85578
| epoch  27 | lr 0.00035 | R 40.14324 | entropy 0.2074 | loss -10.37134
| epoch  27 | lr 0.00035 | R 40.40978 | entropy 0.2075 | loss -6.43524
derive | max_R: 59.207340
========> finish evaluate on one epoch<======
eval | loss:    26.38 | ppl: 286252267927.88 | accuracy:     0.87
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch27_step22678.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch27_step58000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch23_step50000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch23_step19550.pth
| epoch  28 | lr 0.06 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  28 | lr 0.06 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  28 | lr 0.06 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  28 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  28 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  28 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  28 | lr 0.06 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  28 | lr 0.06 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  28 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  28 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  28 | lr 0.06 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  28 | lr 0.06 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  28 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  28 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  28 | lr 0.06 | raw loss 0.18 | loss 0.18 | ppl     1.20
====>train_shared<====== finish one epoch
| epoch  28 | lr 0.00035 | R 33.54801 | entropy 0.2070 | loss -21.82896
| epoch  28 | lr 0.00035 | R 37.62489 | entropy 0.2062 | loss 4.47101
| epoch  28 | lr 0.00035 | R 35.08639 | entropy 0.2065 | loss -8.61857
| epoch  28 | lr 0.00035 | R 36.89405 | entropy 0.2078 | loss -9.68351
| epoch  28 | lr 0.00035 | R 36.27127 | entropy 0.2088 | loss -12.35003
| epoch  28 | lr 0.00035 | R 36.31918 | entropy 0.2091 | loss -9.09425
| epoch  28 | lr 0.00035 | R 36.36711 | entropy 0.2093 | loss -7.92528
| epoch  28 | lr 0.00035 | R 35.13274 | entropy 0.2097 | loss -20.85954
| epoch  28 | lr 0.00035 | R 36.35311 | entropy 0.2105 | loss -12.32415
| epoch  28 | lr 0.00035 | R 35.92683 | entropy 0.2140 | loss -8.88126
| epoch  28 | lr 0.00035 | R 34.63004 | entropy 0.2172 | loss -25.74737
| epoch  28 | lr 0.00035 | R 36.97853 | entropy 0.2197 | loss -4.47387
| epoch  28 | lr 0.00035 | R 36.43585 | entropy 0.2224 | loss -8.89139
| epoch  28 | lr 0.00035 | R 36.26598 | entropy 0.2245 | loss -14.24352
| epoch  28 | lr 0.00035 | R 37.36599 | entropy 0.2251 | loss -8.71873
| epoch  28 | lr 0.00035 | R 38.07849 | entropy 0.2255 | loss -13.99323
| epoch  28 | lr 0.00035 | R 37.07359 | entropy 0.2256 | loss -10.43072
| epoch  28 | lr 0.00035 | R 38.68600 | entropy 0.2255 | loss -8.36568
| epoch  28 | lr 0.00035 | R 37.62977 | entropy 0.2254 | loss -15.54406
| epoch  28 | lr 0.00035 | R 37.00972 | entropy 0.2253 | loss -17.11374
| epoch  28 | lr 0.00035 | R 36.60321 | entropy 0.2252 | loss -19.97740
| epoch  28 | lr 0.00035 | R 37.29705 | entropy 0.2248 | loss -9.41396
| epoch  28 | lr 0.00035 | R 36.04765 | entropy 0.2246 | loss -23.96738
| epoch  28 | lr 0.00035 | R 37.43521 | entropy 0.2248 | loss -6.85967
| epoch  28 | lr 0.00035 | R 38.18181 | entropy 0.2248 | loss -7.67016
| epoch  28 | lr 0.00035 | R 37.04813 | entropy 0.2246 | loss -14.75874
| epoch  28 | lr 0.00035 | R 37.51204 | entropy 0.2243 | loss -12.80549
| epoch  28 | lr 0.00035 | R 36.47496 | entropy 0.2241 | loss -24.21019
| epoch  28 | lr 0.00035 | R 38.00011 | entropy 0.2240 | loss -11.35614
| epoch  28 | lr 0.00035 | R 37.25171 | entropy 0.2239 | loss -14.99545
| epoch  28 | lr 0.00035 | R 38.17249 | entropy 0.2238 | loss -18.33542
| epoch  28 | lr 0.00035 | R 36.06910 | entropy 0.2236 | loss -17.37330
| epoch  28 | lr 0.00035 | R 37.32754 | entropy 0.2234 | loss -8.25141
| epoch  28 | lr 0.00035 | R 38.68102 | entropy 0.2234 | loss -5.56740
| epoch  28 | lr 0.00035 | R 36.35714 | entropy 0.2235 | loss -28.28118
| epoch  28 | lr 0.00035 | R 35.84228 | entropy 0.2237 | loss -19.36262
| epoch  28 | lr 0.00035 | R 36.96978 | entropy 0.2240 | loss -15.32527
| epoch  28 | lr 0.00035 | R 36.38538 | entropy 0.2240 | loss -17.00335
| epoch  28 | lr 0.00035 | R 36.45891 | entropy 0.2239 | loss -17.88315
derive | max_R: 60.921844
========> finish evaluate on one epoch<======
eval | loss:    24.36 | ppl: 37811699684.32 | accuracy:     0.87
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch28_step23460.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch28_step60000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch24_step20332.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch24_step52000.pth
| epoch  29 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  29 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  29 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  29 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  29 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  29 | lr 0.05 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  29 | lr 0.05 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  29 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  29 | lr 0.05 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  29 | lr 0.05 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  29 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  29 | lr 0.05 | raw loss 0.18 | loss 0.18 | ppl     1.19
| epoch  29 | lr 0.05 | raw loss 0.19 | loss 0.19 | ppl     1.21
| epoch  29 | lr 0.05 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  29 | lr 0.05 | raw loss 0.20 | loss 0.20 | ppl     1.22
====>train_shared<====== finish one epoch
| epoch  29 | lr 0.00035 | R 33.05367 | entropy 0.2237 | loss -10.56321
| epoch  29 | lr 0.00035 | R 34.90698 | entropy 0.2240 | loss 3.18897
| epoch  29 | lr 0.00035 | R 34.00666 | entropy 0.2242 | loss -16.74181
| epoch  29 | lr 0.00035 | R 36.67057 | entropy 0.2237 | loss -19.04689
| epoch  29 | lr 0.00035 | R 36.38044 | entropy 0.2234 | loss -22.31358
| epoch  29 | lr 0.00035 | R 36.33757 | entropy 0.2230 | loss -10.75093
| epoch  29 | lr 0.00035 | R 37.23188 | entropy 0.2231 | loss -12.03310
| epoch  29 | lr 0.00035 | R 37.08132 | entropy 0.2232 | loss -15.04117
| epoch  29 | lr 0.00035 | R 37.15446 | entropy 0.2232 | loss -10.19861
| epoch  29 | lr 0.00035 | R 37.43603 | entropy 0.2233 | loss -9.68157
| epoch  29 | lr 0.00035 | R 38.42511 | entropy 0.2234 | loss -3.07432
| epoch  29 | lr 0.00035 | R 36.86794 | entropy 0.2234 | loss -14.68450
| epoch  29 | lr 0.00035 | R 37.62719 | entropy 0.2234 | loss -11.83226
| epoch  29 | lr 0.00035 | R 36.86752 | entropy 0.2232 | loss -14.38860
| epoch  29 | lr 0.00035 | R 37.03490 | entropy 0.2232 | loss -11.86300
| epoch  29 | lr 0.00035 | R 36.69173 | entropy 0.2231 | loss -12.69055
| epoch  29 | lr 0.00035 | R 37.46139 | entropy 0.2230 | loss -8.89498
| epoch  29 | lr 0.00035 | R 37.83356 | entropy 0.2228 | loss -11.77791
| epoch  29 | lr 0.00035 | R 37.03197 | entropy 0.2229 | loss -12.16400
| epoch  29 | lr 0.00035 | R 37.44967 | entropy 0.2229 | loss -11.75437
| epoch  29 | lr 0.00035 | R 37.90006 | entropy 0.2227 | loss -9.16063
| epoch  29 | lr 0.00035 | R 36.83218 | entropy 0.2226 | loss -14.12964
| epoch  29 | lr 0.00035 | R 36.53073 | entropy 0.2226 | loss -20.71939
| epoch  29 | lr 0.00035 | R 36.96815 | entropy 0.2223 | loss -17.59174
| epoch  29 | lr 0.00035 | R 37.09035 | entropy 0.2221 | loss -8.19786
| epoch  29 | lr 0.00035 | R 37.19440 | entropy 0.2222 | loss -14.72107
| epoch  29 | lr 0.00035 | R 36.30779 | entropy 0.2223 | loss -20.22171
| epoch  29 | lr 0.00035 | R 36.31579 | entropy 0.2221 | loss -18.31396
| epoch  29 | lr 0.00035 | R 36.75901 | entropy 0.2221 | loss -10.19390
| epoch  29 | lr 0.00035 | R 37.10596 | entropy 0.2222 | loss -8.77509
| epoch  29 | lr 0.00035 | R 37.25850 | entropy 0.2222 | loss -10.43925
| epoch  29 | lr 0.00035 | R 37.72306 | entropy 0.2222 | loss -10.32489
| epoch  29 | lr 0.00035 | R 36.93160 | entropy 0.2219 | loss -18.19981
| epoch  29 | lr 0.00035 | R 37.40702 | entropy 0.2219 | loss -11.30063
| epoch  29 | lr 0.00035 | R 36.22807 | entropy 0.2219 | loss -19.79036
| epoch  29 | lr 0.00035 | R 36.57899 | entropy 0.2219 | loss -17.25245
| epoch  29 | lr 0.00035 | R 37.65344 | entropy 0.2218 | loss -11.54231
| epoch  29 | lr 0.00035 | R 37.89781 | entropy 0.2218 | loss -10.10657
| epoch  29 | lr 0.00035 | R 36.74916 | entropy 0.2219 | loss -15.82030
derive | max_R: 58.974430
========> finish evaluate on one epoch<======
eval | loss:    28.39 | ppl: 2145576222020.66 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch29_step24242.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch29_step62000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch25_step54000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch25_step21114.pth
| epoch  30 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  30 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  30 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  30 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  30 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  30 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  30 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  30 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  30 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  30 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  30 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  30 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  30 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  30 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  30 | lr 0.05 | raw loss 0.20 | loss 0.20 | ppl     1.22
====>train_shared<====== finish one epoch
| epoch  30 | lr 0.00035 | R 31.17128 | entropy 0.2220 | loss -19.25404
| epoch  30 | lr 0.00035 | R 33.24657 | entropy 0.2221 | loss -9.61073
| epoch  30 | lr 0.00035 | R 32.19693 | entropy 0.2224 | loss -13.75214
| epoch  30 | lr 0.00035 | R 31.82705 | entropy 0.2231 | loss -3.22395
| epoch  30 | lr 0.00035 | R 29.88265 | entropy 0.2231 | loss -11.80231
| epoch  30 | lr 0.00035 | R 31.50772 | entropy 0.2231 | loss -4.12087
| epoch  30 | lr 0.00035 | R 30.70576 | entropy 0.2231 | loss -9.19014
| epoch  30 | lr 0.00035 | R 30.59240 | entropy 0.2231 | loss -13.51562
| epoch  30 | lr 0.00035 | R 30.60442 | entropy 0.2228 | loss -9.19752
| epoch  30 | lr 0.00035 | R 30.86092 | entropy 0.2226 | loss -8.78202
| epoch  30 | lr 0.00035 | R 31.02583 | entropy 0.2225 | loss -6.72543
| epoch  30 | lr 0.00035 | R 30.34080 | entropy 0.2224 | loss -16.89341
| epoch  30 | lr 0.00035 | R 30.37201 | entropy 0.2223 | loss -8.04332
| epoch  30 | lr 0.00035 | R 31.33348 | entropy 0.2225 | loss -3.80338
| epoch  30 | lr 0.00035 | R 31.04945 | entropy 0.2227 | loss -6.94765
| epoch  30 | lr 0.00035 | R 30.75435 | entropy 0.2228 | loss -7.68868
| epoch  30 | lr 0.00035 | R 31.44828 | entropy 0.2230 | loss -3.68840
| epoch  30 | lr 0.00035 | R 30.60612 | entropy 0.2234 | loss -14.45794
| epoch  30 | lr 0.00035 | R 30.67403 | entropy 0.2237 | loss -9.64138
| epoch  30 | lr 0.00035 | R 31.11598 | entropy 0.2241 | loss -6.29980
| epoch  30 | lr 0.00035 | R 30.78033 | entropy 0.2245 | loss -8.93175
| epoch  30 | lr 0.00035 | R 31.75846 | entropy 0.2251 | loss -1.96725
| epoch  30 | lr 0.00035 | R 30.42407 | entropy 0.2265 | loss -14.42104
| epoch  30 | lr 0.00035 | R 30.66814 | entropy 0.2273 | loss -9.62833
| epoch  30 | lr 0.00035 | R 31.40610 | entropy 0.2294 | loss -3.69497
| epoch  30 | lr 0.00035 | R 30.79598 | entropy 0.2325 | loss -6.75790
| epoch  30 | lr 0.00035 | R 30.96304 | entropy 0.2370 | loss -8.97659
| epoch  30 | lr 0.00035 | R 31.38516 | entropy 0.2382 | loss -6.43257
| epoch  30 | lr 0.00035 | R 31.09615 | entropy 0.2420 | loss -12.29858
| epoch  30 | lr 0.00035 | R 31.69144 | entropy 0.2440 | loss -2.22681
| epoch  30 | lr 0.00035 | R 30.70399 | entropy 0.2479 | loss -10.30520
| epoch  30 | lr 0.00035 | R 30.71598 | entropy 0.2503 | loss -12.46309
| epoch  30 | lr 0.00035 | R 30.53411 | entropy 0.2523 | loss -13.42464
| epoch  30 | lr 0.00035 | R 31.42985 | entropy 0.2538 | loss -5.06814
| epoch  30 | lr 0.00035 | R 30.73893 | entropy 0.2546 | loss -15.99176
| epoch  30 | lr 0.00035 | R 31.80256 | entropy 0.2549 | loss -5.88499
| epoch  30 | lr 0.00035 | R 32.33968 | entropy 0.2534 | loss -6.42059
| epoch  30 | lr 0.00035 | R 31.90115 | entropy 0.2518 | loss -7.56029
| epoch  30 | lr 0.00035 | R 31.88107 | entropy 0.2512 | loss -12.41273
derive | max_R: 58.793400
========> finish evaluate on one epoch<======
eval | loss:    28.55 | ppl: 2502617715213.78 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch30_step25024.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch30_step64000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch26_step56000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch26_step21896.pth
| epoch  31 | lr 0.05 | raw loss 0.20 | loss 0.20 | ppl     1.22
| epoch  31 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  31 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  31 | lr 0.05 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  31 | lr 0.05 | raw loss 0.19 | loss 0.19 | ppl     1.22
| epoch  31 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  31 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  31 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  31 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.17
| epoch  31 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.16
| epoch  31 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  31 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.17
| epoch  31 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  31 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  31 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
====>train_shared<====== finish one epoch
| epoch  31 | lr 0.00035 | R 32.60763 | entropy 0.2474 | loss -19.48083
| epoch  31 | lr 0.00035 | R 36.11411 | entropy 0.2531 | loss 10.82442
| epoch  31 | lr 0.00035 | R 33.42200 | entropy 0.2566 | loss -31.80677
| epoch  31 | lr 0.00035 | R 34.64936 | entropy 0.2577 | loss -3.46364
| epoch  31 | lr 0.00035 | R 34.92865 | entropy 0.2574 | loss -8.05171
| epoch  31 | lr 0.00035 | R 33.88016 | entropy 0.2561 | loss -25.79895
| epoch  31 | lr 0.00035 | R 35.07129 | entropy 0.2550 | loss -3.90245
| epoch  31 | lr 0.00035 | R 35.59423 | entropy 0.2531 | loss -10.01068
| epoch  31 | lr 0.00035 | R 34.48198 | entropy 0.2515 | loss -16.29852
| epoch  31 | lr 0.00035 | R 35.18842 | entropy 0.2509 | loss -9.55327
| epoch  31 | lr 0.00035 | R 34.10168 | entropy 0.2519 | loss -13.77546
| epoch  31 | lr 0.00035 | R 34.59845 | entropy 0.2509 | loss -10.53947
| epoch  31 | lr 0.00035 | R 34.38111 | entropy 0.2497 | loss -8.49017
| epoch  31 | lr 0.00035 | R 35.45887 | entropy 0.2508 | loss -3.51027
| epoch  31 | lr 0.00035 | R 35.05746 | entropy 0.2510 | loss -10.95835
| epoch  31 | lr 0.00035 | R 35.24988 | entropy 0.2501 | loss -0.85263
| epoch  31 | lr 0.00035 | R 36.17493 | entropy 0.2491 | loss -7.57423
| epoch  31 | lr 0.00035 | R 34.71629 | entropy 0.2483 | loss -6.37284
| epoch  31 | lr 0.00035 | R 34.49659 | entropy 0.2491 | loss -15.51079
| epoch  31 | lr 0.00035 | R 34.21223 | entropy 0.2480 | loss -11.15248
| epoch  31 | lr 0.00035 | R 35.98772 | entropy 0.2475 | loss -2.89475
| epoch  31 | lr 0.00035 | R 35.56543 | entropy 0.2466 | loss -5.53156
| epoch  31 | lr 0.00035 | R 35.75268 | entropy 0.2460 | loss -6.34934
| epoch  31 | lr 0.00035 | R 35.14657 | entropy 0.2459 | loss -10.33030
| epoch  31 | lr 0.00035 | R 34.75470 | entropy 0.2461 | loss -9.29058
| epoch  31 | lr 0.00035 | R 34.81234 | entropy 0.2450 | loss -12.70336
| epoch  31 | lr 0.00035 | R 34.82447 | entropy 0.2446 | loss -6.17031
| epoch  31 | lr 0.00035 | R 34.64857 | entropy 0.2449 | loss -13.96866
| epoch  31 | lr 0.00035 | R 35.73760 | entropy 0.2450 | loss -4.73851
| epoch  31 | lr 0.00035 | R 34.54107 | entropy 0.2443 | loss -12.06715
| epoch  31 | lr 0.00035 | R 36.80018 | entropy 0.2430 | loss -2.30074
| epoch  31 | lr 0.00035 | R 34.08555 | entropy 0.2436 | loss -26.18691
| epoch  31 | lr 0.00035 | R 34.29256 | entropy 0.2439 | loss -18.79301
| epoch  31 | lr 0.00035 | R 34.69938 | entropy 0.2425 | loss -13.55393
| epoch  31 | lr 0.00035 | R 35.32053 | entropy 0.2428 | loss -4.92288
| epoch  31 | lr 0.00035 | R 34.46126 | entropy 0.2436 | loss -13.76520
| epoch  31 | lr 0.00035 | R 34.29953 | entropy 0.2442 | loss -9.49705
| epoch  31 | lr 0.00035 | R 34.66737 | entropy 0.2443 | loss -10.89915
| epoch  31 | lr 0.00035 | R 35.03399 | entropy 0.2430 | loss -11.83577
derive | max_R: 71.095245
========> finish evaluate on one epoch<======
eval | loss:    26.94 | ppl: 500630837472.81 | accuracy:     0.87
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch31_step25806.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch31_step66000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch27_step22678.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch27_step58000.pth
| epoch  32 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.17
| epoch  32 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  32 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  32 | lr 0.05 | raw loss 0.18 | loss 0.18 | ppl     1.19
| epoch  32 | lr 0.05 | raw loss 0.18 | loss 0.18 | ppl     1.20
| epoch  32 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  32 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  32 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.17
| epoch  32 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  32 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  32 | lr 0.05 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  32 | lr 0.05 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  32 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  32 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.19
| epoch  32 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
====>train_shared<====== finish one epoch
| epoch  32 | lr 0.00035 | R 32.17039 | entropy 0.2428 | loss -3.10974
| epoch  32 | lr 0.00035 | R 34.62960 | entropy 0.2434 | loss -4.83274
| epoch  32 | lr 0.00035 | R 33.88238 | entropy 0.2429 | loss 1.49659
| epoch  32 | lr 0.00035 | R 32.24412 | entropy 0.2409 | loss -21.01803
| epoch  32 | lr 0.00035 | R 33.67291 | entropy 0.2402 | loss -9.92500
| epoch  32 | lr 0.00035 | R 33.85094 | entropy 0.2397 | loss -17.81242
| epoch  32 | lr 0.00035 | R 34.67807 | entropy 0.2391 | loss -5.44605
| epoch  32 | lr 0.00035 | R 34.54129 | entropy 0.2380 | loss -7.66084
| epoch  32 | lr 0.00035 | R 33.27174 | entropy 0.2375 | loss -10.18145
| epoch  32 | lr 0.00035 | R 32.99252 | entropy 0.2373 | loss -12.55202
| epoch  32 | lr 0.00035 | R 33.90294 | entropy 0.2362 | loss -3.44695
| epoch  32 | lr 0.00035 | R 34.44204 | entropy 0.2355 | loss -6.40120
| epoch  32 | lr 0.00035 | R 33.23751 | entropy 0.2345 | loss -17.16706
| epoch  32 | lr 0.00035 | R 34.32620 | entropy 0.2342 | loss -6.23073
| epoch  32 | lr 0.00035 | R 34.16336 | entropy 0.2331 | loss -13.40001
| epoch  32 | lr 0.00035 | R 33.66690 | entropy 0.2325 | loss -13.52570
| epoch  32 | lr 0.00035 | R 34.15289 | entropy 0.2325 | loss -6.73134
| epoch  32 | lr 0.00035 | R 33.37589 | entropy 0.2324 | loss -14.80123
| epoch  32 | lr 0.00035 | R 33.11534 | entropy 0.2321 | loss -17.05093
| epoch  32 | lr 0.00035 | R 33.84926 | entropy 0.2320 | loss -9.63324
| epoch  32 | lr 0.00035 | R 33.75055 | entropy 0.2326 | loss -8.65820
| epoch  32 | lr 0.00035 | R 34.16203 | entropy 0.2326 | loss -8.63030
| epoch  32 | lr 0.00035 | R 33.64811 | entropy 0.2325 | loss -13.13033
| epoch  32 | lr 0.00035 | R 34.77693 | entropy 0.2320 | loss -6.58513
| epoch  32 | lr 0.00035 | R 34.68005 | entropy 0.2310 | loss -7.29546
| epoch  32 | lr 0.00035 | R 33.06691 | entropy 0.2304 | loss -12.87021
| epoch  32 | lr 0.00035 | R 34.85084 | entropy 0.2305 | loss -5.30277
| epoch  32 | lr 0.00035 | R 34.81313 | entropy 0.2304 | loss -7.35762
| epoch  32 | lr 0.00035 | R 34.56720 | entropy 0.2298 | loss -7.20690
| epoch  32 | lr 0.00035 | R 34.69115 | entropy 0.2297 | loss -7.33388
| epoch  32 | lr 0.00035 | R 34.42583 | entropy 0.2295 | loss -10.93436
| epoch  32 | lr 0.00035 | R 34.46866 | entropy 0.2286 | loss -11.27912
| epoch  32 | lr 0.00035 | R 33.80109 | entropy 0.2281 | loss -10.07062
| epoch  32 | lr 0.00035 | R 34.28141 | entropy 0.2280 | loss -13.81418
| epoch  32 | lr 0.00035 | R 33.84069 | entropy 0.2277 | loss -11.28852
| epoch  32 | lr 0.00035 | R 33.63470 | entropy 0.2279 | loss -8.74794
| epoch  32 | lr 0.00035 | R 33.92192 | entropy 0.2282 | loss -14.56654
| epoch  32 | lr 0.00035 | R 34.27559 | entropy 0.2283 | loss -6.74280
| epoch  32 | lr 0.00035 | R 33.41380 | entropy 0.2286 | loss -15.16336
derive | max_R: 71.098160
========> finish evaluate on one epoch<======
eval | loss:    26.77 | ppl: 422623512008.30 | accuracy:     0.87
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch32_step26588.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch32_step68000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch28_step23460.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch28_step60000.pth
| epoch  33 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  33 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  33 | lr 0.05 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  33 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  33 | lr 0.05 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  33 | lr 0.05 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  33 | lr 0.05 | raw loss 0.17 | loss 0.17 | ppl     1.18
| epoch  33 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  33 | lr 0.05 | raw loss 0.16 | loss 0.16 | ppl     1.17
| epoch  33 | lr 0.05 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  33 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  33 | lr 0.05 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  33 | lr 0.05 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  33 | lr 0.05 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  33 | lr 0.05 | raw loss 0.13 | loss 0.13 | ppl     1.13
====>train_shared<====== finish one epoch
| epoch  33 | lr 0.00035 | R 31.70953 | entropy 0.2280 | loss 10.04117
| epoch  33 | lr 0.00035 | R 34.88700 | entropy 0.2282 | loss 0.32611
| epoch  33 | lr 0.00035 | R 32.45660 | entropy 0.2292 | loss -16.08289
| epoch  33 | lr 0.00035 | R 26.63632 | entropy 0.2304 | loss -23.47118
| epoch  33 | lr 0.00035 | R 24.88977 | entropy 0.2311 | loss -10.52311
| epoch  33 | lr 0.00035 | R 24.38250 | entropy 0.2310 | loss -11.57799
| epoch  33 | lr 0.00035 | R 25.10359 | entropy 0.2304 | loss -5.94582
| epoch  33 | lr 0.00035 | R 24.94334 | entropy 0.2300 | loss -17.24179
| epoch  33 | lr 0.00035 | R 24.23633 | entropy 0.2288 | loss -12.93065
| epoch  33 | lr 0.00035 | R 25.20711 | entropy 0.2286 | loss -4.69257
| epoch  33 | lr 0.00035 | R 24.85270 | entropy 0.2288 | loss -10.82149
| epoch  33 | lr 0.00035 | R 25.60898 | entropy 0.2275 | loss -4.79788
| epoch  33 | lr 0.00035 | R 25.56277 | entropy 0.2252 | loss -10.48659
| epoch  33 | lr 0.00035 | R 24.99464 | entropy 0.2226 | loss -10.00235
| epoch  33 | lr 0.00035 | R 25.36343 | entropy 0.2205 | loss -9.31689
| epoch  33 | lr 0.00035 | R 25.72903 | entropy 0.2194 | loss -8.70431
| epoch  33 | lr 0.00035 | R 25.60067 | entropy 0.2171 | loss -11.49234
| epoch  33 | lr 0.00035 | R 25.93375 | entropy 0.2158 | loss -6.74405
| epoch  33 | lr 0.00035 | R 24.73130 | entropy 0.2153 | loss -19.64310
| epoch  33 | lr 0.00035 | R 25.41016 | entropy 0.2152 | loss -10.41257
| epoch  33 | lr 0.00035 | R 26.50669 | entropy 0.2149 | loss -4.41857
| epoch  33 | lr 0.00035 | R 26.07264 | entropy 0.2144 | loss -9.98651
| epoch  33 | lr 0.00035 | R 25.87561 | entropy 0.2137 | loss -9.57735
| epoch  33 | lr 0.00035 | R 25.15688 | entropy 0.2111 | loss -7.92875
| epoch  33 | lr 0.00035 | R 25.97803 | entropy 0.2111 | loss -8.46376
| epoch  33 | lr 0.00035 | R 25.27133 | entropy 0.2109 | loss -13.45613
| epoch  33 | lr 0.00035 | R 25.81965 | entropy 0.2098 | loss -6.39475
| epoch  33 | lr 0.00035 | R 26.35861 | entropy 0.2096 | loss -10.03391
| epoch  33 | lr 0.00035 | R 26.20988 | entropy 0.2096 | loss -7.22632
| epoch  33 | lr 0.00035 | R 25.95552 | entropy 0.2101 | loss -5.48263
| epoch  33 | lr 0.00035 | R 26.32489 | entropy 0.2102 | loss -5.24657
| epoch  33 | lr 0.00035 | R 26.19542 | entropy 0.2096 | loss -8.64112
| epoch  33 | lr 0.00035 | R 24.34810 | entropy 0.2090 | loss -23.44779
| epoch  33 | lr 0.00035 | R 26.61882 | entropy 0.2098 | loss -3.13375
| epoch  33 | lr 0.00035 | R 25.65817 | entropy 0.2104 | loss -13.59270
| epoch  33 | lr 0.00035 | R 25.70028 | entropy 0.2096 | loss -9.12696
| epoch  33 | lr 0.00035 | R 25.49766 | entropy 0.2091 | loss -15.14733
| epoch  33 | lr 0.00035 | R 26.03952 | entropy 0.2087 | loss -3.67491
| epoch  33 | lr 0.00035 | R 26.41749 | entropy 0.2087 | loss -6.39110
derive | max_R: 58.777641
========> finish evaluate on one epoch<======
eval | loss:    30.28 | ppl: 14101866748523.21 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch33_step27370.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch33_step70000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch29_step62000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch29_step24242.pth
| epoch  34 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.13
| epoch  34 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.13
| epoch  34 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  34 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  34 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  34 | lr 0.04 | raw loss 0.15 | loss 0.15 | ppl     1.16
| epoch  34 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  34 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  34 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  34 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  34 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  34 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  34 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  34 | lr 0.04 | raw loss 0.16 | loss 0.16 | ppl     1.18
| epoch  34 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
====>train_shared<====== finish one epoch
| epoch  34 | lr 0.00035 | R 35.00418 | entropy 0.2083 | loss -30.33239
| epoch  34 | lr 0.00035 | R 38.04263 | entropy 0.2080 | loss -6.13942
| epoch  34 | lr 0.00035 | R 36.04237 | entropy 0.2063 | loss -12.94615
| epoch  34 | lr 0.00035 | R 41.47247 | entropy 0.2044 | loss -1.02278
| epoch  34 | lr 0.00035 | R 40.59633 | entropy 0.2039 | loss -15.31958
| epoch  34 | lr 0.00035 | R 40.74368 | entropy 0.2037 | loss -9.46745
| epoch  34 | lr 0.00035 | R 40.98851 | entropy 0.2037 | loss -12.53780
| epoch  34 | lr 0.00035 | R 40.98473 | entropy 0.2034 | loss -7.19597
| epoch  34 | lr 0.00035 | R 41.34105 | entropy 0.2034 | loss -6.73480
| epoch  34 | lr 0.00035 | R 41.27663 | entropy 0.2034 | loss -11.31486
| epoch  34 | lr 0.00035 | R 40.25532 | entropy 0.2033 | loss -17.60236
| epoch  34 | lr 0.00035 | R 40.55674 | entropy 0.2033 | loss -10.50584
| epoch  34 | lr 0.00035 | R 40.41538 | entropy 0.2030 | loss -17.73215
| epoch  34 | lr 0.00035 | R 40.72376 | entropy 0.2024 | loss -10.69768
| epoch  34 | lr 0.00035 | R 40.73786 | entropy 0.2023 | loss -7.21765
| epoch  34 | lr 0.00035 | R 41.49098 | entropy 0.2022 | loss -12.56064
| epoch  34 | lr 0.00035 | R 41.47728 | entropy 0.2018 | loss -7.19094
| epoch  34 | lr 0.00035 | R 40.50718 | entropy 0.2018 | loss -16.03397
| epoch  34 | lr 0.00035 | R 40.80136 | entropy 0.2017 | loss -6.78740
| epoch  34 | lr 0.00035 | R 41.92389 | entropy 0.2013 | loss -1.66663
| epoch  34 | lr 0.00035 | R 40.58301 | entropy 0.2012 | loss -11.79975
| epoch  34 | lr 0.00035 | R 40.95317 | entropy 0.2011 | loss -10.35959
| epoch  34 | lr 0.00035 | R 40.65341 | entropy 0.2011 | loss -12.08970
| epoch  34 | lr 0.00035 | R 40.35127 | entropy 0.2012 | loss -11.32090
| epoch  34 | lr 0.00035 | R 40.77092 | entropy 0.2013 | loss -8.64771
| epoch  34 | lr 0.00035 | R 40.04855 | entropy 0.2013 | loss -12.02692
| epoch  34 | lr 0.00035 | R 41.31356 | entropy 0.2012 | loss -6.10556
| epoch  34 | lr 0.00035 | R 39.76673 | entropy 0.2009 | loss -17.11128
| epoch  34 | lr 0.00035 | R 41.83292 | entropy 0.2008 | loss -1.59446
| epoch  34 | lr 0.00035 | R 40.87723 | entropy 0.2006 | loss -11.30997
| epoch  34 | lr 0.00035 | R 40.15561 | entropy 0.2005 | loss -21.10783
| epoch  34 | lr 0.00035 | R 41.37913 | entropy 0.2004 | loss -5.35504
| epoch  34 | lr 0.00035 | R 40.86339 | entropy 0.2003 | loss -9.92704
| epoch  34 | lr 0.00035 | R 40.89651 | entropy 0.2003 | loss -9.24386
| epoch  34 | lr 0.00035 | R 40.80750 | entropy 0.2003 | loss -11.34544
| epoch  34 | lr 0.00035 | R 41.23204 | entropy 0.2004 | loss -9.07944
| epoch  34 | lr 0.00035 | R 40.68544 | entropy 0.2003 | loss -16.97297
| epoch  34 | lr 0.00035 | R 41.11317 | entropy 0.2001 | loss -10.32741
| epoch  34 | lr 0.00035 | R 41.72861 | entropy 0.2001 | loss -2.22975
derive | max_R: 65.512291
========> finish evaluate on one epoch<======
eval | loss:    23.62 | ppl: 18190589391.69 | accuracy:     0.88
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch34_step28152.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch34_step72000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch30_step25024.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch30_step64000.pth
| epoch  35 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  35 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  35 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  35 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  35 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  35 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  35 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  35 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  35 | lr 0.04 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  35 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  35 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.13
| epoch  35 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.13
| epoch  35 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  35 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  35 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
====>train_shared<====== finish one epoch
| epoch  35 | lr 0.00035 | R 35.94490 | entropy 0.2000 | loss -11.65165
| epoch  35 | lr 0.00035 | R 38.61072 | entropy 0.2000 | loss -6.81618
| epoch  35 | lr 0.00035 | R 35.79831 | entropy 0.2001 | loss -24.35501
| epoch  35 | lr 0.00035 | R 35.01543 | entropy 0.2001 | loss -14.42763
| epoch  35 | lr 0.00035 | R 34.90602 | entropy 0.2001 | loss -7.28506
| epoch  35 | lr 0.00035 | R 35.77404 | entropy 0.2001 | loss -2.64873
| epoch  35 | lr 0.00035 | R 34.88921 | entropy 0.2001 | loss -8.49914
| epoch  35 | lr 0.00035 | R 34.59264 | entropy 0.2001 | loss -10.09912
| epoch  35 | lr 0.00035 | R 36.10298 | entropy 0.2002 | loss -3.66170
| epoch  35 | lr 0.00035 | R 35.53918 | entropy 0.2002 | loss -7.74036
| epoch  35 | lr 0.00035 | R 35.70367 | entropy 0.2003 | loss -7.63419
| epoch  35 | lr 0.00035 | R 34.96923 | entropy 0.2004 | loss -15.51730
| epoch  35 | lr 0.00035 | R 35.64417 | entropy 0.2004 | loss -4.81447
| epoch  35 | lr 0.00035 | R 34.57479 | entropy 0.2004 | loss -12.62666
| epoch  35 | lr 0.00035 | R 34.97257 | entropy 0.2006 | loss -12.41725
| epoch  35 | lr 0.00035 | R 36.00162 | entropy 0.2006 | loss -6.17678
| epoch  35 | lr 0.00035 | R 35.48473 | entropy 0.2005 | loss -6.51656
| epoch  35 | lr 0.00035 | R 35.11695 | entropy 0.2005 | loss -10.20527
| epoch  35 | lr 0.00035 | R 35.28637 | entropy 0.2006 | loss -6.82620
| epoch  35 | lr 0.00035 | R 34.67351 | entropy 0.2010 | loss -13.54579
| epoch  35 | lr 0.00035 | R 35.69738 | entropy 0.2007 | loss -2.43723
| epoch  35 | lr 0.00035 | R 35.17072 | entropy 0.2007 | loss -6.63147
| epoch  35 | lr 0.00035 | R 35.27075 | entropy 0.2007 | loss -9.11126
| epoch  35 | lr 0.00035 | R 35.20887 | entropy 0.2008 | loss -12.21666
| epoch  35 | lr 0.00035 | R 35.42575 | entropy 0.2007 | loss -7.69536
| epoch  35 | lr 0.00035 | R 36.00001 | entropy 0.2008 | loss -5.49815
| epoch  35 | lr 0.00035 | R 34.71241 | entropy 0.2005 | loss -18.76975
| epoch  35 | lr 0.00035 | R 35.02732 | entropy 0.2003 | loss -13.61138
| epoch  35 | lr 0.00035 | R 35.95142 | entropy 0.2005 | loss -3.71102
| epoch  35 | lr 0.00035 | R 35.06183 | entropy 0.2006 | loss -9.08852
| epoch  35 | lr 0.00035 | R 35.80494 | entropy 0.2007 | loss -5.57690
| epoch  35 | lr 0.00035 | R 36.04759 | entropy 0.2009 | loss -5.76306
| epoch  35 | lr 0.00035 | R 34.85425 | entropy 0.2010 | loss -15.78123
| epoch  35 | lr 0.00035 | R 35.13693 | entropy 0.2016 | loss -8.98985
| epoch  35 | lr 0.00035 | R 35.12489 | entropy 0.2020 | loss -6.89740
| epoch  35 | lr 0.00035 | R 34.20886 | entropy 0.2027 | loss -19.01367
| epoch  35 | lr 0.00035 | R 34.49716 | entropy 0.2043 | loss -13.64737
| epoch  35 | lr 0.00035 | R 36.06290 | entropy 0.2047 | loss -6.70745
| epoch  35 | lr 0.00035 | R 35.84488 | entropy 0.2040 | loss -6.32642
derive | max_R: 68.328651
========> finish evaluate on one epoch<======
eval | loss:    23.06 | ppl: 10325410053.98 | accuracy:     0.89
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch35_step28934.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch35_step74000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch31_step66000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch31_step25806.pth
| epoch  36 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  36 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  36 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  36 | lr 0.04 | raw loss 0.09 | loss 0.09 | ppl     1.09
| epoch  36 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  36 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  36 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  36 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  36 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  36 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.13
| epoch  36 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  36 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  36 | lr 0.04 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  36 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  36 | lr 0.04 | raw loss 0.09 | loss 0.09 | ppl     1.10
====>train_shared<====== finish one epoch
| epoch  36 | lr 0.00035 | R 33.82288 | entropy 0.2047 | loss -14.46257
| epoch  36 | lr 0.00035 | R 38.55126 | entropy 0.2049 | loss -3.24274
| epoch  36 | lr 0.00035 | R 35.36388 | entropy 0.2044 | loss -21.48959
| epoch  36 | lr 0.00035 | R 35.14407 | entropy 0.2046 | loss -9.11463
| epoch  36 | lr 0.00035 | R 34.87185 | entropy 0.2050 | loss -10.54982
| epoch  36 | lr 0.00035 | R 34.43876 | entropy 0.2051 | loss -9.06174
| epoch  36 | lr 0.00035 | R 34.10482 | entropy 0.2048 | loss -14.40323
| epoch  36 | lr 0.00035 | R 34.35487 | entropy 0.2048 | loss -9.48175
| epoch  36 | lr 0.00035 | R 34.76392 | entropy 0.2049 | loss -5.21523
| epoch  36 | lr 0.00035 | R 34.91634 | entropy 0.2055 | loss -2.64512
| epoch  36 | lr 0.00035 | R 35.16265 | entropy 0.2060 | loss -7.08323
| epoch  36 | lr 0.00035 | R 34.99188 | entropy 0.2065 | loss -5.03647
| epoch  36 | lr 0.00035 | R 34.17950 | entropy 0.2074 | loss -13.14415
| epoch  36 | lr 0.00035 | R 34.62227 | entropy 0.2115 | loss -4.59157
| epoch  36 | lr 0.00035 | R 34.25079 | entropy 0.2144 | loss -14.53761
| epoch  36 | lr 0.00035 | R 35.35777 | entropy 0.2158 | loss -3.55279
| epoch  36 | lr 0.00035 | R 34.48084 | entropy 0.2202 | loss -10.39325
| epoch  36 | lr 0.00035 | R 35.43642 | entropy 0.2240 | loss -4.91959
| epoch  36 | lr 0.00035 | R 34.08111 | entropy 0.2293 | loss -12.61589
| epoch  36 | lr 0.00035 | R 35.06786 | entropy 0.2327 | loss -6.25284
| epoch  36 | lr 0.00035 | R 35.47866 | entropy 0.2370 | loss -4.05051
| epoch  36 | lr 0.00035 | R 36.10985 | entropy 0.2397 | loss -6.08690
| epoch  36 | lr 0.00035 | R 34.08266 | entropy 0.2396 | loss -16.23266
| epoch  36 | lr 0.00035 | R 34.91864 | entropy 0.2409 | loss -10.31494
| epoch  36 | lr 0.00035 | R 35.77482 | entropy 0.2429 | loss -10.43390
| epoch  36 | lr 0.00035 | R 35.79071 | entropy 0.2459 | loss -27.60074
| epoch  36 | lr 0.00035 | R 36.03563 | entropy 0.2489 | loss -13.83201
| epoch  36 | lr 0.00035 | R 35.83495 | entropy 0.2515 | loss -10.74267
| epoch  36 | lr 0.00035 | R 36.34588 | entropy 0.2543 | loss -19.29246
| epoch  36 | lr 0.00035 | R 34.17907 | entropy 0.2548 | loss -34.26392
| epoch  36 | lr 0.00035 | R 37.02491 | entropy 0.2556 | loss 0.80752
| epoch  36 | lr 0.00035 | R 35.56084 | entropy 0.2581 | loss -22.06755
| epoch  36 | lr 0.00035 | R 35.84075 | entropy 0.2588 | loss -10.43020
| epoch  36 | lr 0.00035 | R 36.83107 | entropy 0.2579 | loss -4.95206
| epoch  36 | lr 0.00035 | R 38.40137 | entropy 0.2590 | loss -5.40346
| epoch  36 | lr 0.00035 | R 37.81355 | entropy 0.2614 | loss -12.66636
| epoch  36 | lr 0.00035 | R 38.30986 | entropy 0.2622 | loss -2.38254
| epoch  36 | lr 0.00035 | R 37.46890 | entropy 0.2622 | loss -13.70248
| epoch  36 | lr 0.00035 | R 36.53807 | entropy 0.2618 | loss -18.87776
derive | max_R: 53.728405
========> finish evaluate on one epoch<======
eval | loss:    30.75 | ppl: 22682772452922.80 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch36_step29716.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch36_step76000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch32_step68000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch32_step26588.pth
| epoch  37 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  37 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  37 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  37 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  37 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  37 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.10
| epoch  37 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  37 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  37 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  37 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  37 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  37 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  37 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  37 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.13
| epoch  37 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
====>train_shared<====== finish one epoch
| epoch  37 | lr 0.00035 | R 29.87618 | entropy 0.2633 | loss 9.29300
| epoch  37 | lr 0.00035 | R 31.02635 | entropy 0.2629 | loss 7.32658
| epoch  37 | lr 0.00035 | R 29.70176 | entropy 0.2635 | loss -9.55765
| epoch  37 | lr 0.00035 | R 25.11725 | entropy 0.2657 | loss -13.62356
| epoch  37 | lr 0.00035 | R 23.47628 | entropy 0.2662 | loss -6.60877
| epoch  37 | lr 0.00035 | R 23.74600 | entropy 0.2673 | loss -2.94722
| epoch  37 | lr 0.00035 | R 23.86864 | entropy 0.2679 | loss -1.91940
| epoch  37 | lr 0.00035 | R 23.62054 | entropy 0.2684 | loss -3.14576
| epoch  37 | lr 0.00035 | R 23.23041 | entropy 0.2690 | loss -5.84952
| epoch  37 | lr 0.00035 | R 24.44914 | entropy 0.2693 | loss 1.36393
| epoch  37 | lr 0.00035 | R 24.04331 | entropy 0.2698 | loss -3.45359
| epoch  37 | lr 0.00035 | R 24.77067 | entropy 0.2699 | loss -1.81991
| epoch  37 | lr 0.00035 | R 23.50775 | entropy 0.2700 | loss -9.14941
| epoch  37 | lr 0.00035 | R 24.10483 | entropy 0.2701 | loss -3.20763
| epoch  37 | lr 0.00035 | R 23.78027 | entropy 0.2709 | loss -3.72663
| epoch  37 | lr 0.00035 | R 23.53188 | entropy 0.2719 | loss -5.48357
| epoch  37 | lr 0.00035 | R 23.93447 | entropy 0.2721 | loss -4.60697
| epoch  37 | lr 0.00035 | R 23.75694 | entropy 0.2729 | loss -7.94628
| epoch  37 | lr 0.00035 | R 23.88960 | entropy 0.2715 | loss -12.24730
| epoch  37 | lr 0.00035 | R 25.20721 | entropy 0.2694 | loss 2.66609
| epoch  37 | lr 0.00035 | R 24.04762 | entropy 0.2697 | loss -10.23488
| epoch  37 | lr 0.00035 | R 23.60028 | entropy 0.2670 | loss -7.72365
| epoch  37 | lr 0.00035 | R 25.23827 | entropy 0.2650 | loss -4.41533
| epoch  37 | lr 0.00035 | R 24.23560 | entropy 0.2628 | loss -9.26348
| epoch  37 | lr 0.00035 | R 25.22507 | entropy 0.2617 | loss 0.09666
| epoch  37 | lr 0.00035 | R 24.27773 | entropy 0.2594 | loss -13.51627
| epoch  37 | lr 0.00035 | R 25.36738 | entropy 0.2570 | loss -3.83003
| epoch  37 | lr 0.00035 | R 25.14862 | entropy 0.2552 | loss -3.62227
| epoch  37 | lr 0.00035 | R 24.32607 | entropy 0.2545 | loss -5.23947
| epoch  37 | lr 0.00035 | R 25.65834 | entropy 0.2539 | loss -5.05516
| epoch  37 | lr 0.00035 | R 25.23270 | entropy 0.2547 | loss -8.84322
| epoch  37 | lr 0.00035 | R 25.16884 | entropy 0.2545 | loss -7.50925
| epoch  37 | lr 0.00035 | R 25.03087 | entropy 0.2545 | loss -6.70157
| epoch  37 | lr 0.00035 | R 25.20704 | entropy 0.2535 | loss -3.99446
| epoch  37 | lr 0.00035 | R 25.60306 | entropy 0.2533 | loss -5.11575
| epoch  37 | lr 0.00035 | R 25.25723 | entropy 0.2538 | loss -8.36786
| epoch  37 | lr 0.00035 | R 25.06351 | entropy 0.2546 | loss -3.92625
| epoch  37 | lr 0.00035 | R 24.44287 | entropy 0.2554 | loss -8.45228
| epoch  37 | lr 0.00035 | R 25.82341 | entropy 0.2555 | loss -5.29579
derive | max_R: 53.051193
========> finish evaluate on one epoch<======
eval | loss:    31.06 | ppl: 30943561553259.54 | accuracy:     0.85
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch37_step30498.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch37_step78000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch33_step27370.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch33_step70000.pth
| epoch  38 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  38 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  38 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  38 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  38 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  38 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  38 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  38 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  38 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  38 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  38 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  38 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  38 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  38 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  38 | lr 0.04 | raw loss 0.09 | loss 0.09 | ppl     1.09
====>train_shared<====== finish one epoch
| epoch  38 | lr 0.00035 | R 32.51827 | entropy 0.2565 | loss -20.51118
| epoch  38 | lr 0.00035 | R 37.37480 | entropy 0.2592 | loss 8.02062
| epoch  38 | lr 0.00035 | R 36.07104 | entropy 0.2616 | loss -4.14918
| epoch  38 | lr 0.00035 | R 37.39108 | entropy 0.2659 | loss -11.13893
| epoch  38 | lr 0.00035 | R 37.16077 | entropy 0.2662 | loss -9.15573
| epoch  38 | lr 0.00035 | R 37.27160 | entropy 0.2652 | loss -13.95665
| epoch  38 | lr 0.00035 | R 37.87480 | entropy 0.2647 | loss -4.36747
| epoch  38 | lr 0.00035 | R 36.76040 | entropy 0.2643 | loss -17.06371
| epoch  38 | lr 0.00035 | R 37.28982 | entropy 0.2639 | loss -7.37016
| epoch  38 | lr 0.00035 | R 37.76901 | entropy 0.2629 | loss -5.16731
| epoch  38 | lr 0.00035 | R 37.64685 | entropy 0.2629 | loss -8.31000
| epoch  38 | lr 0.00035 | R 37.51646 | entropy 0.2625 | loss -7.47253
| epoch  38 | lr 0.00035 | R 37.00399 | entropy 0.2617 | loss -16.93946
| epoch  38 | lr 0.00035 | R 37.37799 | entropy 0.2598 | loss -13.66913
| epoch  38 | lr 0.00035 | R 37.41740 | entropy 0.2585 | loss -9.47304
| epoch  38 | lr 0.00035 | R 37.10205 | entropy 0.2549 | loss -15.10245
| epoch  38 | lr 0.00035 | R 36.41734 | entropy 0.2536 | loss -22.80923
| epoch  38 | lr 0.00035 | R 38.28640 | entropy 0.2491 | loss -7.52230
| epoch  38 | lr 0.00035 | R 38.45838 | entropy 0.2448 | loss -10.19143
| epoch  38 | lr 0.00035 | R 37.21343 | entropy 0.2420 | loss -15.00352
| epoch  38 | lr 0.00035 | R 39.11579 | entropy 0.2409 | loss -4.64680
| epoch  38 | lr 0.00035 | R 37.77873 | entropy 0.2372 | loss -11.31877
| epoch  38 | lr 0.00035 | R 37.10381 | entropy 0.2352 | loss -9.56708
| epoch  38 | lr 0.00035 | R 38.85119 | entropy 0.2352 | loss -4.46276
| epoch  38 | lr 0.00035 | R 36.68396 | entropy 0.2347 | loss -25.77816
| epoch  38 | lr 0.00035 | R 37.65261 | entropy 0.2350 | loss -12.99214
| epoch  38 | lr 0.00035 | R 38.45810 | entropy 0.2351 | loss -7.22950
| epoch  38 | lr 0.00035 | R 37.81653 | entropy 0.2342 | loss -16.71444
| epoch  38 | lr 0.00035 | R 38.47854 | entropy 0.2322 | loss -10.94824
| epoch  38 | lr 0.00035 | R 37.70883 | entropy 0.2316 | loss -13.86229
| epoch  38 | lr 0.00035 | R 37.52863 | entropy 0.2317 | loss -15.56961
| epoch  38 | lr 0.00035 | R 38.84181 | entropy 0.2315 | loss -4.97169
| epoch  38 | lr 0.00035 | R 38.39408 | entropy 0.2313 | loss -11.62128
| epoch  38 | lr 0.00035 | R 38.13990 | entropy 0.2317 | loss -8.18022
| epoch  38 | lr 0.00035 | R 38.00621 | entropy 0.2320 | loss -11.61798
| epoch  38 | lr 0.00035 | R 38.19762 | entropy 0.2319 | loss -14.67497
| epoch  38 | lr 0.00035 | R 38.39299 | entropy 0.2307 | loss -4.50210
| epoch  38 | lr 0.00035 | R 38.57904 | entropy 0.2303 | loss -10.13761
| epoch  38 | lr 0.00035 | R 38.11097 | entropy 0.2303 | loss -12.68365
derive | max_R: 66.649025
========> finish evaluate on one epoch<======
eval | loss:    34.43 | ppl: 894934332185607.38 | accuracy:     0.83
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch38_step31280.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch38_step80000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch34_step72000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch34_step28152.pth
| epoch  39 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  39 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  39 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  39 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  39 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  39 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  39 | lr 0.04 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  39 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  39 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  39 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  39 | lr 0.04 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  39 | lr 0.04 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  39 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.10
| epoch  39 | lr 0.04 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  39 | lr 0.04 | raw loss 0.10 | loss 0.10 | ppl     1.10
====>train_shared<====== finish one epoch
| epoch  39 | lr 0.00035 | R 33.71740 | entropy 0.2296 | loss -20.07813
| epoch  39 | lr 0.00035 | R 33.94373 | entropy 0.2292 | loss -1.38215
| epoch  39 | lr 0.00035 | R 35.58079 | entropy 0.2288 | loss 0.63112
| epoch  39 | lr 0.00035 | R 33.15094 | entropy 0.2285 | loss -20.86359
| epoch  39 | lr 0.00035 | R 33.98899 | entropy 0.2276 | loss -8.80582
| epoch  39 | lr 0.00035 | R 33.69607 | entropy 0.2275 | loss -10.17529
| epoch  39 | lr 0.00035 | R 32.20918 | entropy 0.2276 | loss -12.91058
| epoch  39 | lr 0.00035 | R 33.11232 | entropy 0.2271 | loss -13.24826
| epoch  39 | lr 0.00035 | R 34.25819 | entropy 0.2267 | loss -3.80092
| epoch  39 | lr 0.00035 | R 33.10489 | entropy 0.2266 | loss -7.80646
| epoch  39 | lr 0.00035 | R 34.04602 | entropy 0.2266 | loss -7.94590
| epoch  39 | lr 0.00035 | R 33.25714 | entropy 0.2265 | loss -13.58378
| epoch  39 | lr 0.00035 | R 34.03422 | entropy 0.2263 | loss -7.56550
| epoch  39 | lr 0.00035 | R 34.01702 | entropy 0.2260 | loss -9.44124
| epoch  39 | lr 0.00035 | R 34.31576 | entropy 0.2259 | loss -5.68364
| epoch  39 | lr 0.00035 | R 34.40878 | entropy 0.2259 | loss -6.09736
| epoch  39 | lr 0.00035 | R 33.33834 | entropy 0.2260 | loss -7.56312
| epoch  39 | lr 0.00035 | R 34.26038 | entropy 0.2263 | loss -9.65999
| epoch  39 | lr 0.00035 | R 34.00640 | entropy 0.2261 | loss -16.83434
| epoch  39 | lr 0.00035 | R 33.59037 | entropy 0.2253 | loss -12.82052
| epoch  39 | lr 0.00035 | R 32.97439 | entropy 0.2250 | loss -20.37162
| epoch  39 | lr 0.00035 | R 34.43034 | entropy 0.2250 | loss -2.15408
| epoch  39 | lr 0.00035 | R 32.31483 | entropy 0.2253 | loss -14.87249
| epoch  39 | lr 0.00035 | R 32.92366 | entropy 0.2254 | loss -11.07569
| epoch  39 | lr 0.00035 | R 33.34392 | entropy 0.2256 | loss -11.70124
| epoch  39 | lr 0.00035 | R 32.72252 | entropy 0.2257 | loss -18.39423
| epoch  39 | lr 0.00035 | R 33.08391 | entropy 0.2253 | loss -13.96573
| epoch  39 | lr 0.00035 | R 32.28760 | entropy 0.2250 | loss -14.85126
| epoch  39 | lr 0.00035 | R 34.12317 | entropy 0.2249 | loss -3.73681
| epoch  39 | lr 0.00035 | R 34.20940 | entropy 0.2247 | loss -11.37533
| epoch  39 | lr 0.00035 | R 33.44942 | entropy 0.2245 | loss -5.70045
| epoch  39 | lr 0.00035 | R 34.26874 | entropy 0.2245 | loss -5.07551
| epoch  39 | lr 0.00035 | R 32.85146 | entropy 0.2248 | loss -12.84588
| epoch  39 | lr 0.00035 | R 32.30279 | entropy 0.2249 | loss -19.05766
| epoch  39 | lr 0.00035 | R 33.81020 | entropy 0.2248 | loss -6.89595
| epoch  39 | lr 0.00035 | R 33.26454 | entropy 0.2249 | loss -11.17352
| epoch  39 | lr 0.00035 | R 33.27770 | entropy 0.2252 | loss -10.65037
| epoch  39 | lr 0.00035 | R 34.05742 | entropy 0.2253 | loss -7.68350
| epoch  39 | lr 0.00035 | R 33.73180 | entropy 0.2246 | loss -15.93917
derive | max_R: 60.319160
========> finish evaluate on one epoch<======
eval | loss:    41.43 | ppl: 983005217509447040.00 | accuracy:     0.78
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch39_step32062.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch39_step82000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch35_step28934.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch35_step74000.pth
| epoch  40 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.10
| epoch  40 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  40 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  40 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  40 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  40 | lr 0.03 | raw loss 0.08 | loss 0.08 | ppl     1.08
| epoch  40 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  40 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  40 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  40 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  40 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  40 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  40 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  40 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  40 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.11
====>train_shared<====== finish one epoch
| epoch  40 | lr 0.00035 | R 34.19405 | entropy 0.2242 | loss -30.73032
| epoch  40 | lr 0.00035 | R 37.01550 | entropy 0.2247 | loss 4.05250
| epoch  40 | lr 0.00035 | R 36.31810 | entropy 0.2247 | loss -18.03541
| epoch  40 | lr 0.00035 | R 38.16064 | entropy 0.2244 | loss -5.98096
| epoch  40 | lr 0.00035 | R 38.57490 | entropy 0.2245 | loss -17.58730
| epoch  40 | lr 0.00035 | R 38.83073 | entropy 0.2249 | loss -15.24974
| epoch  40 | lr 0.00035 | R 40.05584 | entropy 0.2247 | loss -6.60439
| epoch  40 | lr 0.00035 | R 39.57183 | entropy 0.2246 | loss -7.70600
| epoch  40 | lr 0.00035 | R 39.56092 | entropy 0.2246 | loss -4.18121
| epoch  40 | lr 0.00035 | R 39.38554 | entropy 0.2247 | loss -11.66305
| epoch  40 | lr 0.00035 | R 39.91415 | entropy 0.2251 | loss -5.22014
| epoch  40 | lr 0.00035 | R 39.51655 | entropy 0.2252 | loss -5.43699
| epoch  40 | lr 0.00035 | R 38.82466 | entropy 0.2256 | loss -10.39349
| epoch  40 | lr 0.00035 | R 40.38798 | entropy 0.2269 | loss -4.43934
| epoch  40 | lr 0.00035 | R 39.44624 | entropy 0.2283 | loss -8.50644
| epoch  40 | lr 0.00035 | R 39.80520 | entropy 0.2295 | loss -7.68708
| epoch  40 | lr 0.00035 | R 39.03804 | entropy 0.2301 | loss -9.39563
| epoch  40 | lr 0.00035 | R 38.66032 | entropy 0.2308 | loss -15.05801
| epoch  40 | lr 0.00035 | R 39.79860 | entropy 0.2326 | loss -13.28410
| epoch  40 | lr 0.00035 | R 39.07837 | entropy 0.2296 | loss -19.70097
| epoch  40 | lr 0.00035 | R 39.96459 | entropy 0.2275 | loss -15.22095
| epoch  40 | lr 0.00035 | R 39.55341 | entropy 0.2291 | loss -19.72784
| epoch  40 | lr 0.00035 | R 39.24373 | entropy 0.2270 | loss -11.89697
| epoch  40 | lr 0.00035 | R 41.03423 | entropy 0.2263 | loss -4.98100
| epoch  40 | lr 0.00035 | R 39.79333 | entropy 0.2270 | loss -14.95526
| epoch  40 | lr 0.00035 | R 40.90755 | entropy 0.2278 | loss -4.31358
| epoch  40 | lr 0.00035 | R 40.09710 | entropy 0.2280 | loss -18.46213
| epoch  40 | lr 0.00035 | R 39.97268 | entropy 0.2307 | loss -11.44869
| epoch  40 | lr 0.00035 | R 38.76719 | entropy 0.2343 | loss -19.70441
| epoch  40 | lr 0.00035 | R 40.62097 | entropy 0.2392 | loss -5.54478
| epoch  40 | lr 0.00035 | R 40.46617 | entropy 0.2423 | loss -11.05467
| epoch  40 | lr 0.00035 | R 40.15451 | entropy 0.2436 | loss -13.29684
| epoch  40 | lr 0.00035 | R 39.75160 | entropy 0.2419 | loss -13.97874
| epoch  40 | lr 0.00035 | R 38.96117 | entropy 0.2425 | loss -26.37983
| epoch  40 | lr 0.00035 | R 40.93078 | entropy 0.2440 | loss -9.32046
| epoch  40 | lr 0.00035 | R 41.17936 | entropy 0.2443 | loss -7.91707
| epoch  40 | lr 0.00035 | R 39.80376 | entropy 0.2440 | loss -13.94528
| epoch  40 | lr 0.00035 | R 40.05767 | entropy 0.2447 | loss -15.28110
| epoch  40 | lr 0.00035 | R 39.29449 | entropy 0.2444 | loss -16.38168
derive | max_R: 66.936607
========> finish evaluate on one epoch<======
eval | loss:    37.55 | ppl: 20253806388974088.00 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch40_step32844.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch40_step84000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch36_step76000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch36_step29716.pth
| epoch  41 | lr 0.03 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  41 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  41 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  41 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  41 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  41 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  41 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  41 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  41 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  41 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.09
| epoch  41 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  41 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  41 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  41 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  41 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
====>train_shared<====== finish one epoch
| epoch  41 | lr 0.00035 | R 33.31934 | entropy 0.2432 | loss -1.27503
| epoch  41 | lr 0.00035 | R 36.14911 | entropy 0.2432 | loss 8.49339
| epoch  41 | lr 0.00035 | R 35.44102 | entropy 0.2444 | loss -21.25376
| epoch  41 | lr 0.00035 | R 31.03250 | entropy 0.2449 | loss -17.99432
| epoch  41 | lr 0.00035 | R 31.47707 | entropy 0.2453 | loss -3.08411
| epoch  41 | lr 0.00035 | R 29.90776 | entropy 0.2454 | loss -18.12415
| epoch  41 | lr 0.00035 | R 30.98703 | entropy 0.2450 | loss -6.14972
| epoch  41 | lr 0.00035 | R 30.19513 | entropy 0.2454 | loss -14.82393
| epoch  41 | lr 0.00035 | R 30.04498 | entropy 0.2446 | loss -17.01929
| epoch  41 | lr 0.00035 | R 31.25656 | entropy 0.2443 | loss -3.90967
| epoch  41 | lr 0.00035 | R 30.72731 | entropy 0.2437 | loss -11.49324
| epoch  41 | lr 0.00035 | R 30.93261 | entropy 0.2437 | loss -9.35252
| epoch  41 | lr 0.00035 | R 31.79230 | entropy 0.2428 | loss -4.76242
| epoch  41 | lr 0.00035 | R 31.69035 | entropy 0.2405 | loss -8.77553
| epoch  41 | lr 0.00035 | R 31.20306 | entropy 0.2396 | loss -7.91584
| epoch  41 | lr 0.00035 | R 31.31673 | entropy 0.2396 | loss -5.36491
| epoch  41 | lr 0.00035 | R 31.53226 | entropy 0.2391 | loss -14.20449
| epoch  41 | lr 0.00035 | R 30.82907 | entropy 0.2387 | loss -8.46368
| epoch  41 | lr 0.00035 | R 29.98075 | entropy 0.2390 | loss -13.04100
| epoch  41 | lr 0.00035 | R 31.89523 | entropy 0.2394 | loss -5.12375
| epoch  41 | lr 0.00035 | R 29.99080 | entropy 0.2401 | loss -19.26998
| epoch  41 | lr 0.00035 | R 30.33298 | entropy 0.2405 | loss -11.85514
| epoch  41 | lr 0.00035 | R 32.33176 | entropy 0.2406 | loss -5.62025
| epoch  41 | lr 0.00035 | R 31.74319 | entropy 0.2396 | loss -8.23195
| epoch  41 | lr 0.00035 | R 31.20875 | entropy 0.2396 | loss -9.73360
| epoch  41 | lr 0.00035 | R 31.87741 | entropy 0.2405 | loss -8.95356
| epoch  41 | lr 0.00035 | R 31.18083 | entropy 0.2410 | loss -11.94847
| epoch  41 | lr 0.00035 | R 32.52084 | entropy 0.2410 | loss -3.00821
| epoch  41 | lr 0.00035 | R 31.69882 | entropy 0.2413 | loss -12.62275
| epoch  41 | lr 0.00035 | R 32.07727 | entropy 0.2416 | loss -1.90546
| epoch  41 | lr 0.00035 | R 31.60137 | entropy 0.2423 | loss -6.34773
| epoch  41 | lr 0.00035 | R 32.03973 | entropy 0.2430 | loss -9.11728
| epoch  41 | lr 0.00035 | R 31.36651 | entropy 0.2444 | loss -8.71271
| epoch  41 | lr 0.00035 | R 31.90866 | entropy 0.2474 | loss -8.75357
| epoch  41 | lr 0.00035 | R 31.77727 | entropy 0.2475 | loss -9.09975
| epoch  41 | lr 0.00035 | R 31.60734 | entropy 0.2469 | loss -7.90907
| epoch  41 | lr 0.00035 | R 31.75474 | entropy 0.2456 | loss -13.49594
| epoch  41 | lr 0.00035 | R 32.29541 | entropy 0.2458 | loss -13.93823
| epoch  41 | lr 0.00035 | R 32.51927 | entropy 0.2425 | loss -8.64933
derive | max_R: 65.700249
========> finish evaluate on one epoch<======
eval | loss:    34.95 | ppl: 1507414602024205.75 | accuracy:     0.86
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch41_step33626.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch41_step86000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch37_step78000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch37_step30498.pth
| epoch  42 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  42 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  42 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  42 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  42 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  42 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  42 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  42 | lr 0.03 | raw loss 0.14 | loss 0.14 | ppl     1.15
| epoch  42 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  42 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  42 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  42 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  42 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  42 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.11
| epoch  42 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
====>train_shared<====== finish one epoch
| epoch  42 | lr 0.00035 | R 33.54724 | entropy 0.2375 | loss -4.54901
| epoch  42 | lr 0.00035 | R 34.86081 | entropy 0.2370 | loss -15.36111
| epoch  42 | lr 0.00035 | R 34.25615 | entropy 0.2350 | loss -7.21599
| epoch  42 | lr 0.00035 | R 30.38351 | entropy 0.2336 | loss -15.08536
| epoch  42 | lr 0.00035 | R 29.53488 | entropy 0.2334 | loss -9.31343
| epoch  42 | lr 0.00035 | R 29.40648 | entropy 0.2324 | loss -7.35108
| epoch  42 | lr 0.00035 | R 29.53527 | entropy 0.2307 | loss -12.94970
| epoch  42 | lr 0.00035 | R 29.27677 | entropy 0.2300 | loss -10.41341
| epoch  42 | lr 0.00035 | R 29.65871 | entropy 0.2309 | loss -7.23465
| epoch  42 | lr 0.00035 | R 29.31186 | entropy 0.2300 | loss -10.36807
| epoch  42 | lr 0.00035 | R 29.56545 | entropy 0.2296 | loss -7.90810
| epoch  42 | lr 0.00035 | R 30.18085 | entropy 0.2299 | loss -7.53072
| epoch  42 | lr 0.00035 | R 29.54547 | entropy 0.2305 | loss -6.10518
| epoch  42 | lr 0.00035 | R 29.96260 | entropy 0.2312 | loss -5.07218
| epoch  42 | lr 0.00035 | R 29.26738 | entropy 0.2308 | loss -16.01619
| epoch  42 | lr 0.00035 | R 30.42511 | entropy 0.2313 | loss -1.73334
| epoch  42 | lr 0.00035 | R 29.78899 | entropy 0.2316 | loss -6.66025
| epoch  42 | lr 0.00035 | R 29.20366 | entropy 0.2320 | loss -11.51841
| epoch  42 | lr 0.00035 | R 29.76704 | entropy 0.2322 | loss -8.35634
| epoch  42 | lr 0.00035 | R 30.08749 | entropy 0.2327 | loss -4.69939
| epoch  42 | lr 0.00035 | R 29.73585 | entropy 0.2329 | loss -10.40002
| epoch  42 | lr 0.00035 | R 30.26498 | entropy 0.2327 | loss -9.20244
| epoch  42 | lr 0.00035 | R 29.52529 | entropy 0.2323 | loss -12.61116
| epoch  42 | lr 0.00035 | R 30.53023 | entropy 0.2331 | loss -4.38470
| epoch  42 | lr 0.00035 | R 29.43974 | entropy 0.2315 | loss -10.65751
| epoch  42 | lr 0.00035 | R 29.92696 | entropy 0.2319 | loss -5.47872
| epoch  42 | lr 0.00035 | R 30.67236 | entropy 0.2321 | loss -1.98679
| epoch  42 | lr 0.00035 | R 30.40369 | entropy 0.2324 | loss -5.08296
srun: interrupt (one more within 1 sec to abort)
srun: task 0: running
srun: interrupt (one more within 1 sec to abort)
srun: task 0: running
srun: interrupt (one more within 1 sec to abort)
srun: task 0: running
srun: sending Ctrl-C to job 1542902.0
Traceback (most recent call last):
  File "main.py", line 4, in <module>
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: forcing job termination
    import torch
  File "/mnt/lustre/lianqing/anaconda3/lib/python3.6/site-packages/torch/__init__.py", line 37, in <module>
slurmstepd: *** STEP 1542902.0 ON BJ-IDC1-10-10-30-223 CANCELLED AT 2018-03-27T10:59:44 ***
| epoch  42 | lr 0.00035 | R 28.88280 | entropy 0.2324 | loss -22.27653
| epoch  42 | lr 0.00035 | R 29.86944 | entropy 0.2326 | loss -6.87014
| epoch  42 | lr 0.00035 | R 30.83287 | entropy 0.2320 | loss 2.11915
| epoch  42 | lr 0.00035 | R 30.06862 | entropy 0.2321 | loss -9.94121
| epoch  42 | lr 0.00035 | R 30.23274 | entropy 0.2322 | loss -5.47696
| epoch  42 | lr 0.00035 | R 30.08416 | entropy 0.2321 | loss -9.41681
| epoch  42 | lr 0.00035 | R 30.24793 | entropy 0.2316 | loss -3.06073
| epoch  42 | lr 0.00035 | R 30.24942 | entropy 0.2315 | loss -6.88780
| epoch  42 | lr 0.00035 | R 30.65127 | entropy 0.2312 | loss -2.72724
| epoch  42 | lr 0.00035 | R 30.20493 | entropy 0.2314 | loss -12.71661
| epoch  42 | lr 0.00035 | R 29.72008 | entropy 0.2296 | loss -7.86578
derive | max_R: 61.628780
========> finish evaluate on one epoch<======
eval | loss:    25.54 | ppl: 123773637887.41 | accuracy:     0.88
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch42_step34408.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch42_step88000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch38_step80000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch38_step31280.pth
| epoch  43 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.09
| epoch  43 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.10
| epoch  43 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  43 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  43 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  43 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  43 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  43 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  43 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.13
| epoch  43 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  43 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
| epoch  43 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  43 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.11
| epoch  43 | lr 0.03 | raw loss 0.13 | loss 0.13 | ppl     1.14
| epoch  43 | lr 0.03 | raw loss 0.11 | loss 0.11 | ppl     1.12
====>train_shared<====== finish one epoch
| epoch  43 | lr 0.00035 | R 35.23878 | entropy 0.2297 | loss -0.16595
| epoch  43 | lr 0.00035 | R 39.75024 | entropy 0.2298 | loss 1.45559
| epoch  43 | lr 0.00035 | R 37.21919 | entropy 0.2278 | loss -4.18446
| epoch  43 | lr 0.00035 | R 33.31521 | entropy 0.2282 | loss -13.02756
| epoch  43 | lr 0.00035 | R 32.76303 | entropy 0.2282 | loss -4.81072
| epoch  43 | lr 0.00035 | R 32.75902 | entropy 0.2286 | loss -7.49902
| epoch  43 | lr 0.00035 | R 32.34252 | entropy 0.2288 | loss -11.91019
| epoch  43 | lr 0.00035 | R 33.09090 | entropy 0.2287 | loss -5.39807
| epoch  43 | lr 0.00035 | R 31.62497 | entropy 0.2283 | loss -18.14389
| epoch  43 | lr 0.00035 | R 32.46080 | entropy 0.2275 | loss -5.27017
| epoch  43 | lr 0.00035 | R 32.54320 | entropy 0.2283 | loss -4.96013
| epoch  43 | lr 0.00035 | R 31.68974 | entropy 0.2286 | loss -10.82090
| epoch  43 | lr 0.00035 | R 32.60823 | entropy 0.2285 | loss -6.43250
| epoch  43 | lr 0.00035 | R 31.66899 | entropy 0.2276 | loss -15.14744
| epoch  43 | lr 0.00035 | R 31.63399 | entropy 0.2275 | loss -7.97970
| epoch  43 | lr 0.00035 | R 32.29490 | entropy 0.2275 | loss -11.26422
| epoch  43 | lr 0.00035 | R 32.46016 | entropy 0.2274 | loss -9.96162
| epoch  43 | lr 0.00035 | R 32.28442 | entropy 0.2274 | loss -8.23304
| epoch  43 | lr 0.00035 | R 32.12139 | entropy 0.2275 | loss -8.49941
| epoch  43 | lr 0.00035 | R 33.26982 | entropy 0.2279 | loss -2.79978
| epoch  43 | lr 0.00035 | R 32.07107 | entropy 0.2279 | loss -10.97651
| epoch  43 | lr 0.00035 | R 33.07179 | entropy 0.2269 | loss -4.73535
| epoch  43 | lr 0.00035 | R 33.24572 | entropy 0.2265 | loss -3.48158
| epoch  43 | lr 0.00035 | R 32.90645 | entropy 0.2262 | loss -11.49802
| epoch  43 | lr 0.00035 | R 31.68037 | entropy 0.2260 | loss -13.42988
| epoch  43 | lr 0.00035 | R 32.20274 | entropy 0.2252 | loss -10.07455
| epoch  43 | lr 0.00035 | R 33.17169 | entropy 0.2248 | loss -1.50693
| epoch  43 | lr 0.00035 | R 32.07420 | entropy 0.2246 | loss -11.67563
| epoch  43 | lr 0.00035 | R 31.52132 | entropy 0.2251 | loss -9.41494
| epoch  43 | lr 0.00035 | R 32.62149 | entropy 0.2252 | loss -6.11722
| epoch  43 | lr 0.00035 | R 32.92248 | entropy 0.2254 | loss -5.02621
| epoch  43 | lr 0.00035 | R 32.82705 | entropy 0.2253 | loss -8.00590
| epoch  43 | lr 0.00035 | R 32.17006 | entropy 0.2252 | loss -10.24436
| epoch  43 | lr 0.00035 | R 32.65218 | entropy 0.2248 | loss -6.33798
| epoch  43 | lr 0.00035 | R 31.24657 | entropy 0.2245 | loss -15.96328
| epoch  43 | lr 0.00035 | R 32.82203 | entropy 0.2247 | loss -5.29590
| epoch  43 | lr 0.00035 | R 32.48793 | entropy 0.2249 | loss -11.57219
| epoch  43 | lr 0.00035 | R 32.36047 | entropy 0.2248 | loss -6.72290
| epoch  43 | lr 0.00035 | R 32.45463 | entropy 0.2245 | loss -5.44252
derive | max_R: 64.670517
========> finish evaluate on one epoch<======
eval | loss:    34.86 | ppl: 1376156490351298.75 | accuracy:     0.88
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/shared_epoch43_step35190.pth
[*] SAVED: logs/cifar10_2018-03-26_15-10-32/controller_epoch43_step90000.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/shared_epoch39_step32062.pth
[*] Removed: logs/cifar10_2018-03-26_15-10-32/controller_epoch39_step82000.pth
| epoch  44 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.10
| epoch  44 | lr 0.03 | raw loss 0.12 | loss 0.12 | ppl     1.12
| epoch  44 | lr 0.03 | raw loss 0.08 | loss 0.08 | ppl     1.08
| epoch  44 | lr 0.03 | raw loss 0.08 | loss 0.08 | ppl     1.09
| epoch  44 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.09
| epoch  44 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  44 | lr 0.03 | raw loss 0.08 | loss 0.08 | ppl     1.09
| epoch  44 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.10
| epoch  44 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.10
| epoch  44 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.09
| epoch  44 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.09
| epoch  44 | lr 0.03 | raw loss 0.08 | loss 0.08 | ppl     1.08
| epoch  44 | lr 0.03 | raw loss 0.09 | loss 0.09 | ppl     1.10
| epoch  44 | lr 0.03 | raw loss 0.08 | loss 0.08 | ppl     1.08
| epoch  44 | lr 0.03 | raw loss 0.10 | loss 0.10 | ppl     1.10
====>train_shared<====== finish one epoch
| epoch  44 | lr 0.00035 | R 37.12559 | entropy 0.2244 | loss -6.74307
| epoch  44 | lr 0.00035 | R 37.20924 | entropy 0.2240 | loss -20.31082
| epoch  44 | lr 0.00035 | R 37.42547 | entropy 0.2237 | loss -14.93847
| epoch  44 | lr 0.00035 | R 35.27082 | entropy 0.2240 | loss -11.58195
| epoch  44 | lr 0.00035 | R 36.26751 | entropy 0.2242 | loss -2.26810
| epoch  44 | lr 0.00035 | R 35.45356 | entropy 0.2241 | loss -6.92764
| epoch  44 | lr 0.00035 | R 35.13309 | entropy 0.2244 | loss -9.67452
| epoch  44 | lr 0.00035 | R 34.80265 | entropy 0.2242 | loss -13.61537
| epoch  44 | lr 0.00035 | R 35.49363 | entropy 0.2238 | loss -3.83735
| epoch  44 | lr 0.00035 | R 34.68092 | entropy 0.2238 | loss -12.70053
| epoch  44 | lr 0.00035 | R 35.77611 | entropy 0.2238 | loss -6.99660
| epoch  44 | lr 0.00035 | R 35.56192 | entropy 0.2238 | loss -8.62553
| epoch  44 | lr 0.00035 | R 36.58992 | entropy 0.2239 | loss -2.05444
| epoch  44 | lr 0.00035 | R 34.89764 | entropy 0.2238 | loss -16.05964
| epoch  44 | lr 0.00035 | R 35.36438 | entropy 0.2238 | loss -5.17667
| epoch  44 | lr 0.00035 | R 35.34954 | entropy 0.2238 | loss -9.74525
| epoch  44 | lr 0.00035 | R 35.46935 | entropy 0.2239 | loss -8.89049
| epoch  44 | lr 0.00035 | R 34.89997 | entropy 0.2239 | loss -5.40544
| epoch  44 | lr 0.00035 | R 35.79868 | entropy 0.2243 | loss -4.78699
| epoch  44 | lr 0.00035 | R 35.69540 | entropy 0.2243 | loss -10.12335
| epoch  44 | lr 0.00035 | R 34.69738 | entropy 0.2241 | loss -21.91683
| epoch  44 | lr 0.00035 | R 34.93130 | entropy 0.2233 | loss -15.82923
| epoch  44 | lr 0.00035 | R 36.04710 | entropy 0.2238 | loss -1.27680
| epoch  44 | lr 0.00035 | R 35.45487 | entropy 0.2240 | loss -9.32234
| epoch  44 | lr 0.00035 | R 34.92660 | entropy 0.2238 | loss -13.30788
| epoch  44 | lr 0.00035 | R 34.76789 | entropy 0.2231 | loss -12.63806
| epoch  44 | lr 0.00035 | R 35.99847 | entropy 0.2230 | loss -7.22646
| epoch  44 | lr 0.00035 | R 36.44880 | entropy 0.2229 | loss -0.78969
| epoch  44 | lr 0.00035 | R 35.63287 | entropy 0.2224 | loss -8.13584
| epoch  44 | lr 0.00035 | R 36.17341 | entropy 0.2206 | loss -7.47231
| epoch  44 | lr 0.00035 | R 35.77758 | entropy 0.2195 | loss -12.66743
| epoch  44 | lr 0.00035 | R 35.00477 | entropy 0.2190 | loss -12.68898
| epoch  44 | lr 0.00035 | R 36.05443 | entropy 0.2188 | loss -4.49613
| epoch  44 | lr 0.00035 | R 34.76474 | entropy 0.2181 | loss -11.92598
| epoch  44 | lr 0.00035 | R 36.10299 | entropy 0.2187 | loss -6.79768
